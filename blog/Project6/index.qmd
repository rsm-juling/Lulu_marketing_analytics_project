---
title: "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification"
author: "Lulu Ling"
date: today
callout-appearance: minimal 
---
## 1a. K-Means
### Data organization and variable selection
K-means can only process numerical variables, so we need to first select suitable features (in this question, we use beak length and fin length) and remove missing values.

```{python}
# | code-fold: true
# | code-summary: "Data organization"
import pandas as pd
penguins_df=pd.read_csv("/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/palmer_penguins.csv")
data = penguins_df[["bill_length_mm", "flipper_length_mm"]].dropna().values
```

These two variables are highly heterogeneous in body structure and can help the model to effectively classify the groups. Using .dropna() can ensure data integrity and avoid subsequent errors.

### Custom K-means algorithm

Manual implementation allows to deeply understand each step: assigning groups, updating centers, convergence conditions, etc. It can also be used to compare the results with the package.

```{python}
import numpy as np
def kmeans(X, k=3, max_iters=100):
    centroids = X[np.random.choice(len(X), k, replace=False)]
    for _ in range(max_iters):
        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return labels, centroids

labels_custom, centroids_custom = kmeans(data, k=3)
```

This program will assign data points to the nearest center based on distance and repeatedly update the center until it stabilizes. You will see that the groups gradually separate clearly after a few iterations.

### Visualizing K-means clustering results

K-means is a distance-based algorithm. Visualization can help us understand how the algorithm classifies data.

- Each cluster is represented by a different color
- The red X is the cluster center
- You can see how the center points are clustered at the average position of the data within the cluster and show stable cluster boundaries

```{python}
# | code-fold: true
# | code-summary: "Visualizing"
import matplotlib.pyplot as plt

plt.scatter(data[:, 0], data[:, 1], c=labels_custom, cmap='viridis')
plt.scatter(centroids_custom[:, 0], centroids_custom[:, 1], color='red', marker='X')
plt.xlabel("bill_length_mm")
plt.ylabel("flipper_length_mm")
plt.title("K-means clustering results")
plt.show()
```

### WCSS
```{python}
# | code-fold: true
# | code-summary: "WCSS Visualizing"
# Re-import necessary libraries after code state reset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
# Reload the dataset
data = penguins_df[["bill_length_mm", "flipper_length_mm"]].dropna().values

# Initialize lists to store evaluation metrics
inertias = []
silhouette_scores = []
k_range = range(2, 8)  # Testing K values from 2 to 7
# Compute metrics for each K
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(data)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(data, labels))

# Plotting results
plt.figure()

# Plot WCSS
plt.subplot()
plt.plot(k_range, inertias, marker='o')
plt.title("Within-Cluster Sum of Squares (WCSS)")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("WCSS")
plt.grid(True)
```
- The smaller the value, the closer the points in the cluster are to the center, and the better the clustering effect.
- As the number of clusters increases, WCSS will continue to decrease, but the so-called "elbow point" will appear.
- Observation: The decline begins to slow down when K=3 or K=4, and K=3 is a reasonable choice.
### Silhouette Score
```{python}
# | code-fold: true
# | code-summary: "Silhouette Score Visualizing"
# Plot Silhouette Score
plt.plot(k_range, silhouette_scores, marker='o', linestyle='-', linewidth=2, color='#FF8C00')
plt.xticks(k_range)
plt.title("Silhouette Score by Number of Clusters", fontsize=14)
plt.xlabel("Number of Clusters (K)", fontsize=12)
plt.ylabel("Silhouette Score", fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
```

- Measures the relative distance of each point from its own group vs. neighboring groups, ranging from -1 to 1, with higher values ​​indicating clearer grouping.
- Observation: K=2 has the highest score (>0.6), and K=3 has the second highest score (~0.48).
- Although K=2 has the highest score, combined with the WCSS inflection point, K=3 achieves a good balance between the contour grouping effect and the WCSS cost.

### Comparison with sklearn's KMeans results

```{python}
# | code-fold: true
# | code-summary: "KMeans Results"
from sklearn.cluster import KMeans
model = KMeans(n_clusters=3, random_state=42)
model_labels = model.fit_predict(data)
comp_result = pd.DataFrame(data, columns=["bill_length_mm", "flipper_length_mm"])
comp_result["KMeans_label"] = model_labels
comp_result.head(10)
```

```{python}
# | code-fold: true
# | code-summary: "Sklearn Results"
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(data[:, 0], data[:, 1], c=model_labels, cmap='cool', s=50)
plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], color='black', marker='X', s=200)
plt.xlabel("bill_length_mm")
plt.ylabel("flipper_length_mm")
plt.title("Sklearn KMeans Clustering Result")
plt.grid(True)
plt.show()
```

```{python}
# | code-fold: true
# | code-summary: "Cluster Results"
import numpy as np
unique, counts = np.unique(model_labels, return_counts=True)
cluster_summary = dict(zip(unique, counts))
print("The number of data per group：", cluster_summary)
```
### Summary
In this analysis, I implemented the K-means clustering algorithm from scratch and applied it to the Palmer Penguins dataset using bill length and flipper length as input features. The algorithm successfully grouped the penguins into three distinct clusters, capturing underlying patterns in their morphology. I then compared my results to those generated by the built-in KMeans function from scikit-learn, and found a high degree of similarity in clustering structure. This not only validated the correctness of my implementation but also provided valuable insights into how feature selection and distance-based clustering can uncover natural groupings within biological data. Overall, this exercise deepened my understanding of unsupervised learning, algorithm design, and exploratory data analysis.

## 1b. Latent-Class MNL

This analysis aims to use the Latent-Class Multinomial Logit (LC-MNL) model to analyze consumers' choice behavior for four yogurt brands, further identify potential market segments, and understand the responses of different categories of consumers to price and promotions.

- The traditional MNL model assumes that everyone has the same preferences.
- The LC-MNL model allows for consumer heterogeneity and estimates the choice tendency of each category through latent groups (segments), and also estimates the probability of each consumer belonging to which group.

### Data Exploring
The model requires structured input data, including candidate brands, attributes, and selection labels for each selection. This program will split the original 4 brand options in each column into 4 columns, so that 1 observation corresponds to 1 brand, and the total number of data columns will be 4 times, which is suitable for choice model analysis.

```{python}
# | code-fold: true
# | code-summary: "Data Preparation"
import pandas as pd

df_yogurt = pd.read_csv("/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/yogurt_data.csv")
df_long = pd.DataFrame()
for i in range(1, 5):  
    temp = pd.DataFrame({
        'id': df_yogurt['id'],
        'brand': i,
        'price': df_yogurt[f'p{i}'],
        'feature': df_yogurt[f'f{i}'],
        'chosen': df_yogurt[f'y{i}']
    })
    df_long = pd.concat([df_long, temp], ignore_index=True)
df_long.head()
```
- id represents each purchase choice scenario
- choice represents the actual chosen brand
- brand, price, and feat are attributes of candidate brands
- chosen represents whether it is an actual choice (1/0), which is a dependent variable of the MNL model

### Building a baseline Multinomial Logit model (MNL)
Before building the Latent-Class MNL model, we need to estimate a traditional Multinomial Logit (MNL) model as a baseline. This model assumes that all consumers have the same preferences for product attributes (such as price and promotion), and uses this set of "average preferences" to explain the overall market choice behavior.

BIC strikes a balance between log-likelihood performance and parameter complexity:
$\text{BIC} = -2 \cdot \ell_n + k \cdot \log(n)$
- $\ell_n$: log-likelihood of the model (the higher the better)
- $k$: number of parameters (the fewer the better)
- $n$: number of data points (number of samples)

The goal is to find the number of groups with the smallest BIC, which represents the best compromise between performance and simplicity.

```{python}
# | code-fold: true
# | code-summary: "Model Building"
from scipy.optimize import minimize
df_subset = df_long[df_long["id"] <= 300].copy()
X_sub = df_subset[["price", "feature"]].values
y_sub = df_subset["chosen"].values
ids_sub = df_subset["id"].values

def estimate_lc_mnl_fast(X, y, ids, S=2, T=3):
    id_map = {v: i for i, v in enumerate(np.unique(ids))}
    id_idx = np.array([id_map[i] for i in ids])
    num_obs = len(np.unique(id_idx))
    J = 4
    np.random.seed(0)
    beta = [np.random.randn(X.shape[1]) for _ in range(S)]
    pi = np.full(S, 1 / S)
    responsibilities = np.zeros((num_obs, S))

    def mnl_prob(X, beta):
        utilities = X @ beta
        exp_util = np.exp(utilities - np.max(utilities))
        return exp_util / np.sum(exp_util)

    for t in range(T):
        # E-step
        for i in range(num_obs):
            probs = []
            for s in range(S):
                xi = X[id_idx == i]
                yi = y[id_idx == i]
                pj = mnl_prob(xi, beta[s])
                prob = np.prod(pj[yi == 1])
                probs.append(pi[s] * prob)
            probs = np.array(probs)
            responsibilities[i] = probs / np.sum(probs)

        # M-step
        for s in range(S):
            weights = responsibilities[:, s]
            def neg_log_likelihood(b):
                ll = 0
                for i in range(num_obs):
                    xi = X[id_idx == i]
                    yi = y[id_idx == i]
                    pj = mnl_prob(xi, b)
                    ll += weights[i] * np.sum(yi * np.log(pj + 1e-10))
                return -ll
            res = minimize(neg_log_likelihood, beta[s], method='L-BFGS-B')
            beta[s] = res.x
        pi = responsibilities.mean(axis=0)

    ll = 0
    for i in range(num_obs):
        xi = X[id_idx == i]
        yi = y[id_idx == i]
        seg_prob = 0
        for s in range(S):
            pj = mnl_prob(xi, beta[s])
            prob = np.prod(pj[yi == 1])
            seg_prob += pi[s] * prob
        ll += np.log(seg_prob + 1e-10)

    return ll, beta, pi

bic_results_simplified = []

for S in range(2, 6):
    ll, beta_out, pi_out = estimate_lc_mnl_fast(X_sub, y_sub, ids_sub, S=S, T=3)
    k = S * X_sub.shape[1] + (S - 1)  # parameters: S * beta + (S - 1) segment proportions
    n = len(np.unique(ids_sub))  # number of choice situations
    bic = -2 * ll + k * np.log(n)
    bic_results_simplified.append((S, ll, k, bic))

bic_df_simplified = pd.DataFrame(bic_results_simplified, columns=["NumSegments", "LogLikelihood", "NumParameters", "BIC"])
bic_df_simplified.sort_values(by="BIC")
```

- Although increasing the number of groups will slightly increase the model's log-likelihood, the number of parameters will also increase dramatically.
- Therefore, the higher the BIC, the higher the "increase in model complexity" offsets the slight improvement.
- Overall, Segment = 2 is currently the best model setting.

### Compare multi-group models using BIC to select the optimal number of groups
In the previous step, we used the EM algorithm to build Latent-Class Multinomial Logit (LC-MNL) models for 2, 3, 4, and 5 groups respectively. Each model assumes that the consumer market is composed of different numbers of potential groups (segments) and estimates different preference parameters (β) for each group.

However, the more groups there are, the more parameters there are, and the more complex the model is, there is a possibility of "overfitting". Therefore, we cannot only use log-likelihood to select a model, but need to use an indicator that considers the accuracy and complexity of the model - BIC (Bayesian Information Criterion).

BIC formula: $\text{BIC} = -2 \cdot \ell_n + k \cdot \log(n)$

The lower the BIC, the better: it means the model remains simple while improving accuracy.

```{python}
```

- Although the model fit (Log-Likelihood) is slightly improved by increasing the number of groups S
- But with each additional group, the number of parameters also increases significantly (the model complexity increases)
- BIC measures the balance between accuracy and complexity, and BIC is lowest when S=2

In the Latent-Class MNL model, the optimal number of groups is 2.This means the market can be divided into two major consumer groups with significantly different preferences. Next, we can analyze the β coefficient and group proportion (π) for these two groups and make further marketing strategy recommendations

### Analyze the preference parameters and group proportions of the best group (S=2)
We use BIC to determine that two groups (S=2) are the best Latent-Class MNL model. Next, we need to:
- Sort out the preference parameters β (sensitivity to price and promotion) of the two groups
- Draw a graph to compare the preference differences of different groups
- Analyze the proportion of each group of consumers in the market (π)
- Provide business decision suggestions

```{python}
# | code-fold: true
# | code-summary: "Segment"
import pandas as pd
ll, beta, pi = estimate_lc_mnl_fast(X_sub, y_sub, ids_sub, S=2, T=3)
segment_df = pd.DataFrame({
    "Segment": [1, 2],
    "Beta_Price": [beta[0][0], beta[1][0]],
    "Beta_Feature": [beta[0][1], beta[1][1]],
    "Segment_Probability": pi
})
segment_df
```
1. Segment 1
- Very low sensitivity to promotion (β ≈ 0)
- Positive but low sensitivity to price
- 50.3% of the market
- Recommended strategy: Emphasize product quality and brand value, no need for excessive promotion

2. Segment 2
- Very high sensitivity to promotion (β ≈ 2.6)
- Higher price sensitivity
- 49.7% of the market
- Recommended strategy: Discounts, gifts, and special offers are effective and suitable for short-term promotional stimulation

```{python}
# | code-fold: true
# | code-summary: "Visualizing"
import matplotlib.pyplot as plt

# Create summary DataFrame again for clarity
segment_df = pd.DataFrame({
    "Segment": [1, 2],
    "Beta_Price": [beta[0][0], beta[1][0]],
    "Beta_Feature": [beta[0][1], beta[1][1]],
    "Segment_Probability": pi
})

# Plotting segment-specific betas and segment share
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# Bar plot for beta values
segment_df.plot(x="Segment", y=["Beta_Price", "Beta_Feature"], kind="bar", ax=ax[0])
ax[0].set_title("Segment-Specific Preferences")
ax[0].set_ylabel("Coefficient (Beta)")
ax[0].set_xlabel("Segment")
ax[0].legend(["Price", "Feature"], title="Variable")

# Pie chart for segment probabilities
ax[1].pie(segment_df["Segment_Probability"], labels=[f"Segment {s}" for s in segment_df["Segment"]],
          autopct='%1.1f%%', colors=["#66c2a5", "#fc8d62"], startangle=90)
ax[1].set_title("Segment Market Share")

plt.tight_layout()
plt.show()
```
1. Left: Segment preference comparison chart (bar chart)
- Displays the β coefficient of each Segment (potential group) for price (Price) and promotion (Feature)
- Clearly compares the sensitivity of two groups of consumers to different variables

2. Right: Segment market share chart (pie chart)
- Displays the proportion of each potential category in the market (π value)
- Segment 1 is about 50.3%, Segment 2 is about 49.7%

According to the model results, the market can be divided into two potential consumer groups. Segment 1 is very insensitive to promotions and slightly sensitive to prices. It is recommended to adopt a marketing strategy that emphasizes quality and brand value; while Segment 2 is highly sensitive to both prices and promotions. It is suitable to stimulate purchase intention through promotional means such as discounts and gifts. Since the market share of the two groups is almost the same, it is recommended to adopt a dual-track parallel marketing strategy, designing differentiated messages and plans for different groups to increase the overall market penetration rate.

## 2a. K Nearest Neighbors
We want to create a simulated data set for a binary classification problem, with the following features:
- The data has two features (x1, x2)
- The class label y is determined by the boundary of x2 > sin(4x1) + x1 (i.e., the classification above and below a wavy line)

Such data can help us test whether the KNN algorithm can effectively handle classification tasks with "non-linear decision boundaries".

### Generate training data set
We need a set of data to train the KNN model. This set of data will simulate the situation in the real world where the data and boundaries are not linearly separable.
```{python}
import numpy as np
import pandas as pd

# Set random seed
np.random.seed(42)
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
x = np.column_stack((x1, x2))

# Define a wiggly boundary
boundary = np.sin(4 * x1) + x1
y = (x2 > boundary).astype(int)
dat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
```

### Visual training materials and boundaries
Plotting the simulated data set allows us to clearly see the relationship between the distribution of data points and the position of the classification boundary. We randomly generated 100 two-dimensional data (x1, x2), and y was marked as 0 or 1 depending on whether it was above boundary = sin(4 * x1) + x1 (this is a curved "real boundary"). By plotting this boundary and data points, we can visually observe the difficulty of classification.

```{python}
# | code-fold: true
# | code-summary: "Visualizing"
import numpy as np
import pandas as pd

np.random.seed(42)
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
boundary = np.sin(4 * x1) + x1
y = (x2 > boundary).astype(int)

train_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(train_df["x1"], train_df["x2"], c=train_df["y"], cmap='coolwarm', edgecolor='k')
x_line = np.linspace(-3, 3, 300)
plt.plot(x_line, np.sin(4 * x_line) + x_line, color='black', linestyle='--', label="True Boundary")
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Training Data and True Boundary")
plt.legend()
plt.show()
```
Through the graph, we can clearly see how the data points are distributed on both sides of the wiggly boundary, and we can also intuitively judge the difficulty of classification.

### Generate test data sets (different random seeds)
The test set is used to verify the model effect on unseen data. Different seeds must be used to avoid data duplication.
```{python}
np.random.seed(99)
x1_test = np.random.uniform(-3, 3, n)
x2_test = np.random.uniform(-3, 3, n)
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = (x2_test > boundary_test).astype(int)

test_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})
```

### Handwritten KNN Classifier
By implementing KNN yourself, you can deepen your understanding of the logic of neighbor classification: find the k closest points → majority vote.
```{python}
def knn_predict(x_test, X_train, y_train, k):
    y_pred = []
    for x in x_test:
        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))
        neighbors = y_train[np.argsort(distances)[:k]]
        vote = np.bincount(neighbors).argmax()
        y_pred.append(vote)
    return np.array(y_pred)
```

### Compare to sklearn's built-in results
```{python}
# | code-fold: true
# | code-summary: "Compare to sklearn's built-in results"
from sklearn.neighbors import KNeighborsClassifier

np.random.seed(42)
n = 100
x1_train = np.random.uniform(-3, 3, n)
x2_train = np.random.uniform(-3, 3, n)
boundary_train = np.sin(4 * x1_train) + x1_train
y_train = (x2_train > boundary_train).astype(int)
train_df = pd.DataFrame({'x1': x1_train, 'x2': x2_train, 'y': y_train})

np.random.seed(99)
x1_test = np.random.uniform(-3, 3, n)
x2_test = np.random.uniform(-3, 3, n)
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = (x2_test > boundary_test).astype(int)
test_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})

model = KNeighborsClassifier(n_neighbors=5)
model.fit(train_df[["x1", "x2"]], train_df["y"])
acc = model.score(test_df[["x1", "x2"]], test_df["y"])

knn_result_df = pd.DataFrame({
    "Model": ["sklearn KNN"],
    "k": [5],
    "Accuracy": [round(acc * 100, 2)]
})
knn_result_df
```
- We used 5 neighboring points (k=5) to classify the test data
- The prediction accuracy is 90%, which means that the model has good recognition ability for nonlinear boundary data
- This can be used as a comparison benchmark when we implement "custom KNN" or test different k values ​​later

### Accuracy performance for runs k = 1 to 30
In the KNN model, the "k value" represents how many neighboring samples each piece of data needs to refer to for classification. Different k values ​​will have a significant impact on the model's prediction results:
- Too small k value: The model is overly dependent on a single neighbor, easily sensitive to noise, and leads to overfitting
- Too large k value: The model averages too much information, the boundaries become blurred, and underfitting may occur

By testing the performance of the model with k = 1 to 30 one by one, we can choose an optimal k value that makes the classification accuracy most stable and effective.
```{python}
# | code-fold: true
# | code-summary: "Performance Visualization"
accuracies = []
for k in range(1, 31):
    y_pred = knn_predict(test_df[["x1", "x2"]].values, train_df[["x1", "x2"]].values, train_df["y"].values, k)
    acc = np.mean(y_pred == test_df["y"].values)
    accuracies.append(acc)

plt.plot(range(1, 31), np.array(accuracies) * 100, marker='o')
plt.xlabel("k (number of neighbors)")
plt.ylabel("Accuracy (%)")
plt.title("KNN Accuracy vs. k")
plt.grid(True)
plt.show()
```
The horizontal axis of the chart is the number of neighbors k, and the vertical axis is the classification accuracy of the test data (Accuracy %).
- When k = 1 or 2, the accuracy reaches the highest, about 92%
- As k increases, the accuracy shows an overall downward trend
- There are occasional fluctuations in the middle (such as k = 16, 24, the accuracy increases slightly)
- When k > 20, the accuracy is mostly stable at around 86% ~ 88%

## 2b. Key Drivers Analysis
Use a variety of variable importance methods to analyze which variables in a set of data have the greatest impact on the target variable (usually a continuous value). Each method represents a "different model assumption and explanation framework", which can be integrated together to provide a more comprehensive and robust variable screening and insight foundation.

### Data Exploring
```{python}
# | code-fold: true
# | code-summary: "Data"
import pandas as pd

driver = pd.read_csv("/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/data_for_drivers_analysis.csv")

df_head = pd.DataFrame(driver.head())
df_head
```
```{python}
# | code-fold: true
# | code-summary: "Data"
df_desc = pd.DataFrame(driver.describe())
df_desc
```
- satisfaction: is the dependent variable (y), with a value range of 1 to 5, indicating customer satisfaction
- Other columns such as trust, easy, rewarding, etc. are explanatory variables (X), most of which are binary variables of 0/1
- brand, id: used to identify samples, can be used as a basis for grouping or group variables (can be ignored in analysis)

### Calculate basic variable importance indicators (Pearson, standardized coefficient, Usefulness)
The purpose of this step is to calculate three common and powerful variable importance indicators from the perspective of linear correlation and linear model, which will help us to have a preliminary understanding of the influence of each variable on satisfaction.

| Metric Name                     | Description                                                                 |
|----------------------------------|------------------------------------------------------------------------------|
| **Pearson Correlation**          | Measures the linear relationship between each variable and `satisfaction` (ignores other variables) |
| **Standardized Coefficient (β)** | Regression coefficients from a standardized linear model, allowing comparison across variables |
| **Usefulness (R² Drop)**         | Drop in model R² when each variable is removed, reflecting its contribution to explanatory power |

```{python}
# | code-fold: true
# | code-summary: "Variable Importance Indicators"
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

X = driver.drop(columns=["brand", "id", "satisfaction"])
y = driver["satisfaction"]

correlations = X.corrwith(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
reg = LinearRegression().fit(X_scaled, y)
standardized_betas = pd.Series(reg.coef_, index=X.columns)

base_r2 = reg.score(X_scaled, y)
usefulness_scores = {}
for col in X.columns:
    cols = [c for c in X.columns if c != col]
    X_subset = X[cols]
    X_subset_scaled = scaler.fit_transform(X_subset)
    r2 = LinearRegression().fit(X_subset_scaled, y).score(X_subset_scaled, y)
    usefulness_scores[col] = base_r2 - r2
usefulness_series = pd.Series(usefulness_scores)

step2_df = pd.DataFrame({
    "Pearson_Correlation": correlations,
    "Standardized_Beta": standardized_betas,
    "Usefulness_R2_Drop": usefulness_series
}).sort_values(by="Usefulness_R2_Drop", ascending=False).round(4)

step2_df
```
1.	Top Contributors:
- Impact, trust, and service consistently rank highest across all three importance metrics.
- These variables have strong linear relationships with satisfaction and contribute meaningfully in the regression model.
2.	Pearson Correlation:
- trust, impact, and service show the strongest correlations with satisfaction (around 0.25).
3.	Standardized Coefficients (β):
- impact (0.15) and trust (0.14) are the most influential variables after accounting for all others.
4.	Usefulness (R² Drop):
- Removing impact, trust, or service causes the largest drop in model R², indicating their key role in explaining satisfaction.
- Variables like rewarding, popular, and build show very low importance across all measures.

Across all three importance measures, impact, trust, and service are the most valuable drivers of satisfaction. These should be prioritized in managerial focus and communication strategies. The remaining variables may still contribute indirectly or interactively but offer limited individual explanatory power in this linear framework.

### Advanced variable importance index analysis (Permutation, Gini, Johnson)
In the previous step, we evaluated the relationship between variables and satisfaction from the perspective of linear models. However, in practice, there may be nonlinear relationships and interactions between variables. For this reason, we introduce the following three more advanced and flexible methods:

| Metric Name                   | Description                                                                 |
|------------------------------|-----------------------------------------------------------------------------|
| **Permutation Importance**   | Measures the impact on model performance when a variable is randomly shuffled, reflecting its "replaceability" |
| **Gini Importance**          | Based on random forests; sums the decrease in impurity caused by each variable during tree splits |
| **Johnson’s Relative Weights** | Uses standardized regression coefficients squared to estimate each variable's relative contribution to explained variance |

```{python}
# | code-fold: true
# | code-summary: "Comparison Table"
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
import pandas as pd

X = driver.drop(columns=["brand", "id", "satisfaction"])
y = driver["satisfaction"]

lr = LinearRegression().fit(X, y)
perm_importance = permutation_importance(lr, X, y, n_repeats=10, random_state=42)
perm_importance_series = pd.Series(perm_importance.importances_mean, index=X.columns)

rf = RandomForestRegressor(random_state=42)
rf.fit(X, y)
gini_importance = pd.Series(rf.feature_importances_, index=X.columns)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
lr_std = LinearRegression().fit(X_scaled, y)
johnson_weights = pd.Series(lr_std.coef_ ** 2, index=X.columns)
johnson_weights /= johnson_weights.sum()

step3_df = pd.DataFrame({
    "Permutation_Importance": perm_importance_series,
    "Gini_Importance": gini_importance,
    "Johnson_Weights": johnson_weights
}).sort_values(by="Permutation_Importance", ascending=False).round(4)

step3_df.reset_index(inplace=True)
step3_df.rename(columns={"index": "Variable"}, inplace=True)
```
- Impact, trust, and service are stable drivers that rank at the top in all indicators.
- Impact has the highest Johnson Weights (40.4%) and also performs well in permutation and Gini.
- Trust ranks highest in Gini importance, indicating that it is particularly critical in the tree model.
- Low-impact variables such as rewarding, popular, and build perform weakly in all three indicators.

```{python}
# | code-fold: true
# | code-summary: "Comparison Table"
# Re-import required libraries after reset
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance

# Load data
driver = pd.read_csv("/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/data_for_drivers_analysis.csv")
X = driver.drop(columns=["brand", "id", "satisfaction"])
y = driver["satisfaction"]

# Step 2 metrics
# Pearson
correlations = X.corrwith(y)

# Standardized beta
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
reg = LinearRegression().fit(X_scaled, y)
standardized_betas = pd.Series(reg.coef_, index=X.columns)

# Usefulness
base_r2 = reg.score(X_scaled, y)
usefulness_scores = {}
for col in X.columns:
    cols = [c for c in X.columns if c != col]
    X_subset_scaled = scaler.fit_transform(X[cols])
    r2 = LinearRegression().fit(X_subset_scaled, y).score(X_subset_scaled, y)
    usefulness_scores[col] = base_r2 - r2
usefulness_series = pd.Series(usefulness_scores)

step2_df = pd.DataFrame({
    "Pearson_Correlation": correlations,
    "Standardized_Beta": standardized_betas,
    "Usefulness_R2_Drop": usefulness_series
}).round(4).reset_index().rename(columns={"index": "Variable"})

# Step 3 metrics
# Permutation Importance
lr = LinearRegression().fit(X, y)
perm_importance = permutation_importance(lr, X, y, n_repeats=10, random_state=42)
perm_series = pd.Series(perm_importance.importances_mean, index=X.columns)

# Gini Importance
rf = RandomForestRegressor(random_state=42)
rf.fit(X, y)
gini_series = pd.Series(rf.feature_importances_, index=X.columns)

# Johnson Weights
X_scaled = scaler.fit_transform(X)
lr_std = LinearRegression().fit(X_scaled, y)
johnson_weights = pd.Series(lr_std.coef_ ** 2, index=X.columns)
johnson_weights /= johnson_weights.sum()

step3_df = pd.DataFrame({
    "Permutation_Importance": perm_series,
    "Gini_Importance": gini_series,
    "Johnson_Weights": johnson_weights
}).round(4).reset_index().rename(columns={"index": "Variable"})

# Merge Step 2 and Step 3
final_df = pd.merge(step2_df, step3_df, on="Variable").sort_values(by="Permutation_Importance", ascending=False).reset_index(drop=True)
final_df
```

```{python}
# | code-fold: true
# | code-summary: "Comparison Plot"
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid") 

# Prepare data for plotting
plot_df = final_df.set_index("Variable")[[
    "Permutation_Importance", "Gini_Importance", "Johnson_Weights"
]]

# Plot the importance scores as a grouped horizontal bar chart
plot_df.plot(kind="barh", figsize=(10, 6), width=0.8)
plt.title("Comparison of Variable Importance Metrics")
plt.xlabel("Importance Score")
plt.ylabel("Variable")
plt.legend(title="Metric")
plt.tight_layout()
plt.gca().invert_yaxis()  # Show most important variables on top
plt.show()
```
Based on the consolidated table and comparative visualization, the top three variables—impact, trust, and service—consistently emerge as the most important drivers of customer satisfaction across all evaluation methods. These variables not only show the strongest Pearson correlations and standardized regression coefficients but also yield the highest scores in permutation importance, Gini index from random forest, and Johnson’s relative weights.

- Impact stands out as the most influential variable, with the highest Johnson weight (over 40%) and top-ranked permutation importance.
- Trust ranks second overall, especially dominant in tree-based models (highest Gini importance).
- Service maintains a strong and stable position across all linear and nonlinear measures.

On the other hand, variables like rewarding, popular, and build consistently appear at the bottom, indicating relatively low predictive value in explaining satisfaction in this context.

Recommendation: Strategic efforts should prioritize reinforcing the perceived impact, trustworthiness, and quality of service associated with the brand, as these are most likely to drive meaningful improvements in customer satisfaction.

### Managerial Implications
1. Top Priority Drivers
- impact、trust、service consistently appear as top drivers across all methods.
- These dimensions should be the core focus of marketing communication, product messaging, and customer engagement efforts.

2. Low-Impact Factors
- Variables like rewarding, popular, and build show minimal influence across models.
- These may be de-prioritized or reframed, unless they serve niche segments or non-satisfaction outcomes.

3. Strategic Recommendations
- Invest in initiatives that reinforce customer trust, such as transparency, testimonials, and ethical branding.
- Emphasize service quality through employee training, faster response times, and better omnichannel support.
- Highlight perceived impact by showing measurable results and customer transformations.

4. Measurement Plan
- Future campaigns should track these top drivers as KPIs alongside satisfaction.
- Consider A/B testing improvements in these variables to validate causal impact.
