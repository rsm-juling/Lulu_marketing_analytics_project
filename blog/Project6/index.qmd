---
title: "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification"
author: "Lulu Ling"
date: today
callout-appearance: minimal 
---
## 1a. K-Means
### Data organization and variable selection
K-means can only process numerical variables, so we need to first select suitable features (in this question, we use beak length and fin length) and remove missing values.

```{python}
# | code-fold: true
# | code-summary: "Data organization"
import pandas as pd
penguins_df=pd.read_csv("/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/palmer_penguins.csv")
data = penguins_df[["bill_length_mm", "flipper_length_mm"]].dropna().values
```

These two variables are highly heterogeneous in body structure and can help the model to effectively classify the groups. Using .dropna() can ensure data integrity and avoid subsequent errors.

### Custom K-means algorithm

Manual implementation allows to deeply understand each step: assigning groups, updating centers, convergence conditions, etc. It can also be used to compare the results with the package.

```{python}
import numpy as np
def kmeans(X, k=3, max_iters=100):
    centroids = X[np.random.choice(len(X), k, replace=False)]
    for _ in range(max_iters):
        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return labels, centroids

labels_custom, centroids_custom = kmeans(data, k=3)
```

This program will assign data points to the nearest center based on distance and repeatedly update the center until it stabilizes. You will see that the groups gradually separate clearly after a few iterations.

### Visualizing K-means clustering results

K-means is a distance-based algorithm. Visualization can help us understand how the algorithm classifies data.

- Each cluster is represented by a different color
- The red X is the cluster center
- You can see how the center points are clustered at the average position of the data within the cluster and show stable cluster boundaries

```{python}
# | code-fold: true
# | code-summary: "Visualizing"
import matplotlib.pyplot as plt

plt.scatter(data[:, 0], data[:, 1], c=labels_custom, cmap='viridis')
plt.scatter(centroids_custom[:, 0], centroids_custom[:, 1], color='red', marker='X')
plt.xlabel("bill_length_mm")
plt.ylabel("flipper_length_mm")
plt.title("K-means clustering results")
plt.show()
```

### WCSS
```{python}
# | code-fold: true
# | code-summary: "WCSS Visualizing"
# Re-import necessary libraries after code state reset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
# Reload the dataset
data = penguins_df[["bill_length_mm", "flipper_length_mm"]].dropna().values

# Initialize lists to store evaluation metrics
inertias = []
silhouette_scores = []
k_range = range(2, 8)  # Testing K values from 2 to 7
# Compute metrics for each K
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(data)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(data, labels))

# Plotting results
plt.figure()

# Plot WCSS
plt.subplot()
plt.plot(k_range, inertias, marker='o')
plt.title("Within-Cluster Sum of Squares (WCSS)")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("WCSS")
plt.grid(True)
```
- The smaller the value, the closer the points in the cluster are to the center, and the better the clustering effect.
- As the number of clusters increases, WCSS will continue to decrease, but the so-called "elbow point" will appear.
- Observation: The decline begins to slow down when K=3 or K=4, and K=3 is a reasonable choice.
### Silhouette Score
```{python}
# | code-fold: true
# | code-summary: "Silhouette Score Visualizing"
# Plot Silhouette Score
plt.plot(k_range, silhouette_scores, marker='o', linestyle='-', linewidth=2, color='#FF8C00')
plt.xticks(k_range)
plt.title("Silhouette Score by Number of Clusters", fontsize=14)
plt.xlabel("Number of Clusters (K)", fontsize=12)
plt.ylabel("Silhouette Score", fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
```

- Measures the relative distance of each point from its own group vs. neighboring groups, ranging from -1 to 1, with higher values ​​indicating clearer grouping.
- Observation: K=2 has the highest score (>0.6), and K=3 has the second highest score (~0.48).
- Although K=2 has the highest score, combined with the WCSS inflection point, K=3 achieves a good balance between the contour grouping effect and the WCSS cost.

### Comparison with sklearn's KMeans results

```{python}
# | code-fold: true
# | code-summary: "KMeans Results"
from sklearn.cluster import KMeans
model = KMeans(n_clusters=3, random_state=42)
model_labels = model.fit_predict(data)
comp_result = pd.DataFrame(data, columns=["bill_length_mm", "flipper_length_mm"])
comp_result["KMeans_label"] = model_labels
comp_result.head(10)
```

```{python}
# | code-fold: true
# | code-summary: "Sklearn Results"
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(data[:, 0], data[:, 1], c=model_labels, cmap='cool', s=50)
plt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], color='black', marker='X', s=200)
plt.xlabel("bill_length_mm")
plt.ylabel("flipper_length_mm")
plt.title("Sklearn KMeans Clustering Result")
plt.grid(True)
plt.show()
```

```{python}
# | code-fold: true
# | code-summary: "Cluster Results"
import numpy as np
unique, counts = np.unique(model_labels, return_counts=True)
cluster_summary = dict(zip(unique, counts))
print("The number of data per group：", cluster_summary)
```
### Summary
In this analysis, I implemented the K-means clustering algorithm from scratch and applied it to the Palmer Penguins dataset using bill length and flipper length as input features. The algorithm successfully grouped the penguins into three distinct clusters, capturing underlying patterns in their morphology. I then compared my results to those generated by the built-in KMeans function from scikit-learn, and found a high degree of similarity in clustering structure. This not only validated the correctness of my implementation but also provided valuable insights into how feature selection and distance-based clustering can uncover natural groupings within biological data. Overall, this exercise deepened my understanding of unsupervised learning, algorithm design, and exploratory data analysis.

## 1b. Latent-Class MNL

This analysis aims to use the Latent-Class Multinomial Logit (LC-MNL) model to analyze consumers' choice behavior for four yogurt brands, further identify potential market segments, and understand the responses of different categories of consumers to price and promotions.

- The traditional MNL model assumes that everyone has the same preferences.
- The LC-MNL model allows for consumer heterogeneity and estimates the choice tendency of each category through latent groups (segments), and also estimates the probability of each consumer belonging to which group.

### Data Exploring
The model requires structured input data, including candidate brands, attributes, and selection labels for each selection. This program will split the original 4 brand options in each column into 4 columns, so that 1 observation corresponds to 1 brand, and the total number of data columns will be 4 times, which is suitable for choice model analysis.

```{python}
# | code-fold: true
# | code-summary: "Data Preparation"
import pandas as pd

df_yogurt = pd.read_csv("/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/yogurt_data.csv")
df_long = pd.DataFrame()
for i in range(1, 5):  
    temp = pd.DataFrame({
        'id': df_yogurt['id'],
        'brand': i,
        'price': df_yogurt[f'p{i}'],
        'feature': df_yogurt[f'f{i}'],
        'chosen': df_yogurt[f'y{i}']
    })
    df_long = pd.concat([df_long, temp], ignore_index=True)
df_long.head()
```
- id represents each purchase choice scenario
- choice represents the actual chosen brand
- brand, price, and feat are attributes of candidate brands
- chosen represents whether it is an actual choice (1/0), which is a dependent variable of the MNL model

### Building a baseline Multinomial Logit model (MNL)
Before building the Latent-Class MNL model, we need to estimate a traditional Multinomial Logit (MNL) model as a baseline. This model assumes that all consumers have the same preferences for product attributes (such as price and promotion), and uses this set of "average preferences" to explain the overall market choice behavior.

BIC strikes a balance between log-likelihood performance and parameter complexity:
$\text{BIC} = -2 \cdot \ell_n + k \cdot \log(n)$
- $\ell_n$: log-likelihood of the model (the higher the better)
- $k$: number of parameters (the fewer the better)
- $n$: number of data points (number of samples)

The goal is to find the number of groups with the smallest BIC, which represents the best compromise between performance and simplicity.

```{python}
# | code-fold: true
# | code-summary: "Model Building"
from scipy.optimize import minimize
df_subset = df_long[df_long["id"] <= 300].copy()
X_sub = df_subset[["price", "feature"]].values
y_sub = df_subset["chosen"].values
ids_sub = df_subset["id"].values

def estimate_lc_mnl_fast(X, y, ids, S=2, T=3):
    id_map = {v: i for i, v in enumerate(np.unique(ids))}
    id_idx = np.array([id_map[i] for i in ids])
    num_obs = len(np.unique(id_idx))
    J = 4
    np.random.seed(0)
    beta = [np.random.randn(X.shape[1]) for _ in range(S)]
    pi = np.full(S, 1 / S)
    responsibilities = np.zeros((num_obs, S))

    def mnl_prob(X, beta):
        utilities = X @ beta
        exp_util = np.exp(utilities - np.max(utilities))
        return exp_util / np.sum(exp_util)

    for t in range(T):
        # E-step
        for i in range(num_obs):
            probs = []
            for s in range(S):
                xi = X[id_idx == i]
                yi = y[id_idx == i]
                pj = mnl_prob(xi, beta[s])
                prob = np.prod(pj[yi == 1])
                probs.append(pi[s] * prob)
            probs = np.array(probs)
            responsibilities[i] = probs / np.sum(probs)

        # M-step
        for s in range(S):
            weights = responsibilities[:, s]
            def neg_log_likelihood(b):
                ll = 0
                for i in range(num_obs):
                    xi = X[id_idx == i]
                    yi = y[id_idx == i]
                    pj = mnl_prob(xi, b)
                    ll += weights[i] * np.sum(yi * np.log(pj + 1e-10))
                return -ll
            res = minimize(neg_log_likelihood, beta[s], method='L-BFGS-B')
            beta[s] = res.x
        pi = responsibilities.mean(axis=0)

    ll = 0
    for i in range(num_obs):
        xi = X[id_idx == i]
        yi = y[id_idx == i]
        seg_prob = 0
        for s in range(S):
            pj = mnl_prob(xi, beta[s])
            prob = np.prod(pj[yi == 1])
            seg_prob += pi[s] * prob
        ll += np.log(seg_prob + 1e-10)

    return ll, beta, pi

bic_results_simplified = []

for S in range(2, 6):
    ll, beta_out, pi_out = estimate_lc_mnl_fast(X_sub, y_sub, ids_sub, S=S, T=3)
    k = S * X_sub.shape[1] + (S - 1)  # parameters: S * beta + (S - 1) segment proportions
    n = len(np.unique(ids_sub))  # number of choice situations
    bic = -2 * ll + k * np.log(n)
    bic_results_simplified.append((S, ll, k, bic))

bic_df_simplified = pd.DataFrame(bic_results_simplified, columns=["NumSegments", "LogLikelihood", "NumParameters", "BIC"])
bic_df_simplified.sort_values(by="BIC")
```

- Although increasing the number of groups will slightly increase the model's log-likelihood, the number of parameters will also increase dramatically.
- Therefore, the higher the BIC, the higher the "increase in model complexity" offsets the slight improvement.
- Overall, Segment = 2 is currently the best model setting.

### Compare multi-group models using BIC to select the optimal number of groups
In the previous step, we used the EM algorithm to build Latent-Class Multinomial Logit (LC-MNL) models for 2, 3, 4, and 5 groups respectively. Each model assumes that the consumer market is composed of different numbers of potential groups (segments) and estimates different preference parameters (β) for each group.

However, the more groups there are, the more parameters there are, and the more complex the model is, there is a possibility of "overfitting". Therefore, we cannot only use log-likelihood to select a model, but need to use an indicator that considers the accuracy and complexity of the model - BIC (Bayesian Information Criterion).

BIC formula: $\text{BIC} = -2 \cdot \ell_n + k \cdot \log(n)$

The lower the BIC, the better: it means the model remains simple while improving accuracy.

```{python}
```

- Although the model fit (Log-Likelihood) is slightly improved by increasing the number of groups S
- But with each additional group, the number of parameters also increases significantly (the model complexity increases)
- BIC measures the balance between accuracy and complexity, and BIC is lowest when S=2

In the Latent-Class MNL model, the optimal number of groups is 2.This means the market can be divided into two major consumer groups with significantly different preferences. Next, we can analyze the β coefficient and group proportion (π) for these two groups and make further marketing strategy recommendations

### Analyze the preference parameters and group proportions of the best group (S=2)
We use BIC to determine that two groups (S=2) are the best Latent-Class MNL model. Next, we need to:
- Sort out the preference parameters β (sensitivity to price and promotion) of the two groups
- Draw a graph to compare the preference differences of different groups
- Analyze the proportion of each group of consumers in the market (π)
- Provide business decision suggestions

```{python}
# | code-fold: true
# | code-summary: "Segment"
import pandas as pd
ll, beta, pi = estimate_lc_mnl_fast(X_sub, y_sub, ids_sub, S=2, T=3)
segment_df = pd.DataFrame({
    "Segment": [1, 2],
    "Beta_Price": [beta[0][0], beta[1][0]],
    "Beta_Feature": [beta[0][1], beta[1][1]],
    "Segment_Probability": pi
})
segment_df
```
1. Segment 1
- Very low sensitivity to promotion (β ≈ 0)
- Positive but low sensitivity to price
- 50.3% of the market
- Recommended strategy: Emphasize product quality and brand value, no need for excessive promotion

2. Segment 2
- Very high sensitivity to promotion (β ≈ 2.6)
- Higher price sensitivity
- 49.7% of the market
- Recommended strategy: Discounts, gifts, and special offers are effective and suitable for short-term promotional stimulation

```{python}
# | code-fold: true
# | code-summary: "Visualizing"
import matplotlib.pyplot as plt

# Create summary DataFrame again for clarity
segment_df = pd.DataFrame({
    "Segment": [1, 2],
    "Beta_Price": [beta[0][0], beta[1][0]],
    "Beta_Feature": [beta[0][1], beta[1][1]],
    "Segment_Probability": pi
})

# Plotting segment-specific betas and segment share
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# Bar plot for beta values
segment_df.plot(x="Segment", y=["Beta_Price", "Beta_Feature"], kind="bar", ax=ax[0])
ax[0].set_title("Segment-Specific Preferences")
ax[0].set_ylabel("Coefficient (Beta)")
ax[0].set_xlabel("Segment")
ax[0].legend(["Price", "Feature"], title="Variable")

# Pie chart for segment probabilities
ax[1].pie(segment_df["Segment_Probability"], labels=[f"Segment {s}" for s in segment_df["Segment"]],
          autopct='%1.1f%%', colors=["#66c2a5", "#fc8d62"], startangle=90)
ax[1].set_title("Segment Market Share")

plt.tight_layout()
plt.show()
```
1. Left: Segment preference comparison chart (bar chart)
- Displays the β coefficient of each Segment (potential group) for price (Price) and promotion (Feature)
- Clearly compares the sensitivity of two groups of consumers to different variables

2. Right: Segment market share chart (pie chart)
- Displays the proportion of each potential category in the market (π value)
- Segment 1 is about 50.3%, Segment 2 is about 49.7%

According to the model results, the market can be divided into two potential consumer groups. Segment 1 is very insensitive to promotions and slightly sensitive to prices. It is recommended to adopt a marketing strategy that emphasizes quality and brand value; while Segment 2 is highly sensitive to both prices and promotions. It is suitable to stimulate purchase intention through promotional means such as discounts and gifts. Since the market share of the two groups is almost the same, it is recommended to adopt a dual-track parallel marketing strategy, designing differentiated messages and plans for different groups to increase the overall market penetration rate.

## 2a. K Nearest Neighbors
We want to create a simulated data set for a binary classification problem, with the following features:
- The data has two features (x1, x2)
- The class label y is determined by the boundary of x2 > sin(4x1) + x1 (i.e., the classification above and below a wavy line)

Such data can help us test whether the KNN algorithm can effectively handle classification tasks with "non-linear decision boundaries".

### Generate training data set
We need a set of data to train the KNN model. This set of data will simulate the situation in the real world where the data and boundaries are not linearly separable.
```{python}
import numpy as np
import pandas as pd

# Set random seed
np.random.seed(42)
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
x = np.column_stack((x1, x2))

# Define a wiggly boundary
boundary = np.sin(4 * x1) + x1
y = (x2 > boundary).astype(int)
dat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
```

### Visual training materials and boundaries
Plotting the simulated data set allows us to clearly see the relationship between the distribution of data points and the position of the classification boundary. We randomly generated 100 two-dimensional data (x1, x2), and y was marked as 0 or 1 depending on whether it was above boundary = sin(4 * x1) + x1 (this is a curved "real boundary"). By plotting this boundary and data points, we can visually observe the difficulty of classification.

```{python}
# | code-fold: true
# | code-summary: "Visualizing"
import numpy as np
import pandas as pd

np.random.seed(42)
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
boundary = np.sin(4 * x1) + x1
y = (x2 > boundary).astype(int)

train_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(train_df["x1"], train_df["x2"], c=train_df["y"], cmap='coolwarm', edgecolor='k')
x_line = np.linspace(-3, 3, 300)
plt.plot(x_line, np.sin(4 * x_line) + x_line, color='black', linestyle='--', label="True Boundary")
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Training Data and True Boundary")
plt.legend()
plt.show()
```
Through the graph, we can clearly see how the data points are distributed on both sides of the wiggly boundary, and we can also intuitively judge the difficulty of classification.

### Generate test data sets (different random seeds)
The test set is used to verify the model effect on unseen data. Different seeds must be used to avoid data duplication.
```{python}
np.random.seed(99)
x1_test = np.random.uniform(-3, 3, n)
x2_test = np.random.uniform(-3, 3, n)
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = (x2_test > boundary_test).astype(int)

test_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})
```

### Handwritten KNN Classifier
By implementing KNN yourself, you can deepen your understanding of the logic of neighbor classification: find the k closest points → majority vote.
```{python}
def knn_predict(x_test, X_train, y_train, k):
    y_pred = []
    for x in x_test:
        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))
        neighbors = y_train[np.argsort(distances)[:k]]
        vote = np.bincount(neighbors).argmax()
        y_pred.append(vote)
    return np.array(y_pred)
```

### Compare to sklearn's built-in results
```{python}
# | code-fold: true
# | code-summary: "Compare to sklearn's built-in results"
from sklearn.neighbors import KNeighborsClassifier

np.random.seed(42)
n = 100
x1_train = np.random.uniform(-3, 3, n)
x2_train = np.random.uniform(-3, 3, n)
boundary_train = np.sin(4 * x1_train) + x1_train
y_train = (x2_train > boundary_train).astype(int)
train_df = pd.DataFrame({'x1': x1_train, 'x2': x2_train, 'y': y_train})

np.random.seed(99)
x1_test = np.random.uniform(-3, 3, n)
x2_test = np.random.uniform(-3, 3, n)
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = (x2_test > boundary_test).astype(int)
test_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})

model = KNeighborsClassifier(n_neighbors=5)
model.fit(train_df[["x1", "x2"]], train_df["y"])
acc = model.score(test_df[["x1", "x2"]], test_df["y"])

knn_result_df = pd.DataFrame({
    "Model": ["sklearn KNN"],
    "k": [5],
    "Accuracy": [round(acc * 100, 2)]
})
knn_result_df
```
- We used 5 neighboring points (k=5) to classify the test data
- The prediction accuracy is 90%, which means that the model has good recognition ability for nonlinear boundary data
- This can be used as a comparison benchmark when we implement "custom KNN" or test different k values ​​later

### Accuracy performance for runs k = 1 to 30
In the KNN model, the "k value" represents how many neighboring samples each piece of data needs to refer to for classification. Different k values ​​will have a significant impact on the model's prediction results:
- Too small k value: The model is overly dependent on a single neighbor, easily sensitive to noise, and leads to overfitting
- Too large k value: The model averages too much information, the boundaries become blurred, and underfitting may occur

By testing the performance of the model with k = 1 to 30 one by one, we can choose an optimal k value that makes the classification accuracy most stable and effective.
```{python}
# | code-fold: true
# | code-summary: "Performance Visualization"
accuracies = []
for k in range(1, 31):
    y_pred = knn_predict(test_df[["x1", "x2"]].values, train_df[["x1", "x2"]].values, train_df["y"].values, k)
    acc = np.mean(y_pred == test_df["y"].values)
    accuracies.append(acc)

plt.plot(range(1, 31), np.array(accuracies) * 100, marker='o')
plt.xlabel("k (number of neighbors)")
plt.ylabel("Accuracy (%)")
plt.title("KNN Accuracy vs. k")
plt.grid(True)
plt.show()
```
The horizontal axis of the chart is the number of neighbors k, and the vertical axis is the classification accuracy of the test data (Accuracy %).
- When k = 1 or 2, the accuracy reaches the highest, about 92%
- As k increases, the accuracy shows an overall downward trend
- There are occasional fluctuations in the middle (such as k = 16, 24, the accuracy increases slightly)
- When k > 20, the accuracy is mostly stable at around 86% ~ 88%

## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._






