[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lulu Ling",
    "section": "",
    "text": "Hi!! I am Lulu Ling. This is my marketing analytics project."
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\nLulu Ling\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nLulu Ling\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nLulu Ling\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Project1/index.html",
    "href": "blog/Project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn oreder to explore if the price affects on chartibal giving behavior, Dean Karlan and John conducted a large scale experiment involving about 50,000 donors to a liberal nonprofit organization. The subjects were randomly assigned to two groups: a control group and an experimental group. The control group received a standard fundraising letter without any additional instructions, while the experimental group received a letter containing a matching grant.\nIn this experiment, people are further randomly assigned to different sub-treatment conditions, such as designs of matching ratio, matching amount cap and suggested donation amount. These details will be further described in the data description section. The group of treatment will receive letters included an additional paragraph inserted at the top of the second page that announced that a “concerned fellow member” will match their donation, and the reply card included in bold type the details of the match. For the control group, the reply card match language was replaced with a large logo of the organization.\nThis design allows researchers to not only estimate the average treatment effect, but also to further analyze the impact of different matching ratios, upper limits, and recommended amounts on donation decisions. In addition, the study also observed differential responses in red states and blue states, indicating that the political environment also affects the sensitivity of donation behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/index.html#section-1-data",
    "href": "blog/Project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/Project1/index.html#section-2-analysis",
    "href": "blog/Project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/Project1/index.html#introduction",
    "href": "blog/Project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn oreder to explore if the price affects on chartibal giving behavior, Dean Karlan and John conducted a large scale experiment involving about 50,000 donors to a liberal nonprofit organization. The subjects were randomly assigned to two groups: a control group and an experimental group. The control group received a standard fundraising letter without any additional instructions, while the experimental group received a letter containing a matching grant.\nIn this experiment, people are further randomly assigned to different sub-treatment conditions, such as designs of matching ratio, matching amount cap and suggested donation amount. These details will be further described in the data description section. The group of treatment will receive letters included an additional paragraph inserted at the top of the second page that announced that a “concerned fellow member” will match their donation, and the reply card included in bold type the details of the match. For the control group, the reply card match language was replaced with a large logo of the organization.\nThis design allows researchers to not only estimate the average treatment effect, but also to further analyze the impact of different matching ratios, upper limits, and recommended amounts on donation decisions. In addition, the study also observed differential responses in red states and blue states, indicating that the political environment also affects the sensitivity of donation behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/index.html#data",
    "href": "blog/Project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis data comes from a large-scale natural field experiment conducted by a liberal nonprofit organization in the United States in 2005. The purpose of the study was to explore:\nDo different donation reminder designs affect people’s actual donation behavior?\nTreatment Conditions\n\nPaired ratios: $1:$1, $2:$1, $3:$1, control\nMaximum amount of matching: $25,000 / $50,000 / $100,000 / control\nAsk amount: based on 1.0 times, 1.25 times, or 1.5 times the donor’s highest past donation\n\nSample size and groups\n\nTotal sample size: 50,083 donors\nControl group: 16,687 people (33%)\nTreatment group: 33,396 people (67%)\n\nThe fundraising letter received contains instructions for matching donations and is randomly assigned to different matching ratio/maximum amount/suggested amount combinations\nLoading dataset\n\n\nCode\nimport pandas as pd\ndata = pd.read_stata('/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project1/karlan_list_2007.dta')\ndata.head()\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI will select several non-significant variables to examine whether there are statistically significant differences (95% confidence level) between the experimental and control groups on these background characteristics. Each variable is analyzed using two methods: one is a t-test, and the other is to estimate the effect of treatment on the variable through simple linear regression, and compare whether the two methods give consistent results.\n\nVariable selections\n\nmrm2: Number of months since last donation\nltmedmra: Small prior donor: last gift was less than median $35\ncouple: Couple\n\nSplit the data into treatment and control groups\n\n\nCode\ntreatment_data = data[data['treatment'] == 1]\ncontrol_data = data[data['treatment'] == 0]\n\n\nT-test for mrm2, ltmedmra, couple\n\n\nCode\ncolumns = ['mrm2', 'ltmedmra', 'couple']\n\nt_stats = {}\n\nfor col in columns:\n\n    treatment_values = [x for x in treatment_data[col] if x == x]\n    control_values = [x for x in control_data[col] if x == x]\n\n    n1 = len(treatment_values)\n    n2 = len(control_values)\n\n    mean1 = sum(treatment_values) / n1\n    mean2 = sum(control_values) / n2\n\n    var1 = sum((x - mean1)**2 for x in treatment_values) / (n1 - 1)\n    var2 = sum((x - mean2)**2 for x in control_values) / (n2 - 1)\n\n    se = ((var1 / n1) + (var2 / n2)) ** 0.5\n\n    t_stat = (mean1 - mean2) / se\n\n    t_stats[col] = t_stat\n\n\nLinear regression for mrm2, ltmedmra, couple\n\n\nCode\nimport statsmodels.api as sm\n\nif 'intercept' not in data.columns:\n    data['intercept'] = 1\n\ncolumns_to_analyze = ['mrm2', 'ltmedmra', 'couple']\n\nregression_results = {}\n\nfor col in columns_to_analyze:\n    model = sm.OLS(data[col], data[['intercept', 'treatment']], missing='drop').fit()\n    t_stat = model.tvalues['treatment'].round(4)\n    p_value = model.pvalues['treatment'].round(4)\n    regression_results[col] = {'t-stat': t_stat, 'p-value': p_value}\n\n\nThe result of t-test and linear regression for mrm2, ltmedmra, couple\n\n\nCode\ncombined_t_stats_df = pd.DataFrame({\n    \"Variable\": list(t_stats.keys()) + list(regression_results.keys()),\n    \"T-statistic\": [round(value, 4) for value in t_stats.values()] + [result['t-stat'] for result in regression_results.values()],\n    \"Method\": [\"T-test\"] * len(t_stats) + [\"Regression\"] * len(regression_results)\n})\ncombined_t_stats_df\n\n\n\n\n\n\n\n\n\nVariable\nT-statistic\nMethod\n\n\n\n\n0\nmrm2\n0.1195\nT-test\n\n\n1\nltmedmra\n1.9099\nT-test\n\n\n2\ncouple\n-0.5823\nT-test\n\n\n3\nmrm2\n0.1195\nRegression\n\n\n4\nltmedmra\n1.9097\nRegression\n\n\n5\ncouple\n-0.5838\nRegression\n\n\n\n\n\n\n\nFirst, from the results of t-test:\n\nThe t-value of mrm2 was 0.1195, indicating that there was no significant difference between the treatment group and the control group.\nThe t-value of ltmedmra is 1.9099, which is close to the statistically significant level (usually the critical value is about 1.96), indicating that the difference between the treatment group and the control group in this variable is potentially significant.\nThe T value of couple was -0.5823, indicating that there was no significant difference in this variable between the two groups.\n\nNext, we further verified the responses of these variables to the treatment effects through linear regression. In regression analysis, we treat each variable as a dependent variable and the treatment variable (treatment) as an independent variable, and observe the estimated value of its coefficient and the T statistic:\n\nThe treatment coefficient t-value of mrm2 is still 0.1195, which is consistent with the T test, indicating that the treatment has no effect on this variable.\nThe regression t-value of ltmedmra is 1.9097, which is almost consistent with the T-test, further strengthening the inference that this variable may be affected by the treatment.\nThe regression t-value of couple is -0.5838, which is also close to the T-test, indicating that the treatment has no significant effect.\n\nThese results can be compared with Table 1 in the paper by Karlan and List. This table mainly presents the average values ​​and differences of various basic characteristics between the treatment group and the control group, with the aim of verifying whether the random assignment is successful. If there is no significant difference between the two groups on most variables, it can be reasonably inferred that the sample allocation is random, and subsequent causal inferences are more credible."
  },
  {
    "objectID": "blog/Project1/index.html#experimental-results",
    "href": "blog/Project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nBar plot of the proportion of peole donated between treatment and controal group.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ntreatment_prop = treatment_data['gave'].mean()\ncontrol_prop = control_data['gave'].mean()\n\nplt.bar(['Treatment', 'Control'], [treatment_prop, control_prop], color=['pink', 'lightblue'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors in Treatment and Control')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe purpose of the following analysis is to compare the differences between the experimental group and the control group in terms of whether or not they donated. I will first use a t-test to preliminarily check whether there is a significant difference in the donation rates of the two groups, and then use a simple linear regression model with donation behavior as the dependent variable and the experimental treatment as the independent variable to further verify whether the results are consistent. These results will help us determine whether paired donation reminders can effectively enhance donation behavior and explore donors’ behavioral responses and potential psychological motivations. In addition, the data will be compared with Table 2A in the paper to confirm the consistency of the analysis direction with the original research.\nT test: compare whether there is a significant difference in the donation rate between the treatment and control groups\n\n\nCode\nfrom scipy import stats\ntreatment_gave = treatment_data['gave']\ncontrol_gave = control_data['gave']\n\nmean_treatment = treatment_gave.mean()\nmean_control = control_gave.mean()\n\nvar_treatment = treatment_gave.var(ddof=1)\nvar_control = control_gave.var(ddof=1)\n\nn_treatment = len(treatment_gave)\nn_control = len(control_gave)\n\nse = ((var_treatment / n_treatment) + (var_control / n_control)) ** 0.5\n\nt_stat_manual = (mean_treatment - mean_control) / se\n\ndf = ((var_treatment / n_treatment + var_control / n_control) ** 2) / \\\n    (((var_treatment / n_treatment) ** 2) / (n_treatment - 1) + ((var_control / n_control) ** 2) / (n_control - 1))\n\np_value_manual = 2 * (1 - stats.t.cdf(abs(t_stat_manual), df))\n\ngave_t_test_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_manual.round(4), p_value_manual.round(4)]\n})\n\ngave_t_test_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n3.2095\n\n\n1\np-value\n0.0013\n\n\n\n\n\n\n\nWe first conducted an independent sample t-test on the binary variable gave. The results showed that the t-value and the p-value indicating that the difference in donation rates between the treatment group and the control group was statistically significant at a 95% confidence level. This suggests that simply including the phrase “your donation will be matched” in your fundraising email can significantly increase donation rates.\nLinear regression: Using OLS to test the effect of treatment on donation behavior\n\n\nCode\ngave_model = sm.OLS(data['gave'], data[['intercept', 'treatment']], missing='drop').fit()\n\ngave_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [gave_model.tvalues['treatment'].round(4), gave_model.pvalues['treatment'].round(4)]\n})\ngave_model_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n3.1014\n\n\n1\np-value\n0.0019\n\n\n\n\n\n\n\nTo verify this, we used simple linear regression with gave as the dependent variable and treatment as the independent variable. The results showed that the t-value and p-value were almost consistent with the t-test results, proving that the two methods are consistent when analyzing this type of binary outcome variable.\nThe proportion of respsonse rate in control gorup and treatment group\n\n\nCode\nproportions_df = pd.DataFrame({\n    \"Group\": [\"Control\", \"Treatment\"],\n    \"Proportion\": [control_prop, treatment_prop]\n})\nproportions_df\n\n\n\n\n\n\n\n\n\nGroup\nProportion\n\n\n\n\n0\nControl\n0.017858\n\n\n1\nTreatment\n0.022039\n\n\n\n\n\n\n\nThis result is also consistent with the data in Table 2A of the original text (1.8% in the control group and 2.2% in the experimental group). From a behavioral economics perspective, this stable difference may be because when people see the message that “your donation will be matched,” they feel that their donation is more valuable and more influential. This feeling will make them more willing to donate. It’s like people feel a sense of satisfaction when they donate, and the message of matching donations makes this satisfaction even stronger, thus increasing their willingness to act.\nOverall, this analysis supports the original authors’ conclusion: even without changing the amount, providing matching information can effectively increase the likelihood of donations, which has important implications for practical fundraising strategies.\nNext, I will conduct a Probit regression analysis to test the impact of “whether or not to receive a matching donation reminder” (treatment) on the outcome of “whether or not to donate” (gave, a variable of 0 or 1). This model can help you estimate the effect of the matching message on the probability of donating and can be used to verify whether your results are consistent with the analysis results in column 1 of Table 3 of the paper. This step is to confirm whether you have successfully reproduced the main conclusions of the original study.\nProbit Regression: Estimating the Effect of Pairing Prompts on the Probability of Donating\n\n\nCode\nprobit_model = sm.Probit(data['gave'], data[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\n\ncoefficients = probit_results.params\nt_values = probit_results.tvalues\n\nprobit_summary_df = pd.DataFrame({\n    \"Variable\": coefficients.index,\n    \"Coefficient\": coefficients.values,\n    \"T-value\": t_values.values\n})\nprobit_summary_df.round(4)\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nT-value\n\n\n\n\n0\nintercept\n-2.1001\n-90.0728\n\n\n1\ntreatment\n0.0868\n3.1129\n\n\n\n\n\n\n\nMarginal effect analysis: explaining the actual effect of treatment on the probability of donating\n\n\nCode\nmarginal_effects = probit_results.get_margeff()\nmarginal_summary = marginal_effects.summary_frame()\n\nmarginal_summary = marginal_summary.reset_index().rename(columns={\n    'index': 'Variable',\n    'dy/dx': 'Marginal Effect (dy/dx)',\n    'Std. Err.': 'Std. Error',\n    'z': 'z',\n    'P&gt;|z|': 'P-value',\n    '[0.025': 'CI Lower',\n    '0.975]': 'CI Upper'\n})\n\nmarginal_summary.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nMarginal Effect (dy/dx)\nStd. Error\nz\nPr(&gt;|z|)\nConf. Int. Low\nCont. Int. Hi.\n\n\n\n\n0\ntreatment\n0.0043\n0.0014\n3.1044\n0.0019\n0.0016\n0.007\n\n\n\n\n\n\n\nThe results are completely consistent with column 1 of Table 3 , successfully replicating the reported analysis. This means that the pairing prompt can significantly increase the probability of people donating, and even if the effect is small, it is statistically stable and significant. In the Probit model, the original coefficient cannot be directly interpreted as “how much the donation rate increased”, but it can be converted into a marginal effect. We can see from the Probit marginal effect model that the result is 0.0043, which corresponds exactly to 0.004 in the first column of Table 3.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nThrough t-test, we examine whether different matching ratios affect people’s donation behavior. Specifically, you will compare whether there are statistically significant differences in the donation rates of subjects under the 1:1, 2:1, and 3:1 pairing conditions. This will help you assess whether a higher or lower matching ratio has an additional impact on donation willingness.\nT-tests for match ratio effects on donation\n\n\nCode\nr1 = treatment_data[treatment_data['ratio'] == 1]\nr2 = treatment_data[treatment_data['ratio'] == 2]\nr3 = treatment_data[treatment_data['ratio'] == 3]\n\nmean_r1 = r1['gave'].mean()\nmean_r2 = r2['gave'].mean()\nmean_r3 = r3['gave'].mean()\n\nvar_r1 = r1['gave'].var(ddof=1)\nvar_r2 = r2['gave'].var(ddof=1)\nvar_r3 = r3['gave'].var(ddof=1)\n\nn_r1 = len(r1['gave'])\nn_r2 = len(r2['gave'])\nn_r3 = len(r3['gave'])\n\nse_1v2 = ((var_r1 / n_r1) + (var_r2 / n_r2)) ** 0.5\nse_2v3 = ((var_r2 / n_r2) + (var_r3 / n_r3)) ** 0.5\n\nt_stat_1v2 = (mean_r1 - mean_r2) / se_1v2\nt_stat_2v3 = (mean_r2 - mean_r3) / se_2v3\n\ndf_1v2 = ((var_r1 / n_r1 + var_r2 / n_r2) ** 2) / \\\n         (((var_r1 / n_r1) ** 2) / (n_r1 - 1) + ((var_r2 / n_r2) ** 2) / (n_r2 - 1))\ndf_2v3 = ((var_r2 / n_r2 + var_r3 / n_r3) ** 2) / \\\n         (((var_r2 / n_r2) ** 2) / (n_r2 - 1) + ((var_r3 / n_r3) ** 2) / (n_r3 - 1))\n\np_value_1v2 = 2 * (1 - stats.t.cdf(abs(t_stat_1v2), df_1v2))\np_value_2v3 = 2 * (1 - stats.t.cdf(abs(t_stat_2v3), df_2v3))\n\nt_test_results_df = pd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"2:1 vs 3:1\"],\n    \"T-statistic\": [t_stat_1v2, t_stat_2v3],\n    \"P-value\": [p_value_1v2, p_value_2v3]\n})\n\nt_test_results_df\n\n\n\n\n\n\n\n\n\nComparison\nT-statistic\nP-value\n\n\n\n\n0\n1:1 vs 2:1\n-0.965049\n0.334531\n\n\n1\n2:1 vs 3:1\n-0.050116\n0.960031\n\n\n\n\n\n\n\nIn the paired donation prompt group, there were no significant behavioral differences between the different pairing ratios (1:1, 2:1, and 3:1).The t-value of the 1:1 and 2:1 groups is -0.965, and the p-value is 0.3345, indicating that we cannot reject the null hypothesis and there is no statistically significant difference in the donation rates between the two groups. The difference between the 2:1 and 3:1 groups is even smaller, with a t-value of only -0.0501 and a corresponding p-value of 0.96, indicating that there is no difference in donation behavior between the two groups.\nThe analysis results of the t-test support the author’s observations on page 8 of the paper. The authors note that while the pairing prompt itself increased donation rates, further increasing the pairing ratio (from 1:1 to 2:1 or 3:1) in the pairing prompt group did not lead to additional effects. The t-test you conducted also clearly reflects this point: the difference in donation rates between different matching ratios is not statistically significant, and the p-values ​​are all far higher than the traditional significance level, especially the difference between 2:1 and 3:1 is almost zero. This shows that in actual donation behavior, people are more sensitive to whether there is a match rather than the size of the matching ratio.\nRegression analysis was used to assess the effects of different pairing ratios (1:1, 2:1, 3:1) on donation behavior. The specific approach is to establish a linear regression model, with gave (whether to donate) as the dependent variable and three dummy variables representing the pairing ratios (ratio1, ratio2, ratio3) as independent variables. This allows us to simultaneously compare the effects of each pairing condition on the donation rate and analyze the regression coefficient of each variable and its statistical significance. Using this model, you will be able to determine whether a particular pairing ratio is particularly effective and whether the results are explanatory and stable.\nRegression analysis for match ratio effects\n\n\nCode\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\nratio_model = sm.OLS(data['gave'], data[['intercept', 'ratio1', 'ratio2', 'ratio3']], missing='drop').fit()\n\n\nAccording to the regression results, we observed that different pairing ratios do have an impact on donation behavior, but the strength of the effect varies. The donation rate for ratio1 is about 0.29 percentage points higher, which is positive but only slightly statistically significant. The effects of the paired groups of ratio2 and ratio3 are more obvious, with the donation rates being approximately 0.48 and 0.49 percentage points higher than the benchmark group, respectively, and are significant at the 1% significance level.\nThis means that as long as there is matching information, even ratio 1 may increase people’s willingness to donate, and increasing the matching ratio to ratio 2 or ratio 3 will further strengthen this incentive. However, the effects of ratio2 and ratio3 are similar and almost the same, indicating that the marginal benefit of increasing the pairing ratio tends to be flat or saturated. This is consistent with the authors’ observation in the paper that higher pairing ratios do not necessarily produce additional significant behavioral changes.\nResponse rate differences between different matching ratio\n\n\nCode\nresp_rate_1 = r1['gave'].mean()\nresp_rate_2 = r2['gave'].mean()\nresp_rate_3 = r3['gave'].mean()\n\ndiff_1v2 = resp_rate_2 - resp_rate_1\ndiff_2v3 = resp_rate_3 - resp_rate_2\n\nprint(f\"Response rate difference 1:1 vs 2:1: {diff_1v2:.4f}\")\nprint(f\"Response rate difference 2:1 vs 3:1: {diff_2v3:.4f}\")\n\n\nResponse rate difference 1:1 vs 2:1: 0.0019\nResponse rate difference 2:1 vs 3:1: 0.0001\n\n\nThe difference between the donation rates is very small, and further increasing the matching ratio, for example from ratio1 to ratio2 or ratio3, has very limited effect on the donation rate.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI will use a t-test to exclude missing values ​​from the two groups and then compare whether there is a statistically significant difference in the average donation amounts of the two groups. This is a statistical method commonly used to compare whether two groups of means are different. A simple linear regression model was established, with the donation amount as the dependent variable and the treatment status as the explanatory variable, to examine whether there was a significant difference in the donation amount between the experimental group and the control group before controlling other variables.\nT-test for donation amount\n\n\nCode\ntreatment_amount = treatment_data['amount'].dropna()\ncontrol_amount = control_data['amount'].dropna()\n\nmean_treatment_amount = treatment_amount.mean()\nmean_control_amount = control_amount.mean()\n\nvar_treatment_amount = treatment_amount.var(ddof=1)\nvar_control_amount = control_amount.var(ddof=1)\n\nn_treatment_amount = len(treatment_amount)\nn_control_amount = len(control_amount)\n\nse_amount = ((var_treatment_amount / n_treatment_amount) + (var_control_amount / n_control_amount)) ** 0.5\n\nt_stat_amount_manual = (mean_treatment_amount - mean_control_amount) / se_amount\n\ndf_amount = ((var_treatment_amount / n_treatment_amount + var_control_amount / n_control_amount) ** 2) / \\\n    (((var_treatment_amount / n_treatment_amount) ** 2) / (n_treatment_amount - 1) + \n     ((var_control_amount / n_control_amount) ** 2) / (n_control_amount - 1))\n\np_value_amount_manual = 2 * (1 - stats.t.cdf(abs(t_stat_amount_manual), df_amount))\n\nt_test_amount_manual_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_amount_manual.round(4), p_value_amount_manual.round(4)]\n})\n\nt_test_amount_manual_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n1.9182\n\n\n1\np-value\n0.0551\n\n\n\n\n\n\n\nBivariate linear regression for donation amount\n\n\nCode\namount_model = sm.OLS(data['amount'], data[['intercept', 'treatment']], missing='drop').fit()\n\namount_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"], \n    \"Value\": [amount_model.tvalues['treatment'].round(4), amount_model.pvalues['treatment'].round(4)]\n})\namount_model_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n1.8605\n\n\n1\np-value\n0.0628\n\n\n\n\n\n\n\nWhen we analyzed the donation amount, both the independent sample t-test and the bivariate linear regression showed that the average donation amount of the treatment group was slightly higher than that of the control group, but the difference was only marginally significant. The p-value of the t-test is 0.0551, and the p-value of the regression is 0.063, both slightly higher than the traditional 5% significance level.\nOverall, the matching prompt has a clear impact on whether to donate, while the impact of amount is weaker. From a behavioral perspective, the matching message is more like a “motivation switch” that prompts people to take action rather than a reinforcement tool that influences the amount of donations. This also means that in terms of fundraising strategy, matching donations are more suitable as an incentive to guide donation behavior rather than a means to increase the single amount.\nNext, we will conduct a regression analysis on those who actually donated to assess whether the paired prompt (treatment) affects the amount they donated. First, the program will filter out all observations with donation amounts greater than 0 from the data, and then build a simple linear regression model with the donation amount as the dependent variable and whether or not the matching prompt was received as the independent variable.\nConditional donation amount regression analysis: evaluating the impact and explanatory power of matching prompts only for actual donors\n\n\nCode\ndonors = data[data['amount'] &gt; 0]\n\ndonors_model = sm.OLS(donors['amount'], donors[['intercept', 'treatment']], missing='drop').fit()\n\ntreatment_coef = donors_model.params['treatment']\n\ntreatment_coef_df = pd.DataFrame({\n    \"Metric\": [\"Treatment Coefficient\"],\n    \"Value\": [treatment_coef.round(4)]\n})\n\ntreatment_coef_df\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nTreatment Coefficient\n-1.6684\n\n\n\n\n\n\n\nIn this regression analysis of those who have already donated, the treatment coefficient is -1.6684, it means that the average donation amount of the treatment group is about 1.67 yuan lower than that of the control group., however, this result is not statistically significant because the p-value is only 0.561, indicating that the matching prompt has no stable effect on the amount donated by those who have already decided to donate. It should be noted that this regression result cannot be interpreted as a causal effect of treatment on the amount of donations, because the analysis is limited to people who actually donated. This is a conditional subsample and not a random assignment, so there is a risk of selection bias. Taken together, our findings suggest that matching donation prompts are more likely to influence the behavior of whether to donate rather than the amount of donation.\nCompare the distribution of donation amounts among those who actually donated in the treatment group and the control group. First, the program will screen out the subjects in the two groups whose donation amount is greater than 0, and then calculate the average donation amount of each group.\nCompare the distribution of donations between treatment and control groups (limited to donors)\n\n\nCode\ntreatment_donors = treatment_data[treatment_data['amount'] &gt; 0]\ncontrol_donors = control_data[control_data['amount'] &gt; 0]\n\ntreatment_avg = treatment_donors['amount'].mean()\ncontrol_avg = control_donors['amount'].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\naxes[0].hist(treatment_donors['amount'], bins=30, color='skyblue', alpha=0.7)\naxes[0].axvline(treatment_avg, color='red', linestyle='--', label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group: Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\naxes[1].hist(control_donors['amount'], bins=30, color='pink', alpha=0.7)\naxes[1].axvline(control_avg, color='red', linestyle='--', label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group: Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/Project1/index.html#simulation-experiment",
    "href": "blog/Project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe Law of Large Numbers is demonstrated through a simulation. 100,000 records are simulated from the control group (donation rate 1.8%) and 10,000 records are simulated from the treatment group (donation rate 2.2%). Then, the same number of samples are randomly selected from the control data to pair with the treatment data. The difference between the treatment and control donation results is calculated for each\nSimulation of the Law of Large Numbers: Cumulative Average Difference in Donation Rates\n\n\nCode\nimport numpy as np\n#calculation\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(n=1, p=0.018, size=100000)\ntreatment_draws = np.random.binomial(n=1, p=0.022, size=10000)\ncontrol_sample = np.random.choice(control_draws, size=10000, replace=False)\ndiff = treatment_draws - control_sample\ncumulative_avg = np.cumsum(diff) / np.arange(1, len(diff) + 1)\n#plot\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative mean difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label=\"Theoretical mean difference = 0.004\")\nplt.title(\"Simulation: Cumulative Average Difference in Donation Rates)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis graph shows the difference in donation rates between the treatment group and the control group, calculated through simulations, as the number of samples increases. As can be seen from the figure, although the difference fluctuates greatly when the number of samples is small, as the number of samples gradually increases, the cumulative average curve steadily approaches the theoretical true difference value of 0.004.\nThis is a typical manifestation of the Law of Large Numbers: when we observe enough samples, the mean of the samples will approach the true mean of the population. This also means that the difference in donation rates observed in the original experiment (the slightly higher donation rate in the treatment group than in the control group) was not caused by random errors, but was a stable and reproducible result.\nTherefore, we can reasonably say that the simulation results in this figure verify the stability and credibility of the treatment effect and provide strong visual evidence to support that the observations in the experiment are reliable.\n\n\nCentral Limit Theorem\nNext, we will show the distribution of the average difference in donation rates between the treatment group and the control group under different sample sizes (50, 200, 500, 1000). For each sample size, 1000 random draws were made, taking an equal number of samples from the simulated distributions for both treatment and control, calculating the mean differences between the two groups, and plotting these differences in a histogram.\n\n\nCode\nimport numpy as np\n\ncontrol_draws = np.random.binomial(1, 0.018, 100000)\ntreatment_draws = np.random.binomial(1, 0.022, 10000)\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n\n    for _ in range(1000):\n        treatment_sample = np.random.choice(treatment_draws, size=sample_size, replace=True)\n        control_sample = np.random.choice(control_draws, size=sample_size, replace=True)\n        avg_differences.append(np.mean(treatment_sample - control_sample))\n\n    axes[i].hist(avg_differences, bins=30, color='pink', edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese four histograms show the simulated distribution changes of the difference in donation rates between the treatment group and the control group under different sample sizes (50, 200, 500, 1000). When the sample size is small, the distribution is more dispersed, and 0 almost falls in the center, which means that it is impossible to determine whether the treatment effect is significant. However, as the number of samples increases, the distribution begins to become concentrated and biased toward positive differences, especially when the number of samples reaches 500 or 1000, when 0 is clearly off the center and falls in the left tail of the distribution. This means that when the sample size is sufficient, the treatment group does show a stable and positive effect, and the average donation rate is higher than that of the control group. This difference is unlikely to be caused by random errors. Overall, this set of charts reinforces a basic principle in statistical inference: the larger the sample size, the more stable the results and the more reliably they reveal true behavioral differences."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html",
    "href": "blog/Project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#introduction",
    "href": "blog/Project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#data",
    "href": "blog/Project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#experimental-results",
    "href": "blog/Project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#simulation-experiment",
    "href": "blog/Project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/Project1/project1.html",
    "href": "blog/Project1/project1.html",
    "title": "Lulu's Marketing Analytics Project",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\n\n\ndata = pd.read_stata('/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project1/karlan_list_2007.dta')\ndata\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\ndata.to_excel('output_data.xlsx', index=False)\n\n\ntreatment_data = data[data['treatment'] == 1]\n\n\ncontrol_data = data[data['treatment'] == 0]\n\n\nfrom scipy.stats import ttest_ind\n\n# T-test for mrm2\ntreatment_mrm2 = treatment_data['mrm2'].dropna()\ncontrol_mrm2 = control_data['mrm2'].dropna()\nt_stat, p_value = ttest_ind(treatment_mrm2, control_mrm2, equal_var=False)\nprint(f\"T-test for mrm2: t-statistic = {t_stat.round(4)}, p-value = {p_value.round(4)}\")\n\nT-test for mrm2: t-statistic = 0.1195, p-value = 0.9049\n\n\n\n# 去除缺漏值\ntreatment_mrm2 = [x for x in treatment_data['mrm2'] if x == x]\ncontrol_mrm2 = [x for x in control_data['mrm2'] if x == x]\n\n# 計算樣本數\nn1 = len(treatment_mrm2)\nn2 = len(control_mrm2)\n\n# 計算平均\nmean1 = sum(treatment_mrm2) / n1\nmean2 = sum(control_mrm2) / n2\n\n# 計算變異數（無偏估計，分母用 n-1）\nvar1 = sum((x - mean1)**2 for x in treatment_mrm2) / (n1 - 1)\nvar2 = sum((x - mean2)**2 for x in control_mrm2) / (n2 - 1)\n\n# 計算標準誤\nse = ((var1 / n1) + (var2 / n2)) ** 0.5\n\n# 計算 t 統計量\nt_stat = (mean1 - mean2) / se\n\nprint(f\"T-test (by formula) for mrm2: t-statistic = {round(t_stat, 4)}\")\n\nT-test (by formula) for mrm2: t-statistic = 0.1195\n\n\n\nimport statsmodels.api as sm\n\n# Linear regression for mrm2\ndata['intercept'] = 1 \nmodel = sm.OLS(data['mrm2'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(model.summary())\n# Extract the t-statistic for the 'treatment' coefficient from the regression model\nregression_t_stat = model.tvalues['treatment'].round(4)\nprint(f\"Regression model t-statistic: {regression_t_stat}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):              0.905\nTime:                        16:22:47   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nRegression model t-statistic: 0.1195\n\n\n\nimport matplotlib.pyplot as plt\n\ntreatment_proportion = treatment_data['gave'].mean()\ncontrol_proportion = control_data['gave'].mean()\n\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['pink', 'lightblue'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n# T-test for the binary outcome 'gave'\ntreatment_gave = treatment_data['gave']\ncontrol_gave = control_data['gave']\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\nprint(f\"T-test for 'gave': t-statistic = {t_stat_gave.round(4)}, p-value = {p_value_gave.round(4)}\")\n\n# Bivariate linear regression for 'gave'\ngave_model = sm.OLS(data['gave'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(gave_model.summary())\nprint(f\"Linear regression for 'gave': t-statistic = {gave_model.tvalues['treatment'].round(4)}\")\n\nT-test for 'gave': t-statistic = 3.2095, p-value = 0.0013\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        16:22:47   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLinear regression for 'gave': t-statistic = 3.1014\n\n\n\nprint(f\"Control Proportion: {control_proportion:.3f}\")\nprint(f\"Treatment Proportion: {treatment_proportion:.3f}\")\n\nControl Proportion: 0.018\nTreatment Proportion: 0.022\n\n\n\n# Probit regression for charitable donation\nprobit_model = sm.Probit(data['gave'], data[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n# 邊際效應估計\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 22 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        00:08:21   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Task 1: T-tests for match ratio effects on donation likelihood\n\n# Filter data for each match ratio\nratio1_data = treatment_data[treatment_data['ratio'] == 1]\nratio2_data = treatment_data[treatment_data['ratio'] == 2]\nratio3_data = treatment_data[treatment_data['ratio'] == 3]\n\n# Perform t-tests\nt_stat_1v2, p_value_1v2 = ttest_ind(ratio1_data['gave'], ratio2_data['gave'], equal_var=False)\nt_stat_2v3, p_value_2v3 = ttest_ind(ratio2_data['gave'], ratio3_data['gave'], equal_var=False)\n\nprint(f\"T-test 1:1 vs 2:1 - t-statistic: {t_stat_1v2:.4f}, p-value: {p_value_1v2:.4f}\")\nprint(f\"T-test 2:1 vs 3:1 - t-statistic: {t_stat_2v3:.4f}, p-value: {p_value_2v3:.4f}\")\n\n\n\n\nT-test 1:1 vs 2:1 - t-statistic: -0.9650, p-value: 0.3345\nT-test 2:1 vs 3:1 - t-statistic: -0.0501, p-value: 0.9600\n\n\n\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\nratio_model = sm.OLS(data['gave'], data[['intercept', 'ratio1', 'ratio2', 'ratio3']], missing='drop').fit()\nprint(ratio_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):             0.0118\nTime:                        11:02:58   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n# Task 3: Response rate differences\n# Direct calculation from data\nresponse_rate_1 = ratio1_data['gave'].mean()\nresponse_rate_2 = ratio2_data['gave'].mean()\nresponse_rate_3 = ratio3_data['gave'].mean()\n\ndiff_1v2 = response_rate_2 - response_rate_1\ndiff_2v3 = response_rate_3 - response_rate_2\n\nprint(f\"Response rate difference 1:1 vs 2:1: {diff_1v2:.4f}\")\nprint(f\"Response rate difference 2:1 vs 3:1: {diff_2v3:.4f}\")\n\nResponse rate difference 1:1 vs 2:1: 0.0019\nResponse rate difference 2:1 vs 3:1: 0.0001\n\n\n\n# Differences from regression coefficients\ncoef_diff_1v2 = ratio_model.params['ratio2'] - ratio_model.params['ratio1']\ncoef_diff_2v3 = ratio_model.params['ratio3'] - ratio_model.params['ratio2']\n\nprint(f\"Coefficient difference 1:1 vs 2:1: {coef_diff_1v2:.4f}\")\nprint(f\"Coefficient difference 2:1 vs 3:1: {coef_diff_2v3:.4f}\")\n\nCoefficient difference 1:1 vs 2:1: 0.0019\nCoefficient difference 2:1 vs 3:1: 0.0001\n\n\n\n# T-test for donation amount\ntreatment_amount = treatment_data['amount'].dropna()\ncontrol_amount = control_data['amount'].dropna()\nt_stat_amount, p_value_amount = ttest_ind(treatment_amount, control_amount, equal_var=False)\nprint(f\"T-test for donation amount: t-statistic = {t_stat_amount:.4f}, p-value = {p_value_amount:.4f}\")\n\n# Bivariate linear regression for donation amount\namount_model = sm.OLS(data['amount'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(amount_model.summary())\nprint(f\"Linear regression for donation amount: t-statistic = {amount_model.tvalues['treatment']:.4f}, p-value = {amount_model.pvalues['treatment']:.4f}\")\n\nT-test for donation amount: t-statistic = 1.9183, p-value = 0.0551\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        16:22:48   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLinear regression for donation amount: t-statistic = 1.8605, p-value = 0.0628\n\n\n\n# Filter data to include only those who made a donation\ndonors_data = data[data['amount'] &gt; 0]\n\n# Regression analysis for donation amount conditional on donating\ndonors_model = sm.OLS(donors_data['amount'], donors_data[['intercept', 'treatment']], missing='drop').fit()\nprint(donors_model.summary())\n\n# Interpretation of the treatment coefficient\ntreatment_coef = donors_model.params['treatment']\nprint(f\"Treatment coefficient: {treatment_coef:.4f}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):              0.561\nTime:                        16:22:48   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nTreatment coefficient: -1.6684\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 模擬參數\nn_control = 100000\nn_treatment = 10000\np_control = 0.018\np_treatment = 0.022\n\n# 隨機抽樣\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(n=1, p=p_control, size=n_control)\ntreatment_draws = np.random.binomial(n=1, p=p_treatment, size=n_treatment)\n\n# 從控制組中隨機抽出與 treatment 組一樣多的樣本\ncontrol_sample = np.random.choice(control_draws, size=n_treatment, replace=False)\n\n# 計算逐筆差異\ndifferences = treatment_draws - control_sample\n\n# 計算累積平均\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# 繪製圖形\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative mean difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label=\"Theoretical mean difference = 0.004\")\nplt.title(\"Simulation: Cumulative Average Difference in Donation Rates)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Filter data for donors only\ntreatment_donors = treatment_data[treatment_data['amount'] &gt; 0]\ncontrol_donors = control_data[control_data['amount'] &gt; 0]\n\n# Calculate sample averages\ntreatment_avg = treatment_donors['amount'].mean()\ncontrol_avg = control_donors['amount'].mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donors['amount'], bins=30, color='skyblue', alpha=0.7)\naxes[0].axvline(treatment_avg, color='red', linestyle='--', label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group: Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donors['amount'], bins=30, color='pink', alpha=0.7)\naxes[1].axvline(control_avg, color='red', linestyle='--', label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group: Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Initialize a figure for subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\n# Loop through each sample size\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n    \n    # Simulate 1000 averages\n    for _ in range(1000):\n        treatment_sample = np.random.choice(treatment_draws, size=sample_size, replace=True)\n        control_sample = np.random.choice(control_draws, size=sample_size, replace=True)\n        avg_differences.append(np.mean(treatment_sample - control_sample))\n    \n    # Plot histogram\n    axes[i].hist(avg_differences, bins=30, color='pink', edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/Project2/project1.html",
    "href": "blog/Project2/project1.html",
    "title": "Lulu's Marketing Analytics Project",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\n\n\ndata = pd.read_stata('/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project1/karlan_list_2007.dta')\ndata\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\ndata.to_excel('output_data.xlsx', index=False)\n\n\ntreatment_data = data[data['treatment'] == 1]\n\n\ncontrol_data = data[data['treatment'] == 0]\n\n\n# Define the columns to calculate t-statistics for\ncolumns = ['mrm2', 'ltmedmra', 'couple']\n\n# Initialize a dictionary to store t-statistics\nt_stats = {}\n\n# Loop through each column\nfor col in columns:\n    # Filter non-NaN values for treatment and control groups\n    treatment_values = [x for x in treatment_data[col] if x == x]\n    control_values = [x for x in control_data[col] if x == x]\n\n    # Calculate sample sizes\n    n1 = len(treatment_values)\n    n2 = len(control_values)\n\n    # Calculate means\n    mean1 = sum(treatment_values) / n1\n    mean2 = sum(control_values) / n2\n\n    # Calculate variances\n    var1 = sum((x - mean1)**2 for x in treatment_values) / (n1 - 1)\n    var2 = sum((x - mean2)**2 for x in control_values) / (n2 - 1)\n\n    # Calculate standard error\n    se = ((var1 / n1) + (var2 / n2)) ** 0.5\n\n    # Calculate t-statistic\n    t_stat = (mean1 - mean2) / se\n\n    # Store the t-statistic in the dictionary\n    t_stats[col] = t_stat\n\n\nimport statsmodels.api as sm\n\n# 確保資料中有 'intercept' 欄位\nif 'intercept' not in data.columns:\n    data['intercept'] = 1\n\n# 定義要計算 t-stat 的欄位\ncolumns_to_analyze = ['mrm2', 'ltmedmra', 'couple']\n\n# 初始化一個字典來儲存結果\nregression_results = {}\n\n# 迴圈計算每個欄位的 t-stat 和 p-value\nfor col in columns_to_analyze:\n    model = sm.OLS(data[col], data[['intercept', 'treatment']], missing='drop').fit()\n    t_stat = model.tvalues['treatment'].round(4)\n    p_value = model.pvalues['treatment'].round(4)\n    regression_results[col] = {'t-stat': t_stat, 'p-value': p_value}\n\n\n# Combine T-test and Regression results into a single DataFrame\ncombined_t_stats_df = pd.DataFrame({\n    \"Variable\": list(t_stats.keys()) + list(regression_results.keys()),\n    \"T-statistic\": [round(value, 4) for value in t_stats.values()] + [result['t-stat'] for result in regression_results.values()],\n    \"Method\": [\"T-test\"] * len(t_stats) + [\"Regression\"] * len(regression_results)\n})\n\nprint(combined_t_stats_df)\n\n   Variable  T-statistic      Method\n0      mrm2       0.1195      T-test\n1  ltmedmra       1.9099      T-test\n2    couple      -0.5823      T-test\n3      mrm2       0.1195  Regression\n4  ltmedmra       1.9097  Regression\n5    couple      -0.5838  Regression\n\n\n\nimport matplotlib.pyplot as plt\n\ntreatment_proportion = treatment_data['gave'].mean()\ncontrol_proportion = control_data['gave'].mean()\n\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['pink', 'lightblue'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\ntreatment_gave = treatment_data['gave']\ncontrol_gave = control_data['gave']\n\n\n# T-test for the binary outcome 'gave'\nfrom scipy.stats import ttest_ind\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\nprint(f\"T-test for 'gave': t-statistic = {t_stat_gave.round(4)}, p-value = {p_value_gave.round(4)}\")\n\n# Bivariate linear regression for 'gave'\ngave_model = sm.OLS(data['gave'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(gave_model.summary())\nprint(f\"Linear regression for 'gave': t-statistic = {gave_model.tvalues['treatment'].round(4)}\")\n\nT-test for 'gave': t-statistic = 3.2095, p-value = 0.0013\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        15:01:13   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLinear regression for 'gave': t-statistic = 3.1014\n\n\n\nfrom scipy import stats\n\n# Calculate means\nmean_treatment = treatment_gave.mean()\nmean_control = control_gave.mean()\n\n# Calculate variances\nvar_treatment = treatment_gave.var(ddof=1)\nvar_control = control_gave.var(ddof=1)\n\n# Calculate sample sizes\nn_treatment = len(treatment_gave)\nn_control = len(control_gave)\n\n# Calculate the pooled standard error\nse = ((var_treatment / n_treatment) + (var_control / n_control)) ** 0.5\n\n# Calculate the t-statistic\nt_stat_manual = (mean_treatment - mean_control) / se\n\n# Degrees of freedom for unequal variances (Welch's t-test)\ndf = ((var_treatment / n_treatment + var_control / n_control) ** 2) / \\\n    (((var_treatment / n_treatment) ** 2) / (n_treatment - 1) + ((var_control / n_control) ** 2) / (n_control - 1))\n\n# Calculate the p-value (two-tailed)\np_value_manual = 2 * (1 - stats.t.cdf(abs(t_stat_manual), df))\n\nprint(f\"t-statistic = {t_stat_manual.round(4)}, p-value = {p_value_manual.round(4)}\")\n\nt-statistic = 3.2095, p-value = 0.0013\n\n\n\n# Create a DataFrame to store the t-statistic and p-value\ngave_t_test_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_manual.round(4), p_value_manual.round(4)]\n})\n\nprint(gave_t_test_results)\n\n        Metric   Value\n0  t-statistic  3.2095\n1      p-value  0.0013\n\n\n\ngave_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [gave_model.tvalues['treatment'].round(4), gave_model.pvalues['treatment'].round(4)]\n})\n\nprint(gave_model_results)\n\n\nproportions_df = pd.DataFrame({\n    \"Group\": [\"Control\", \"Treatment\"],\n    \"Proportion\": [control_proportion, treatment_proportion]\n})\n\nprint(proportions_df)\n\n\nprint(f\"Control Proportion: {control_proportion:.3f}\")\nprint(f\"Treatment Proportion: {treatment_proportion:.3f}\")\n\n\n# Probit regression for charitable donation\nprobit_model = sm.Probit(data['gave'], data[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n# 邊際效應估計\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\n\nfrom scipy.stats import ttest_ind\n\n# Task 1: T-tests for match ratio effects on donation likelihood\n\n# Filter data for each match ratio\nratio1_data = treatment_data[treatment_data['ratio'] == 1]\nratio2_data = treatment_data[treatment_data['ratio'] == 2]\nratio3_data = treatment_data[treatment_data['ratio'] == 3]\n\n# Perform t-tests\nt_stat_1v2, p_value_1v2 = ttest_ind(ratio1_data['gave'], ratio2_data['gave'], equal_var=False)\nt_stat_2v3, p_value_2v3 = ttest_ind(ratio2_data['gave'], ratio3_data['gave'], equal_var=False)\n\nprint(f\"T-test 1:1 vs 2:1 - t-statistic: {t_stat_1v2:.4f}, p-value: {p_value_1v2:.4f}\")\nprint(f\"T-test 2:1 vs 3:1 - t-statistic: {t_stat_2v3:.4f}, p-value: {p_value_2v3:.4f}\")\n\n\n\n\n\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\nratio_model = sm.OLS(data['gave'], data[['intercept', 'ratio1', 'ratio2', 'ratio3']], missing='drop').fit()\nprint(ratio_model.summary())\n\n\n\n# Task 3: Response rate differences\n# Direct calculation from data\nresponse_rate_1 = ratio1_data['gave'].mean()\nresponse_rate_2 = ratio2_data['gave'].mean()\nresponse_rate_3 = ratio3_data['gave'].mean()\n\ndiff_1v2 = response_rate_2 - response_rate_1\ndiff_2v3 = response_rate_3 - response_rate_2\n\nprint(f\"Response rate difference 1:1 vs 2:1: {diff_1v2:.4f}\")\nprint(f\"Response rate difference 2:1 vs 3:1: {diff_2v3:.4f}\")\n\n\n# Differences from regression coefficients\ncoef_diff_1v2 = ratio_model.params['ratio2'] - ratio_model.params['ratio1']\ncoef_diff_2v3 = ratio_model.params['ratio3'] - ratio_model.params['ratio2']\n\nprint(f\"Coefficient difference 1:1 vs 2:1: {coef_diff_1v2:.4f}\")\nprint(f\"Coefficient difference 2:1 vs 3:1: {coef_diff_2v3:.4f}\")\n\n\n# T-test for donation amount\ntreatment_amount = treatment_data['amount'].dropna()\ncontrol_amount = control_data['amount'].dropna()\nt_stat_amount, p_value_amount = ttest_ind(treatment_amount, control_amount, equal_var=False)\nprint(f\"T-test for donation amount: t-statistic = {t_stat_amount:.4f}, p-value = {p_value_amount:.4f}\")\n\n# Bivariate linear regression for donation amount\namount_model = sm.OLS(data['amount'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(amount_model.summary())\nprint(f\"Linear regression for donation amount: t-statistic = {amount_model.tvalues['treatment']:.4f}, p-value = {amount_model.pvalues['treatment']:.4f}\")\n\n\n# Filter data to include only those who made a donation\ndonors_data = data[data['amount'] &gt; 0]\n\n# Regression analysis for donation amount conditional on donating\ndonors_model = sm.OLS(donors_data['amount'], donors_data[['intercept', 'treatment']], missing='drop').fit()\nprint(donors_model.summary())\n\n# Interpretation of the treatment coefficient\ntreatment_coef = donors_model.params['treatment']\nprint(f\"Treatment coefficient: {treatment_coef:.4f}\")\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 模擬參數\nn_control = 100000\nn_treatment = 10000\np_control = 0.018\np_treatment = 0.022\n\n# 隨機抽樣\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(n=1, p=p_control, size=n_control)\ntreatment_draws = np.random.binomial(n=1, p=p_treatment, size=n_treatment)\n\n# 從控制組中隨機抽出與 treatment 組一樣多的樣本\ncontrol_sample = np.random.choice(control_draws, size=n_treatment, replace=False)\n\n# 計算逐筆差異\ndifferences = treatment_draws - control_sample\n\n# 計算累積平均\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# 繪製圖形\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative mean difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label=\"Theoretical mean difference = 0.004\")\nplt.title(\"Simulation: Cumulative Average Difference in Donation Rates)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# Filter data for donors only\ntreatment_donors = treatment_data[treatment_data['amount'] &gt; 0]\ncontrol_donors = control_data[control_data['amount'] &gt; 0]\n\n# Calculate sample averages\ntreatment_avg = treatment_donors['amount'].mean()\ncontrol_avg = control_donors['amount'].mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donors['amount'], bins=30, color='skyblue', alpha=0.7)\naxes[0].axvline(treatment_avg, color='red', linestyle='--', label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group: Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donors['amount'], bins=30, color='pink', alpha=0.7)\naxes[1].axvline(control_avg, color='red', linestyle='--', label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group: Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n# Define sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Initialize a figure for subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\n# Loop through each sample size\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n    \n    # Simulate 1000 averages\n    for _ in range(1000):\n        treatment_sample = np.random.choice(treatment_draws, size=sample_size, replace=True)\n        control_sample = np.random.choice(control_draws, size=sample_size, replace=True)\n        avg_differences.append(np.mean(treatment_sample - control_sample))\n    \n    # Plot histogram\n    axes[i].hist(avg_differences, bins=30, color='pink', edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n# Calculate means\nmean_r1 = r1['gave'].mean()\nmean_r2 = r2['gave'].mean()\nmean_r3 = r3['gave'].mean()\n\n# Calculate variances\nvar_r1 = r1['gave'].var(ddof=1)\nvar_r2 = r2['gave'].var(ddof=1)\nvar_r3 = r3['gave'].var(ddof=1)\n\n# Calculate sample sizes\nn_r1 = len(r1['gave'])\nn_r2 = len(r2['gave'])\nn_r3 = len(r3['gave'])\n\n# Calculate standard errors\nse_1v2 = ((var_r1 / n_r1) + (var_r2 / n_r2)) ** 0.5\nse_2v3 = ((var_r2 / n_r2) + (var_r3 / n_r3)) ** 0.5\n\n# Calculate t-statistics\nt_stat_1v2 = (mean_r1 - mean_r2) / se_1v2\nt_stat_2v3 = (mean_r2 - mean_r3) / se_2v3\n\n# Degrees of freedom for unequal variances (Welch's t-test)\ndf_1v2 = ((var_r1 / n_r1 + var_r2 / n_r2) ** 2) / \\\n         (((var_r1 / n_r1) ** 2) / (n_r1 - 1) + ((var_r2 / n_r2) ** 2) / (n_r2 - 1))\ndf_2v3 = ((var_r2 / n_r2 + var_r3 / n_r3) ** 2) / \\\n         (((var_r2 / n_r2) ** 2) / (n_r2 - 1) + ((var_r3 / n_r3) ** 2) / (n_r3 - 1))\n\n# Calculate p-values (two-tailed)\np_value_1v2 = 2 * (1 - stats.t.cdf(abs(t_stat_1v2), df_1v2))\np_value_2v3 = 2 * (1 - stats.t.cdf(abs(t_stat_2v3), df_2v3))\n\nprint(f\"T-test 1:1 vs 2:1 - t-statistic: {t_stat_1v2:.4f}, p-value: {p_value_1v2:.4f}\")\nprint(f\"T-test 2:1 vs 3:1 - t-statistic: {t_stat_2v3:.4f}, p-value: {p_value_2v3:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 # Calculate means\n----&gt; 2 mean_r1 = r1['gave'].mean()\n      3 mean_r2 = r2['gave'].mean()\n      4 mean_r3 = r3['gave'].mean()\n\nNameError: name 'r1' is not defined\n\n\n\n\n# Create a DataFrame for the T-test results\nt_test_results_df = pd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"2:1 vs 3:1\"],\n    \"T-statistic\": [t_stat_1v2, t_stat_2v3],\n    \"P-value\": [p_value_1v2, p_value_2v3]\n})\n\nprint(t_test_results_df)\n\n\n# Calculate means\nmean_treatment_amount = treatment_amount.mean()\nmean_control_amount = control_amount.mean()\n\n# Calculate variances\nvar_treatment_amount = treatment_amount.var(ddof=1)\nvar_control_amount = control_amount.var(ddof=1)\n\n# Calculate sample sizes\nn_treatment_amount = len(treatment_amount)\nn_control_amount = len(control_amount)\n\n# Calculate the pooled standard error\nse_amount = ((var_treatment_amount / n_treatment_amount) + (var_control_amount / n_control_amount)) ** 0.5\n\n# Calculate the t-statistic\nt_stat_amount_manual = (mean_treatment_amount - mean_control_amount) / se_amount\n\n# Degrees of freedom for unequal variances (Welch's t-test)\ndf_amount = ((var_treatment_amount / n_treatment_amount + var_control_amount / n_control_amount) ** 2) / \\\n    (((var_treatment_amount / n_treatment_amount) ** 2) / (n_treatment_amount - 1) + \n     ((var_control_amount / n_control_amount) ** 2) / (n_control_amount - 1))\n\n# Calculate the p-value (two-tailed)\np_value_amount_manual = 2 * (1 - stats.t.cdf(abs(t_stat_amount_manual), df_amount))\n\nprint(f\"T-test for donation amount (manual): t-statistic = {t_stat_amount_manual:.4f}, p-value = {p_value_amount_manual:.4f}\")\n\n\nt_test_amount_manual_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_amount_manual.round(4), p_value_amount_manual.round(4)]\n})\n\nprint(t_test_amount_manual_results)\n\n\namount_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [amount_model.tvalues['treatment'].round(4), amount_model.pvalues['treatment'].round(4)]\n})\n\nprint(amount_model_results)\n\n\ntreatment_coef_df = pd.DataFrame({\n    \"Metric\": [\"Treatment Coefficient\"],\n    \"Value\": [treatment_coef.round(4)]\n})\n\nprint(treatment_coef_df)"
  },
  {
    "objectID": "blog/Project2/hw1_questions.html",
    "href": "blog/Project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#introduction",
    "href": "blog/Project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#data",
    "href": "blog/Project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#experimental-results",
    "href": "blog/Project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#simulation-experiment",
    "href": "blog/Project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/Project2/index.html",
    "href": "blog/Project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nReading blueprinty’s data\nimport pandas as pd\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head(5)\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nThis section conducts preliminary observations through EDA, with the aim of comparing the differences in patent output between companies using Blueprinty software and non-users. As can be seen from the chart, the proportion of Blueprinty users in the high patent number range is relatively high, and the overall patent performance is also more outstanding. Comparison of the average number of patents further shows that Blueprinty customers perform significantly better than non-customers. These preliminary results suggest that Blueprinty software may have a positive impact on patent applications, providing reasonable motivation and direction for the subsequent establishment of more rigorous statistical models.\n\n\nAverage number of patents by customer status\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"patents\", hue='iscustomer', kde=False, bins=15, palette=\"Set1\", multiple=\"stack\")\nplt.title(\"Distribution of Patent Counts by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nsns.barplot(data=blueprinty, x=\"iscustomer\", y=\"patents\", hue=\"iscustomer\", palette=\"Set1\", estimator=np.mean, dodge=False, legend=False)\nplt.title(\"Average Number of Patents by Customer Status\")\nplt.xlabel(\"Blueprinty Customer\")\nplt.ylabel(\"Average Patent Count\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe average number of patents by Blueprinty users is significantly higher than that of non-users.\nSuggests that companies using Blueprinty software may be more successful in obtaining patents.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nNext, we will continue to use ED to further confirm whether there are systematic differences in firm characteristics between Blueprinty customers and non-customers, which is crucial for the subsequent establishment of causal inference models (such as regression models).\n\n\nAverage number of patents by customer status\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", kde=True, bins=20, palette=\"Set2\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Distribution of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age (Years)\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize=\"index\") * 100\nregion_counts.plot(kind=\"bar\", stacked=True, ax=plt.gca(), colormap=\"Set2\")\nplt.title(\"Regional Composition by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Percentage (%)\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBlueprinty users (green) and non-users (purple) have slightly different overall age distributions.\nThe distribution of non-users is slightly peaked towards younger companies; the distribution of users is slightly flatter and still has some density at higher ages.\n\nThis further illustrates that the company age may affect whether to become a user and may also be associated with the number of patents, so this variable should be controlled when making causal inferences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\nLikehood\nfrom IPython.display import display, Math\n\ndisplay(Math(r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\"))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\\)\n\n\n\n\nlog_Likehood\nfrom IPython.display import display, Math\n\ndisplay(Math(\n    r\"\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\"\n))\n\n\n\\(\\displaystyle \\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\\)\n\n\n\n\n\n\ndef poisson_loglikelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n\n\n\nUsing the previously defined log-likelihood function, we can visualize the change in log-likelihood for different values ​​of λ (lambda) and find the maximum likelihood estimate (MLE) through a graph.\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\nY = blueprinty[\"patents\"].values\nn = len(Y)\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return -n * lmbda + np.sum(Y * np.log(lmbda)) - np.sum(np.log(factorial(Y)))\nlambda_range = np.linspace(0.1, 10, 200)\n\nloglikelihood_values = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_range, loglikelihood_values, color='darkblue')\nplt.title(\"Log-Likelihood vs Lambda (Poisson Model)\")\nplt.xlabel(\"λ (lambda)\")  \nplt.ylabel(\"Log-Likelihood\")  \nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHorizontal axis (λ): a series of candidate λ values ​​we tried\nVertical axis (log-likelihood): log-likelihood value of each λ under actual data (number of patents) The curve has a typical “peak shape”, which means that there is a certain λ that maximizes the log-likelihood. The location of the peak is the maximum likelihood estimate (MLE).\n\n\n\n\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport sympy as sp\nfrom IPython.display import display, Math\n\nlmbda, n, sum_y = sp.symbols('lambda n sum_y', positive=True)\n\nlog_likelihood = -n * lmbda + sum_y * sp.log(lmbda)\n\n\nd_log_likelihood = sp.diff(log_likelihood, lmbda)\n\nsolution = sp.solve(d_log_likelihood, lmbda)[0]\n\ndisplay(Math(r\"\\textbf{Step 1: Define the log-likelihood function}\"))\ndisplay(Math(r\"\\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\"))\n\ndisplay(Math(r\"\\textbf{Step 2: Take the first derivative}\"))\ndisplay(Math(r\"\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\"))\n\ndisplay(Math(r\"\\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\"))\ndisplay(Math(r\"0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\"))\n\ndisplay(Math(r\"\\boxed{\\hat{\\lambda}_{\\text{MLE}} = \" + sp.latex(solution) + r\"}\"))\n\n\n\\(\\displaystyle \\textbf{Step 1: Define the log-likelihood function}\\)\n\n\n\\(\\displaystyle \\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\\)\n\n\n\\(\\displaystyle \\textbf{Step 2: Take the first derivative}\\)\n\n\n\\(\\displaystyle \\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\\)\n\n\n\\(\\displaystyle \\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\\)\n\n\n\\(\\displaystyle 0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\\)\n\n\n\\(\\displaystyle \\boxed{\\hat{\\lambda}_{\\text{MLE}} = \\frac{sum_{y}}{n}}\\)\n\n\n\n\n\nUse numerical optimization methods to find the maximum likelihood estimate (MLE) of λ in the Poisson model.\n\n\nOptimization of likehood by MLE\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda):\n    return -np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n# Use minimize to find the MLE\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\n\nmle_df = pd.DataFrame({\n    \"Parameter\": [\"lambda\"],\n    \"MLE Estimate\": [lambda_mle]\n})\n\nmle_df\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\n\n\n\n\n0\nlambda\n3.684666\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nThe original Poisson model likelihood function is expanded into a log-likelihood function of a Poisson regression model, where λ is no longer a fixed parameter but is determined by the explanatory variable X and the parameter vector \n\n\nLog-Likelihood Function for the Poisson Regression Model\nimport numpy as np\nfrom IPython.display import display, Math\n\nlikelihood = r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\"\nlambda_def = r\"\\lambda_i = \\exp(X_i^\\top \\beta)\"\nlog_likelihood = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\"\nlog_likelihood_expanded = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\"\n\ndisplay(Math(likelihood))\ndisplay(Math(lambda_def))\ndisplay(Math(log_likelihood))\ndisplay(Math(log_likelihood_expanded))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\n\n\n\\(\\displaystyle \\lambda_i = \\exp(X_i^\\top \\beta)\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\\)\n\n\n\nimport numpy as np\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear_predictor = X @ beta\n    \n    lambda_i = np.exp(linear_predictor)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(np.math.factorial(Y)))\n    \n    return log_likelihood\n\n\n\n\nPerform maximum likelihood estimation (MLE) on the Poisson regression model and use the inverse matrix of the Hessian matrix to calculate the standard errors of the parameters (Standard Errors)\n\n\nPoisson Regression Coefficients\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_mat = X.astype(float).values  \nY = blueprinty[\"patents\"].values\nn, k = X_mat.shape\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lin_pred = X @ beta\n    lambda_i = np.exp(lin_pred)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n\ndef neg_log_likelihood(beta):\n    return -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.zeros(k)\nresult = minimize(neg_log_likelihood, x0=beta_init, method='BFGS')\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv  \nse_beta = np.sqrt(np.diag(hessian_inv))\n\nmle_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se_beta\n})\nmle_table\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n1.554750\n0.063189\n\n\n1\nage\n-0.007970\n0.001890\n\n\n2\nage_squared_std\n-0.155814\n0.013481\n\n\n3\niscustomer\n0.207591\n0.031102\n\n\n4\nNortheast\n0.029170\n0.031939\n\n\n5\nNorthwest\n-0.017575\n0.052020\n\n\n6\nSouth\n0.056561\n0.051001\n\n\n7\nSouthwest\n0.050576\n0.043582\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model (GLM with Poisson family) to quantify and test whether the Blueprinty software usage status (iscustomer) significantly affects the number of patents obtained by the company, while also controlling for other variables that may affect patent performance, such as company age and region.\n\n\nPoisson Regression Coefficients GLM Function\nimport statsmodels.api as sm\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_sm = X_sm.astype(float)\n\nX_sm = sm.add_constant(X_sm)\n\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_df = result.summary2().tables[1].reset_index()\nsummary_df = summary_df.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"coef\",\n    \"Std.Err.\": \"std err\",\n    \"P&gt;|z|\": \"P&gt;|z|\",\n    \"[0.025\": \"[0.025\",\n    \"0.975]\": \"0.975]\"\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nVariable\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n1.554747\n0.066336\n23.437483\n1.773673e-121\n1.424731\n1.684763\n\n\n1\nage\n-0.007970\n0.002074\n-3.843138\n1.214711e-04\n-0.012035\n-0.003905\n\n\n2\nage_squared_std\n-0.155814\n0.013533\n-11.513237\n1.131496e-30\n-0.182339\n-0.129289\n\n\n3\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n0.147037\n0.268144\n\n\n4\nNortheast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n-0.056334\n0.114674\n\n\n5\nNorthwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n-0.122983\n0.087833\n\n\n6\nSouth\n0.056561\n0.052662\n1.074036\n2.828066e-01\n-0.046655\n0.159778\n\n\n7\nSouthwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n-0.041931\n0.143083\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model to analyze the relationship between Blueprinty software usage and the firm’s patent application success. The model controls for firm age (and its square) and regional differences, and uses the number of patents as an explanatory variable to perform maximum likelihood estimation (MLE).\nThe results show:\n\nCompanies using Blueprinty have significantly more patents on average than non-users.\nThe expected number of patents for Blueprinty users is about 1.23 times that of non-users, representing an average improvement of about 23%.\nThere is an inverted U-shaped relationship between company age and patent performance, i.e., as age increases, the number of patents first increases and then decreases.\nThe regional variable did not reach statistical significance, indicating that the region where the company is located has no significant impact on patent performance after controlling for other factors.\n\n\n\n\n\n\nCausal Effect of Blueprinty Usage on Patent Outcomes via Counterfactual Predictionn\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(int)\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_sm = sm.add_constant(X_sm)\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nX_counterfactual_0 = X_sm.copy()\nX_counterfactual_1 = X_sm.copy()\nX_counterfactual_0[\"iscustomer\"] = 0\nX_counterfactual_1[\"iscustomer\"] = 1\n\ny_pred_0 = result.predict(X_counterfactual_0)\ny_pred_1 = result.predict(X_counterfactual_1)\ndiff = y_pred_1 - y_pred_0\nmean = diff.mean()\naverage_effect = pd.DataFrame({\n    \"Effect Type\": [\"Average Treatment Effect (ATE)\"],\n    \"Estimated Effect (Δŷ)\": [mean]\n})\naverage_effect\n\n\n\n\n\n\n\n\n\nEffect Type\nEstimated Effect (Δŷ)\n\n\n\n\n0\nAverage Treatment Effect (ATE)\n0.792768\n\n\n\n\n\n\n\nIf all companies became Blueprinty customers, each company could expect to receive, on average, approximately 0.79 more patents. This means that using Blueprinty’s software has a substantial, positive effect on patent application success, based on the Poisson model you built, after controlling for firm age and region. This estimate is calculated using a counterfactual simulation method, which converts log(λ) into actual predicted differences and is highly interpretable."
  },
  {
    "objectID": "blog/Project2/index.html#blueprinty-case-study",
    "href": "blog/Project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nReading blueprinty’s data\nimport pandas as pd\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head(5)\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nThis section conducts preliminary observations through EDA, with the aim of comparing the differences in patent output between companies using Blueprinty software and non-users. As can be seen from the chart, the proportion of Blueprinty users in the high patent number range is relatively high, and the overall patent performance is also more outstanding. Comparison of the average number of patents further shows that Blueprinty customers perform significantly better than non-customers. These preliminary results suggest that Blueprinty software may have a positive impact on patent applications, providing reasonable motivation and direction for the subsequent establishment of more rigorous statistical models.\n\n\nAverage number of patents by customer status\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"patents\", hue='iscustomer', kde=False, bins=15, palette=\"Set1\", multiple=\"stack\")\nplt.title(\"Distribution of Patent Counts by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nsns.barplot(data=blueprinty, x=\"iscustomer\", y=\"patents\", hue=\"iscustomer\", palette=\"Set1\", estimator=np.mean, dodge=False, legend=False)\nplt.title(\"Average Number of Patents by Customer Status\")\nplt.xlabel(\"Blueprinty Customer\")\nplt.ylabel(\"Average Patent Count\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe average number of patents by Blueprinty users is significantly higher than that of non-users.\nSuggests that companies using Blueprinty software may be more successful in obtaining patents.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nNext, we will continue to use ED to further confirm whether there are systematic differences in firm characteristics between Blueprinty customers and non-customers, which is crucial for the subsequent establishment of causal inference models (such as regression models).\n\n\nAverage number of patents by customer status\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", kde=True, bins=20, palette=\"Set2\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Distribution of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age (Years)\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize=\"index\") * 100\nregion_counts.plot(kind=\"bar\", stacked=True, ax=plt.gca(), colormap=\"Set2\")\nplt.title(\"Regional Composition by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Percentage (%)\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBlueprinty users (green) and non-users (purple) have slightly different overall age distributions.\nThe distribution of non-users is slightly peaked towards younger companies; the distribution of users is slightly flatter and still has some density at higher ages.\n\nThis further illustrates that the company age may affect whether to become a user and may also be associated with the number of patents, so this variable should be controlled when making causal inferences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\nLikehood\nfrom IPython.display import display, Math\n\ndisplay(Math(r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\"))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\\)\n\n\n\n\nlog_Likehood\nfrom IPython.display import display, Math\n\ndisplay(Math(\n    r\"\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\"\n))\n\n\n\\(\\displaystyle \\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\\)\n\n\n\n\n\n\ndef poisson_loglikelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n\n\n\nUsing the previously defined log-likelihood function, we can visualize the change in log-likelihood for different values ​​of λ (lambda) and find the maximum likelihood estimate (MLE) through a graph.\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\nY = blueprinty[\"patents\"].values\nn = len(Y)\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return -n * lmbda + np.sum(Y * np.log(lmbda)) - np.sum(np.log(factorial(Y)))\nlambda_range = np.linspace(0.1, 10, 200)\n\nloglikelihood_values = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_range, loglikelihood_values, color='darkblue')\nplt.title(\"Log-Likelihood vs Lambda (Poisson Model)\")\nplt.xlabel(\"λ (lambda)\")  \nplt.ylabel(\"Log-Likelihood\")  \nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHorizontal axis (λ): a series of candidate λ values ​​we tried\nVertical axis (log-likelihood): log-likelihood value of each λ under actual data (number of patents) The curve has a typical “peak shape”, which means that there is a certain λ that maximizes the log-likelihood. The location of the peak is the maximum likelihood estimate (MLE).\n\n\n\n\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport sympy as sp\nfrom IPython.display import display, Math\n\nlmbda, n, sum_y = sp.symbols('lambda n sum_y', positive=True)\n\nlog_likelihood = -n * lmbda + sum_y * sp.log(lmbda)\n\n\nd_log_likelihood = sp.diff(log_likelihood, lmbda)\n\nsolution = sp.solve(d_log_likelihood, lmbda)[0]\n\ndisplay(Math(r\"\\textbf{Step 1: Define the log-likelihood function}\"))\ndisplay(Math(r\"\\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\"))\n\ndisplay(Math(r\"\\textbf{Step 2: Take the first derivative}\"))\ndisplay(Math(r\"\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\"))\n\ndisplay(Math(r\"\\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\"))\ndisplay(Math(r\"0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\"))\n\ndisplay(Math(r\"\\boxed{\\hat{\\lambda}_{\\text{MLE}} = \" + sp.latex(solution) + r\"}\"))\n\n\n\\(\\displaystyle \\textbf{Step 1: Define the log-likelihood function}\\)\n\n\n\\(\\displaystyle \\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\\)\n\n\n\\(\\displaystyle \\textbf{Step 2: Take the first derivative}\\)\n\n\n\\(\\displaystyle \\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\\)\n\n\n\\(\\displaystyle \\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\\)\n\n\n\\(\\displaystyle 0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\\)\n\n\n\\(\\displaystyle \\boxed{\\hat{\\lambda}_{\\text{MLE}} = \\frac{sum_{y}}{n}}\\)\n\n\n\n\n\nUse numerical optimization methods to find the maximum likelihood estimate (MLE) of λ in the Poisson model.\n\n\nOptimization of likehood by MLE\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda):\n    return -np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n# Use minimize to find the MLE\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\n\nmle_df = pd.DataFrame({\n    \"Parameter\": [\"lambda\"],\n    \"MLE Estimate\": [lambda_mle]\n})\n\nmle_df\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\n\n\n\n\n0\nlambda\n3.684666\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nThe original Poisson model likelihood function is expanded into a log-likelihood function of a Poisson regression model, where λ is no longer a fixed parameter but is determined by the explanatory variable X and the parameter vector \n\n\nLog-Likelihood Function for the Poisson Regression Model\nimport numpy as np\nfrom IPython.display import display, Math\n\nlikelihood = r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\"\nlambda_def = r\"\\lambda_i = \\exp(X_i^\\top \\beta)\"\nlog_likelihood = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\"\nlog_likelihood_expanded = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\"\n\ndisplay(Math(likelihood))\ndisplay(Math(lambda_def))\ndisplay(Math(log_likelihood))\ndisplay(Math(log_likelihood_expanded))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\n\n\n\\(\\displaystyle \\lambda_i = \\exp(X_i^\\top \\beta)\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\\)\n\n\n\nimport numpy as np\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear_predictor = X @ beta\n    \n    lambda_i = np.exp(linear_predictor)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(np.math.factorial(Y)))\n    \n    return log_likelihood\n\n\n\n\nPerform maximum likelihood estimation (MLE) on the Poisson regression model and use the inverse matrix of the Hessian matrix to calculate the standard errors of the parameters (Standard Errors)\n\n\nPoisson Regression Coefficients\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_mat = X.astype(float).values  \nY = blueprinty[\"patents\"].values\nn, k = X_mat.shape\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lin_pred = X @ beta\n    lambda_i = np.exp(lin_pred)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n\ndef neg_log_likelihood(beta):\n    return -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.zeros(k)\nresult = minimize(neg_log_likelihood, x0=beta_init, method='BFGS')\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv  \nse_beta = np.sqrt(np.diag(hessian_inv))\n\nmle_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se_beta\n})\nmle_table\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n1.554750\n0.063189\n\n\n1\nage\n-0.007970\n0.001890\n\n\n2\nage_squared_std\n-0.155814\n0.013481\n\n\n3\niscustomer\n0.207591\n0.031102\n\n\n4\nNortheast\n0.029170\n0.031939\n\n\n5\nNorthwest\n-0.017575\n0.052020\n\n\n6\nSouth\n0.056561\n0.051001\n\n\n7\nSouthwest\n0.050576\n0.043582\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model (GLM with Poisson family) to quantify and test whether the Blueprinty software usage status (iscustomer) significantly affects the number of patents obtained by the company, while also controlling for other variables that may affect patent performance, such as company age and region.\n\n\nPoisson Regression Coefficients GLM Function\nimport statsmodels.api as sm\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_sm = X_sm.astype(float)\n\nX_sm = sm.add_constant(X_sm)\n\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_df = result.summary2().tables[1].reset_index()\nsummary_df = summary_df.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"coef\",\n    \"Std.Err.\": \"std err\",\n    \"P&gt;|z|\": \"P&gt;|z|\",\n    \"[0.025\": \"[0.025\",\n    \"0.975]\": \"0.975]\"\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nVariable\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n1.554747\n0.066336\n23.437483\n1.773673e-121\n1.424731\n1.684763\n\n\n1\nage\n-0.007970\n0.002074\n-3.843138\n1.214711e-04\n-0.012035\n-0.003905\n\n\n2\nage_squared_std\n-0.155814\n0.013533\n-11.513237\n1.131496e-30\n-0.182339\n-0.129289\n\n\n3\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n0.147037\n0.268144\n\n\n4\nNortheast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n-0.056334\n0.114674\n\n\n5\nNorthwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n-0.122983\n0.087833\n\n\n6\nSouth\n0.056561\n0.052662\n1.074036\n2.828066e-01\n-0.046655\n0.159778\n\n\n7\nSouthwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n-0.041931\n0.143083\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model to analyze the relationship between Blueprinty software usage and the firm’s patent application success. The model controls for firm age (and its square) and regional differences, and uses the number of patents as an explanatory variable to perform maximum likelihood estimation (MLE).\nThe results show:\n\nCompanies using Blueprinty have significantly more patents on average than non-users.\nThe expected number of patents for Blueprinty users is about 1.23 times that of non-users, representing an average improvement of about 23%.\nThere is an inverted U-shaped relationship between company age and patent performance, i.e., as age increases, the number of patents first increases and then decreases.\nThe regional variable did not reach statistical significance, indicating that the region where the company is located has no significant impact on patent performance after controlling for other factors.\n\n\n\n\n\n\nCausal Effect of Blueprinty Usage on Patent Outcomes via Counterfactual Predictionn\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(int)\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_sm = sm.add_constant(X_sm)\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nX_counterfactual_0 = X_sm.copy()\nX_counterfactual_1 = X_sm.copy()\nX_counterfactual_0[\"iscustomer\"] = 0\nX_counterfactual_1[\"iscustomer\"] = 1\n\ny_pred_0 = result.predict(X_counterfactual_0)\ny_pred_1 = result.predict(X_counterfactual_1)\ndiff = y_pred_1 - y_pred_0\nmean = diff.mean()\naverage_effect = pd.DataFrame({\n    \"Effect Type\": [\"Average Treatment Effect (ATE)\"],\n    \"Estimated Effect (Δŷ)\": [mean]\n})\naverage_effect\n\n\n\n\n\n\n\n\n\nEffect Type\nEstimated Effect (Δŷ)\n\n\n\n\n0\nAverage Treatment Effect (ATE)\n0.792768\n\n\n\n\n\n\n\nIf all companies became Blueprinty customers, each company could expect to receive, on average, approximately 0.79 more patents. This means that using Blueprinty’s software has a substantial, positive effect on patent application success, based on the Poisson model you built, after controlling for firm age and region. This estimate is calculated using a counterfactual simulation method, which converts log(λ) into actual predicted differences and is highly interpretable."
  },
  {
    "objectID": "blog/Project2/index.html#airbnb-case-study",
    "href": "blog/Project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nReading airbnb’s dataset\nimport pandas as pd\nairbnb = pd.read_csv('airbnb.csv')\nairbnb.head(5)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n\nEDA\n\nMissing Value Check and Handling\n\n\nMissing Value\n# Create a DataFrame from the missing_values Series\nmissing_values = airbnb.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\nrelevant_columns = ['number_of_reviews', 'price', 'room_type', 'bedrooms', 'bathrooms', 'instant_bookable']\nairbnb_cleaned = airbnb[relevant_columns].dropna()\nmissing_values_df\n\n\n\n\n\n\n\n\n\nColumn\nMissing Values\n\n\n\n\n0\nUnnamed: 0\n0\n\n\n1\nid\n0\n\n\n2\ndays\n0\n\n\n3\nlast_scraped\n0\n\n\n4\nhost_since\n35\n\n\n5\nroom_type\n0\n\n\n6\nbathrooms\n160\n\n\n7\nbedrooms\n76\n\n\n8\nprice\n0\n\n\n9\nnumber_of_reviews\n0\n\n\n10\nreview_scores_cleanliness\n10195\n\n\n11\nreview_scores_location\n10254\n\n\n12\nreview_scores_value\n10256\n\n\n13\ninstant_bookable\n0\n\n\n\n\n\n\n\nWe have removed all observation columns with missing values ​​to ensure the accuracy of the model.\n\n\nVariable Distribution\n\nReview Volume and Price Distribution\n\n\n\nDistribution plot\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsummary_stats = airbnb.describe()\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(airbnb[\"number_of_reviews\"], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\n\nplt.subplot(1, 2, 2)\nsns.histplot(airbnb[\"price\"], bins=50, kde=False)\nplt.title(\"Distribution of Price\")\nplt.xlabel(\"Price\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNumber of reviews (number_of_reviews): skewed distribution, most listings have low numbers of reviews, but some have abnormally high values ​​(common for popular listings)\nPrice: right-skewed distribution, some properties are very expensive, need to consider conversion (such as log) in subsequent modeling\n\n\nLog Transformations Distribution\n\n\n\nDistribution plot of price log\nairbnb[\"log_price\"] = np.log1p(airbnb[\"price\"])\nairbnb[\"log_reviews\"] = np.log1p(airbnb[\"number_of_reviews\"])\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.histplot(airbnb[\"log_reviews\"], bins=50)\nplt.title(\"Log(Number of Reviews + 1) Distribution\")\n\nplt.subplot(1, 2, 2)\nsns.histplot(airbnb[\"log_price\"], bins=50)\nplt.title(\"Log(Price + 1) Distribution\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAfter log(price + 1) and log(number_of_reviews + 1) transformation, the distribution obviously tends to be normal, which is conducive to subsequent modeling.\n\n\nCategorical Variable Distributions\n\n\nDistribution plot of price log\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(data=airbnb, x=\"room_type\")\nplt.title(\"Room Type Distribution\")\n\nplt.subplot(1, 2, 2)\nsns.countplot(data=airbnb, x=\"instant_bookable\")\nplt.title(\"Instant Bookable Distribution\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nroom_type: Private room and Entire home/apt are the two most common types, and Shared room is very rare\ninstant_bookable: Most listings do not support instant booking, which means this variable may have explanatory power in the model\n\n\n\nCorrelation Exploration\nUse a scatter plot to check whether number_of_reviews is correlated with review-based variables. This helps validate whether reviews are a reasonable proxy for bookings.\n\n\nnumber_of_reviews correlation with review-based variables\nplt.figure(figsize=(6, 5))\nsns.scatterplot(data=airbnb, x=\"number_of_reviews\", y=\"review_scores_value\", alpha=0.3)\nplt.title(\"Number of Reviews vs. Review Score (Value)\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Review Score - Value\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFrom the scatter plot of number_of_reviews vs. review_scores_value, there is a positive correlation trend to a certain extent.\nShows that the number of reviews is somewhat correlated with user satisfaction, supporting the use of the number of reviews as a proxy variable for the number of orders.\n\n\n\n\nPoisson Regression Model\n\n\nVisualize the relationship between price and number of reviews\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ncols = [\"price\", \"number_of_reviews\", \"instant_bookable\", \"room_type\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"bathrooms\", \"bedrooms\", \"days\"]\ndf = airbnb.dropna(subset=cols).copy()\n\ndf[\"log_price\"] = np.log1p(df[\"price\"])  \ndf[\"instant_bookable\"] = df[\"instant_bookable\"].map({\"t\": 1, \"f\": 0}).astype(int)  \n\ndf = pd.get_dummies(df, columns=[\"room_type\"], drop_first=True)\n\nfeature_cols = [\"log_price\", \"days\", \"bathrooms\", \"bedrooms\",\n                \"review_scores_cleanliness\", \"review_scores_location\",\n                \"review_scores_value\", \"instant_bookable\"] + \\\n               [col for col in df.columns if col.startswith(\"room_type_\")]\nX = df[feature_cols].astype(float)\nX = sm.add_constant(X)\n\nY = df[\"number_of_reviews\"]\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_df = result.summary2().tables[1].copy()\nsummary_df[\"exp(coef)\"] = np.exp(summary_df[\"Coef.\"])\nsummary_df[\"Significance\"] = summary_df[\"P&gt;|z|\"].apply(\n    lambda p: \"***\" if p &lt; 0.001 else \"**\" if p &lt; 0.01 else \"*\" if p &lt; 0.05 else \"\")\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"coef\", \"Std.Err.\": \"std err\", \"P&gt;|z|\": \"P&gt;|z|\",\n    \"[0.025\": \"[0.025\", \"0.975]\": \"0.975]\"\n}).reset_index()\n\nfinal_output_df = summary_df[[\n    \"index\", \"coef\", \"std err\", \"exp(coef)\", \"P&gt;|z|\", \"Significance\", \"[0.025\", \"0.975]\"\n]].rename(columns={\"index\": \"Variable\"})\n\nfinal_output_df.round(3).head(10)\n\n\n\n\n\n\n\n\n\nVariable\ncoef\nstd err\nexp(coef)\nP&gt;|z|\nSignificance\n[0.025\n0.975]\n\n\n\n\n0\nconst\n3.013\n0.019\n20.354\n0.0\n***\n2.975\n3.051\n\n\n1\nlog_price\n0.131\n0.003\n1.140\n0.0\n***\n0.125\n0.137\n\n\n2\ndays\n0.000\n0.000\n1.000\n0.0\n***\n0.000\n0.000\n\n\n3\nbathrooms\n-0.145\n0.004\n0.865\n0.0\n***\n-0.153\n-0.138\n\n\n4\nbedrooms\n0.047\n0.002\n1.048\n0.0\n***\n0.043\n0.051\n\n\n5\nreview_scores_cleanliness\n0.109\n0.001\n1.115\n0.0\n***\n0.106\n0.112\n\n\n6\nreview_scores_location\n-0.097\n0.002\n0.907\n0.0\n***\n-0.101\n-0.094\n\n\n7\nreview_scores_value\n-0.080\n0.002\n0.924\n0.0\n***\n-0.083\n-0.076\n\n\n8\ninstant_bookable\n0.352\n0.003\n1.422\n0.0\n***\n0.346\n0.358\n\n\n9\nroom_type_Private room\n0.087\n0.003\n1.090\n0.0\n***\n0.080\n0.093\n\n\n\n\n\n\n\n\nModele Coefficient Interpretation\n\n\nVisualize the relationship between price and number of reviews\nsns.scatterplot(data=airbnb_cleaned, x='price', y='number_of_reviews', alpha=0.5)\nplt.title('Number of Reviews vs Price')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nModel Interpretation\nIn this analysis, we used a Poisson regression model to predict the number of Airbnb reviews, treating review count as a proxy for bookings. The model included predictors such as price, instant bookability, review scores, and property features. Because Poisson regression coefficients are in log form, we exponentiate them (exp(β)) to interpret their impact as multiplicative effects on the expected number of reviews.\nKey findings from the model are as follows:\n\nPrice (log-transformed) The coefficient for log(price) is 0.135, with exp(β) = 1.14. This indicates that a one-unit increase in log price is associated with a 14% increase in expected review count, suggesting that higher-priced listings may gain more visibility or attract more interest.\nInstant Bookable The coefficient is 0.341, with exp(β) = 1.41. Listings that support instant booking are expected to receive 41% more reviews than those that do not, highlighting the importance of booking convenience in driving customer engagement.\nReview Scores: Cleanliness With a coefficient of 0.109 and exp(β) = 1.11, each one-point increase in cleanliness score corresponds to an 11% increase in expected review count, underscoring how important cleanliness is in guest satisfaction and feedback.\nReview Scores: Location The coefficient is -0.098, with exp(β) = 0.91, meaning that a higher location score is unexpectedly associated with fewer reviews. This may suggest that guests are less inclined to leave feedback when location expectations are already met, or it may reflect an unobserved confounder.\n\nOverall, the model provides interpretable and statistically significant insights into the factors influencing review counts. These results support the idea that certain listing features—especially instant bookability and cleanliness—play a meaningful role in increasing engagement, and they can inform host strategy and platform design going forward.\n\n\nConclusion\nThis analysis demonstrates how listing characteristics on Airbnb relate to the number of reviews, which we use as a proxy for booking activity. Through a series of exploratory data analysis steps and a Poisson regression model, we identified key factors that significantly influence review count.\nAmong these, instant bookability, listing price, and cleanliness score emerge as the strongest predictors. Listings that allow instant booking are associated with a 41% increase in expected review count, highlighting the role of convenience in driving user engagement. Similarly, listings with higher prices and better cleanliness scores tend to receive more reviews, suggesting that both perceived quality and visibility may contribute to user response.\nInterestingly, location score shows a negative relationship with review count. While counterintuitive, this may reflect a behavioral trend where guests are less likely to leave feedback when their expectations are met—or it could indicate unobserved variables influencing this dynamic.\nTaken together, the results confirm that certain listing features meaningfully shape guest behavior and can influence a listing’s success on the platform. These findings can guide hosts in optimizing their property features and booking settings, and may also inform Airbnb’s platform design and recommendation algorithms to enhance both host and guest experience."
  },
  {
    "objectID": "blog/Project3/hw2_questions.html",
    "href": "blog/Project3/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/Project3/hw2_questions.html#blueprinty-case-study",
    "href": "blog/Project3/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/Project3/hw2_questions.html#airbnb-case-study",
    "href": "blog/Project3/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  }
]