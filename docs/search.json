[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lulu Ling",
    "section": "",
    "text": "Hi!! I am Lulu Ling. This is my marketing analytics project."
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification\n\n\n\n\nLulu Ling\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\nLulu Ling\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\nLulu Ling\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nLulu Ling\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\nLulu Ling\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Project1/index.html",
    "href": "blog/Project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn oreder to explore if the price affects on chartibal giving behavior, Dean Karlan and John conducted a large scale experiment involving about 50,000 donors to a liberal nonprofit organization. The subjects were randomly assigned to two groups: a control group and an experimental group. The control group received a standard fundraising letter without any additional instructions, while the experimental group received a letter containing a matching grant.\nIn this experiment, people are further randomly assigned to different sub-treatment conditions, such as designs of matching ratio, matching amount cap and suggested donation amount. These details will be further described in the data description section. The group of treatment will receive letters included an additional paragraph inserted at the top of the second page that announced that a “concerned fellow member” will match their donation, and the reply card included in bold type the details of the match. For the control group, the reply card match language was replaced with a large logo of the organization.\nThis design allows researchers to not only estimate the average treatment effect, but also to further analyze the impact of different matching ratios, upper limits, and recommended amounts on donation decisions. In addition, the study also observed differential responses in red states and blue states, indicating that the political environment also affects the sensitivity of donation behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/index.html#section-1-data",
    "href": "blog/Project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/Project1/index.html#section-2-analysis",
    "href": "blog/Project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/Project1/index.html#introduction",
    "href": "blog/Project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn oreder to explore if the price affects on chartibal giving behavior, Dean Karlan and John conducted a large scale experiment involving about 50,000 donors to a liberal nonprofit organization. The subjects were randomly assigned to two groups: a control group and an experimental group. The control group received a standard fundraising letter without any additional instructions, while the experimental group received a letter containing a matching grant.\nIn this experiment, people are further randomly assigned to different sub-treatment conditions, such as designs of matching ratio, matching amount cap and suggested donation amount. These details will be further described in the data description section. The group of treatment will receive letters included an additional paragraph inserted at the top of the second page that announced that a “concerned fellow member” will match their donation, and the reply card included in bold type the details of the match. For the control group, the reply card match language was replaced with a large logo of the organization.\nThis design allows researchers to not only estimate the average treatment effect, but also to further analyze the impact of different matching ratios, upper limits, and recommended amounts on donation decisions. In addition, the study also observed differential responses in red states and blue states, indicating that the political environment also affects the sensitivity of donation behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/index.html#data",
    "href": "blog/Project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis data comes from a large-scale natural field experiment conducted by a liberal nonprofit organization in the United States in 2005. The purpose of the study was to explore:\nDo different donation reminder designs affect people’s actual donation behavior?\nTreatment Conditions\n\nPaired ratios: $1:$1, $2:$1, $3:$1, control\nMaximum amount of matching: $25,000 / $50,000 / $100,000 / control\nAsk amount: based on 1.0 times, 1.25 times, or 1.5 times the donor’s highest past donation\n\nSample size and groups\n\nTotal sample size: 50,083 donors\nControl group: 16,687 people (33%)\nTreatment group: 33,396 people (67%)\n\nThe fundraising letter received contains instructions for matching donations and is randomly assigned to different matching ratio/maximum amount/suggested amount combinations\nLoading dataset\n\n\nCode\nimport pandas as pd\ndata = pd.read_stata('/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project1/karlan_list_2007.dta')\ndata.head()\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI will select several non-significant variables to examine whether there are statistically significant differences (95% confidence level) between the experimental and control groups on these background characteristics. Each variable is analyzed using two methods: one is a t-test, and the other is to estimate the effect of treatment on the variable through simple linear regression, and compare whether the two methods give consistent results.\n\nVariable selections\n\nmrm2: Number of months since last donation\nltmedmra: Small prior donor: last gift was less than median $35\ncouple: Couple\n\nSplit the data into treatment and control groups\n\n\nCode\ntreatment_data = data[data['treatment'] == 1]\ncontrol_data = data[data['treatment'] == 0]\n\n\nT-test for mrm2, ltmedmra, couple\n\n\nCode\ncolumns = ['mrm2', 'ltmedmra', 'couple']\n\nt_stats = {}\n\nfor col in columns:\n\n    treatment_values = [x for x in treatment_data[col] if x == x]\n    control_values = [x for x in control_data[col] if x == x]\n\n    n1 = len(treatment_values)\n    n2 = len(control_values)\n\n    mean1 = sum(treatment_values) / n1\n    mean2 = sum(control_values) / n2\n\n    var1 = sum((x - mean1)**2 for x in treatment_values) / (n1 - 1)\n    var2 = sum((x - mean2)**2 for x in control_values) / (n2 - 1)\n\n    se = ((var1 / n1) + (var2 / n2)) ** 0.5\n\n    t_stat = (mean1 - mean2) / se\n\n    t_stats[col] = t_stat\n\n\nLinear regression for mrm2, ltmedmra, couple\n\n\nCode\nimport statsmodels.api as sm\n\nif 'intercept' not in data.columns:\n    data['intercept'] = 1\n\ncolumns_to_analyze = ['mrm2', 'ltmedmra', 'couple']\n\nregression_results = {}\n\nfor col in columns_to_analyze:\n    model = sm.OLS(data[col], data[['intercept', 'treatment']], missing='drop').fit()\n    t_stat = model.tvalues['treatment'].round(4)\n    p_value = model.pvalues['treatment'].round(4)\n    regression_results[col] = {'t-stat': t_stat, 'p-value': p_value}\n\n\nThe result of t-test and linear regression for mrm2, ltmedmra, couple\n\n\nCode\ncombined_t_stats_df = pd.DataFrame({\n    \"Variable\": list(t_stats.keys()) + list(regression_results.keys()),\n    \"T-statistic\": [round(value, 4) for value in t_stats.values()] + [result['t-stat'] for result in regression_results.values()],\n    \"Method\": [\"T-test\"] * len(t_stats) + [\"Regression\"] * len(regression_results)\n})\ncombined_t_stats_df\n\n\n\n\n\n\n\n\n\nVariable\nT-statistic\nMethod\n\n\n\n\n0\nmrm2\n0.1195\nT-test\n\n\n1\nltmedmra\n1.9099\nT-test\n\n\n2\ncouple\n-0.5823\nT-test\n\n\n3\nmrm2\n0.1195\nRegression\n\n\n4\nltmedmra\n1.9097\nRegression\n\n\n5\ncouple\n-0.5838\nRegression\n\n\n\n\n\n\n\nFirst, from the results of t-test:\n\nThe t-value of mrm2 was 0.1195, indicating that there was no significant difference between the treatment group and the control group.\nThe t-value of ltmedmra is 1.9099, which is close to the statistically significant level (usually the critical value is about 1.96), indicating that the difference between the treatment group and the control group in this variable is potentially significant.\nThe T value of couple was -0.5823, indicating that there was no significant difference in this variable between the two groups.\n\nNext, we further verified the responses of these variables to the treatment effects through linear regression. In regression analysis, we treat each variable as a dependent variable and the treatment variable (treatment) as an independent variable, and observe the estimated value of its coefficient and the T statistic:\n\nThe treatment coefficient t-value of mrm2 is still 0.1195, which is consistent with the T test, indicating that the treatment has no effect on this variable.\nThe regression t-value of ltmedmra is 1.9097, which is almost consistent with the T-test, further strengthening the inference that this variable may be affected by the treatment.\nThe regression t-value of couple is -0.5838, which is also close to the T-test, indicating that the treatment has no significant effect.\n\nThese results can be compared with Table 1 in the paper by Karlan and List. This table mainly presents the average values ​​and differences of various basic characteristics between the treatment group and the control group, with the aim of verifying whether the random assignment is successful. If there is no significant difference between the two groups on most variables, it can be reasonably inferred that the sample allocation is random, and subsequent causal inferences are more credible."
  },
  {
    "objectID": "blog/Project1/index.html#experimental-results",
    "href": "blog/Project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nBar plot of the proportion of peole donated between treatment and controal group.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ntreatment_prop = treatment_data['gave'].mean()\ncontrol_prop = control_data['gave'].mean()\n\nplt.bar(['Treatment', 'Control'], [treatment_prop, control_prop], color=['pink', 'lightblue'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors in Treatment and Control')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe purpose of the following analysis is to compare the differences between the experimental group and the control group in terms of whether or not they donated. I will first use a t-test to preliminarily check whether there is a significant difference in the donation rates of the two groups, and then use a simple linear regression model with donation behavior as the dependent variable and the experimental treatment as the independent variable to further verify whether the results are consistent. These results will help us determine whether paired donation reminders can effectively enhance donation behavior and explore donors’ behavioral responses and potential psychological motivations. In addition, the data will be compared with Table 2A in the paper to confirm the consistency of the analysis direction with the original research.\nT test: compare whether there is a significant difference in the donation rate between the treatment and control groups\n\n\nCode\nfrom scipy import stats\ntreatment_gave = treatment_data['gave']\ncontrol_gave = control_data['gave']\n\nmean_treatment = treatment_gave.mean()\nmean_control = control_gave.mean()\n\nvar_treatment = treatment_gave.var(ddof=1)\nvar_control = control_gave.var(ddof=1)\n\nn_treatment = len(treatment_gave)\nn_control = len(control_gave)\n\nse = ((var_treatment / n_treatment) + (var_control / n_control)) ** 0.5\n\nt_stat_manual = (mean_treatment - mean_control) / se\n\ndf = ((var_treatment / n_treatment + var_control / n_control) ** 2) / \\\n    (((var_treatment / n_treatment) ** 2) / (n_treatment - 1) + ((var_control / n_control) ** 2) / (n_control - 1))\n\np_value_manual = 2 * (1 - stats.t.cdf(abs(t_stat_manual), df))\n\ngave_t_test_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_manual.round(4), p_value_manual.round(4)]\n})\n\ngave_t_test_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n3.2095\n\n\n1\np-value\n0.0013\n\n\n\n\n\n\n\nWe first conducted an independent sample t-test on the binary variable gave. The results showed that the t-value and the p-value indicating that the difference in donation rates between the treatment group and the control group was statistically significant at a 95% confidence level. This suggests that simply including the phrase “your donation will be matched” in your fundraising email can significantly increase donation rates.\nLinear regression: Using OLS to test the effect of treatment on donation behavior\n\n\nCode\ngave_model = sm.OLS(data['gave'], data[['intercept', 'treatment']], missing='drop').fit()\n\ngave_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [gave_model.tvalues['treatment'].round(4), gave_model.pvalues['treatment'].round(4)]\n})\ngave_model_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n3.1014\n\n\n1\np-value\n0.0019\n\n\n\n\n\n\n\nTo verify this, we used simple linear regression with gave as the dependent variable and treatment as the independent variable. The results showed that the t-value and p-value were almost consistent with the t-test results, proving that the two methods are consistent when analyzing this type of binary outcome variable.\nThe proportion of respsonse rate in control gorup and treatment group\n\n\nCode\nproportions_df = pd.DataFrame({\n    \"Group\": [\"Control\", \"Treatment\"],\n    \"Proportion\": [control_prop, treatment_prop]\n})\nproportions_df\n\n\n\n\n\n\n\n\n\nGroup\nProportion\n\n\n\n\n0\nControl\n0.017858\n\n\n1\nTreatment\n0.022039\n\n\n\n\n\n\n\nThis result is also consistent with the data in Table 2A of the original text (1.8% in the control group and 2.2% in the experimental group). From a behavioral economics perspective, this stable difference may be because when people see the message that “your donation will be matched,” they feel that their donation is more valuable and more influential. This feeling will make them more willing to donate. It’s like people feel a sense of satisfaction when they donate, and the message of matching donations makes this satisfaction even stronger, thus increasing their willingness to act.\nOverall, this analysis supports the original authors’ conclusion: even without changing the amount, providing matching information can effectively increase the likelihood of donations, which has important implications for practical fundraising strategies.\nNext, I will conduct a Probit regression analysis to test the impact of “whether or not to receive a matching donation reminder” (treatment) on the outcome of “whether or not to donate” (gave, a variable of 0 or 1). This model can help you estimate the effect of the matching message on the probability of donating and can be used to verify whether your results are consistent with the analysis results in column 1 of Table 3 of the paper. This step is to confirm whether you have successfully reproduced the main conclusions of the original study.\nProbit Regression: Estimating the Effect of Pairing Prompts on the Probability of Donating\n\n\nCode\nprobit_model = sm.Probit(data['gave'], data[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\n\ncoefficients = probit_results.params\nt_values = probit_results.tvalues\n\nprobit_summary_df = pd.DataFrame({\n    \"Variable\": coefficients.index,\n    \"Coefficient\": coefficients.values,\n    \"T-value\": t_values.values\n})\nprobit_summary_df.round(4)\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nT-value\n\n\n\n\n0\nintercept\n-2.1001\n-90.0728\n\n\n1\ntreatment\n0.0868\n3.1129\n\n\n\n\n\n\n\nMarginal effect analysis: explaining the actual effect of treatment on the probability of donating\n\n\nCode\nmarginal_effects = probit_results.get_margeff()\nmarginal_summary = marginal_effects.summary_frame()\n\nmarginal_summary = marginal_summary.reset_index().rename(columns={\n    'index': 'Variable',\n    'dy/dx': 'Marginal Effect (dy/dx)',\n    'Std. Err.': 'Std. Error',\n    'z': 'z',\n    'P&gt;|z|': 'P-value',\n    '[0.025': 'CI Lower',\n    '0.975]': 'CI Upper'\n})\n\nmarginal_summary.round(4)\n\n\n\n\n\n\n\n\n\nVariable\nMarginal Effect (dy/dx)\nStd. Error\nz\nPr(&gt;|z|)\nConf. Int. Low\nCont. Int. Hi.\n\n\n\n\n0\ntreatment\n0.0043\n0.0014\n3.1044\n0.0019\n0.0016\n0.007\n\n\n\n\n\n\n\nThe results are completely consistent with column 1 of Table 3 , successfully replicating the reported analysis. This means that the pairing prompt can significantly increase the probability of people donating, and even if the effect is small, it is statistically stable and significant. In the Probit model, the original coefficient cannot be directly interpreted as “how much the donation rate increased”, but it can be converted into a marginal effect. We can see from the Probit marginal effect model that the result is 0.0043, which corresponds exactly to 0.004 in the first column of Table 3.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nThrough t-test, we examine whether different matching ratios affect people’s donation behavior. Specifically, you will compare whether there are statistically significant differences in the donation rates of subjects under the 1:1, 2:1, and 3:1 pairing conditions. This will help you assess whether a higher or lower matching ratio has an additional impact on donation willingness.\nT-tests for match ratio effects on donation\n\n\nCode\nr1 = treatment_data[treatment_data['ratio'] == 1]\nr2 = treatment_data[treatment_data['ratio'] == 2]\nr3 = treatment_data[treatment_data['ratio'] == 3]\n\nmean_r1 = r1['gave'].mean()\nmean_r2 = r2['gave'].mean()\nmean_r3 = r3['gave'].mean()\n\nvar_r1 = r1['gave'].var(ddof=1)\nvar_r2 = r2['gave'].var(ddof=1)\nvar_r3 = r3['gave'].var(ddof=1)\n\nn_r1 = len(r1['gave'])\nn_r2 = len(r2['gave'])\nn_r3 = len(r3['gave'])\n\nse_1v2 = ((var_r1 / n_r1) + (var_r2 / n_r2)) ** 0.5\nse_2v3 = ((var_r2 / n_r2) + (var_r3 / n_r3)) ** 0.5\n\nt_stat_1v2 = (mean_r1 - mean_r2) / se_1v2\nt_stat_2v3 = (mean_r2 - mean_r3) / se_2v3\n\ndf_1v2 = ((var_r1 / n_r1 + var_r2 / n_r2) ** 2) / \\\n         (((var_r1 / n_r1) ** 2) / (n_r1 - 1) + ((var_r2 / n_r2) ** 2) / (n_r2 - 1))\ndf_2v3 = ((var_r2 / n_r2 + var_r3 / n_r3) ** 2) / \\\n         (((var_r2 / n_r2) ** 2) / (n_r2 - 1) + ((var_r3 / n_r3) ** 2) / (n_r3 - 1))\n\np_value_1v2 = 2 * (1 - stats.t.cdf(abs(t_stat_1v2), df_1v2))\np_value_2v3 = 2 * (1 - stats.t.cdf(abs(t_stat_2v3), df_2v3))\n\nt_test_results_df = pd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"2:1 vs 3:1\"],\n    \"T-statistic\": [t_stat_1v2, t_stat_2v3],\n    \"P-value\": [p_value_1v2, p_value_2v3]\n})\n\nt_test_results_df\n\n\n\n\n\n\n\n\n\nComparison\nT-statistic\nP-value\n\n\n\n\n0\n1:1 vs 2:1\n-0.965049\n0.334531\n\n\n1\n2:1 vs 3:1\n-0.050116\n0.960031\n\n\n\n\n\n\n\nIn the paired donation prompt group, there were no significant behavioral differences between the different pairing ratios (1:1, 2:1, and 3:1).The t-value of the 1:1 and 2:1 groups is -0.965, and the p-value is 0.3345, indicating that we cannot reject the null hypothesis and there is no statistically significant difference in the donation rates between the two groups. The difference between the 2:1 and 3:1 groups is even smaller, with a t-value of only -0.0501 and a corresponding p-value of 0.96, indicating that there is no difference in donation behavior between the two groups.\nThe analysis results of the t-test support the author’s observations on page 8 of the paper. The authors note that while the pairing prompt itself increased donation rates, further increasing the pairing ratio (from 1:1 to 2:1 or 3:1) in the pairing prompt group did not lead to additional effects. The t-test you conducted also clearly reflects this point: the difference in donation rates between different matching ratios is not statistically significant, and the p-values ​​are all far higher than the traditional significance level, especially the difference between 2:1 and 3:1 is almost zero. This shows that in actual donation behavior, people are more sensitive to whether there is a match rather than the size of the matching ratio.\nRegression analysis was used to assess the effects of different pairing ratios (1:1, 2:1, 3:1) on donation behavior. The specific approach is to establish a linear regression model, with gave (whether to donate) as the dependent variable and three dummy variables representing the pairing ratios (ratio1, ratio2, ratio3) as independent variables. This allows us to simultaneously compare the effects of each pairing condition on the donation rate and analyze the regression coefficient of each variable and its statistical significance. Using this model, you will be able to determine whether a particular pairing ratio is particularly effective and whether the results are explanatory and stable.\nRegression analysis for match ratio effects\n\n\nCode\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\nratio_model = sm.OLS(data['gave'], data[['intercept', 'ratio1', 'ratio2', 'ratio3']], missing='drop').fit()\n\n\nAccording to the regression results, we observed that different pairing ratios do have an impact on donation behavior, but the strength of the effect varies. The donation rate for ratio1 is about 0.29 percentage points higher, which is positive but only slightly statistically significant. The effects of the paired groups of ratio2 and ratio3 are more obvious, with the donation rates being approximately 0.48 and 0.49 percentage points higher than the benchmark group, respectively, and are significant at the 1% significance level.\nThis means that as long as there is matching information, even ratio 1 may increase people’s willingness to donate, and increasing the matching ratio to ratio 2 or ratio 3 will further strengthen this incentive. However, the effects of ratio2 and ratio3 are similar and almost the same, indicating that the marginal benefit of increasing the pairing ratio tends to be flat or saturated. This is consistent with the authors’ observation in the paper that higher pairing ratios do not necessarily produce additional significant behavioral changes.\nResponse rate differences between different matching ratio\n\n\nCode\nresp_rate_1 = r1['gave'].mean()\nresp_rate_2 = r2['gave'].mean()\nresp_rate_3 = r3['gave'].mean()\n\ndiff_1v2 = resp_rate_2 - resp_rate_1\ndiff_2v3 = resp_rate_3 - resp_rate_2\n\nprint(f\"Response rate difference 1:1 vs 2:1: {diff_1v2:.4f}\")\nprint(f\"Response rate difference 2:1 vs 3:1: {diff_2v3:.4f}\")\n\n\nResponse rate difference 1:1 vs 2:1: 0.0019\nResponse rate difference 2:1 vs 3:1: 0.0001\n\n\nThe difference between the donation rates is very small, and further increasing the matching ratio, for example from ratio1 to ratio2 or ratio3, has very limited effect on the donation rate.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI will use a t-test to exclude missing values ​​from the two groups and then compare whether there is a statistically significant difference in the average donation amounts of the two groups. This is a statistical method commonly used to compare whether two groups of means are different. A simple linear regression model was established, with the donation amount as the dependent variable and the treatment status as the explanatory variable, to examine whether there was a significant difference in the donation amount between the experimental group and the control group before controlling other variables.\nT-test for donation amount\n\n\nCode\ntreatment_amount = treatment_data['amount'].dropna()\ncontrol_amount = control_data['amount'].dropna()\n\nmean_treatment_amount = treatment_amount.mean()\nmean_control_amount = control_amount.mean()\n\nvar_treatment_amount = treatment_amount.var(ddof=1)\nvar_control_amount = control_amount.var(ddof=1)\n\nn_treatment_amount = len(treatment_amount)\nn_control_amount = len(control_amount)\n\nse_amount = ((var_treatment_amount / n_treatment_amount) + (var_control_amount / n_control_amount)) ** 0.5\n\nt_stat_amount_manual = (mean_treatment_amount - mean_control_amount) / se_amount\n\ndf_amount = ((var_treatment_amount / n_treatment_amount + var_control_amount / n_control_amount) ** 2) / \\\n    (((var_treatment_amount / n_treatment_amount) ** 2) / (n_treatment_amount - 1) + \n     ((var_control_amount / n_control_amount) ** 2) / (n_control_amount - 1))\n\np_value_amount_manual = 2 * (1 - stats.t.cdf(abs(t_stat_amount_manual), df_amount))\n\nt_test_amount_manual_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_amount_manual.round(4), p_value_amount_manual.round(4)]\n})\n\nt_test_amount_manual_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n1.9182\n\n\n1\np-value\n0.0551\n\n\n\n\n\n\n\nBivariate linear regression for donation amount\n\n\nCode\namount_model = sm.OLS(data['amount'], data[['intercept', 'treatment']], missing='drop').fit()\n\namount_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"], \n    \"Value\": [amount_model.tvalues['treatment'].round(4), amount_model.pvalues['treatment'].round(4)]\n})\namount_model_results\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nt-statistic\n1.8605\n\n\n1\np-value\n0.0628\n\n\n\n\n\n\n\nWhen we analyzed the donation amount, both the independent sample t-test and the bivariate linear regression showed that the average donation amount of the treatment group was slightly higher than that of the control group, but the difference was only marginally significant. The p-value of the t-test is 0.0551, and the p-value of the regression is 0.063, both slightly higher than the traditional 5% significance level.\nOverall, the matching prompt has a clear impact on whether to donate, while the impact of amount is weaker. From a behavioral perspective, the matching message is more like a “motivation switch” that prompts people to take action rather than a reinforcement tool that influences the amount of donations. This also means that in terms of fundraising strategy, matching donations are more suitable as an incentive to guide donation behavior rather than a means to increase the single amount.\nNext, we will conduct a regression analysis on those who actually donated to assess whether the paired prompt (treatment) affects the amount they donated. First, the program will filter out all observations with donation amounts greater than 0 from the data, and then build a simple linear regression model with the donation amount as the dependent variable and whether or not the matching prompt was received as the independent variable.\nConditional donation amount regression analysis: evaluating the impact and explanatory power of matching prompts only for actual donors\n\n\nCode\ndonors = data[data['amount'] &gt; 0]\n\ndonors_model = sm.OLS(donors['amount'], donors[['intercept', 'treatment']], missing='drop').fit()\n\ntreatment_coef = donors_model.params['treatment']\n\ntreatment_coef_df = pd.DataFrame({\n    \"Metric\": [\"Treatment Coefficient\"],\n    \"Value\": [treatment_coef.round(4)]\n})\n\ntreatment_coef_df\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nTreatment Coefficient\n-1.6684\n\n\n\n\n\n\n\nIn this regression analysis of those who have already donated, the treatment coefficient is -1.6684, it means that the average donation amount of the treatment group is about 1.67 yuan lower than that of the control group., however, this result is not statistically significant because the p-value is only 0.561, indicating that the matching prompt has no stable effect on the amount donated by those who have already decided to donate. It should be noted that this regression result cannot be interpreted as a causal effect of treatment on the amount of donations, because the analysis is limited to people who actually donated. This is a conditional subsample and not a random assignment, so there is a risk of selection bias. Taken together, our findings suggest that matching donation prompts are more likely to influence the behavior of whether to donate rather than the amount of donation.\nCompare the distribution of donation amounts among those who actually donated in the treatment group and the control group. First, the program will screen out the subjects in the two groups whose donation amount is greater than 0, and then calculate the average donation amount of each group.\nCompare the distribution of donations between treatment and control groups (limited to donors)\n\n\nCode\ntreatment_donors = treatment_data[treatment_data['amount'] &gt; 0]\ncontrol_donors = control_data[control_data['amount'] &gt; 0]\n\ntreatment_avg = treatment_donors['amount'].mean()\ncontrol_avg = control_donors['amount'].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\naxes[0].hist(treatment_donors['amount'], bins=30, color='skyblue', alpha=0.7)\naxes[0].axvline(treatment_avg, color='red', linestyle='--', label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group: Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\naxes[1].hist(control_donors['amount'], bins=30, color='pink', alpha=0.7)\naxes[1].axvline(control_avg, color='red', linestyle='--', label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group: Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/Project1/index.html#simulation-experiment",
    "href": "blog/Project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe Law of Large Numbers is demonstrated through a simulation. 100,000 records are simulated from the control group (donation rate 1.8%) and 10,000 records are simulated from the treatment group (donation rate 2.2%). Then, the same number of samples are randomly selected from the control data to pair with the treatment data. The difference between the treatment and control donation results is calculated for each\nSimulation of the Law of Large Numbers: Cumulative Average Difference in Donation Rates\n\n\nCode\nimport numpy as np\n#calculation\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(n=1, p=0.018, size=100000)\ntreatment_draws = np.random.binomial(n=1, p=0.022, size=10000)\ncontrol_sample = np.random.choice(control_draws, size=10000, replace=False)\ndiff = treatment_draws - control_sample\ncumulative_avg = np.cumsum(diff) / np.arange(1, len(diff) + 1)\n#plot\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative mean difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label=\"Theoretical mean difference = 0.004\")\nplt.title(\"Simulation: Cumulative Average Difference in Donation Rates)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis graph shows the difference in donation rates between the treatment group and the control group, calculated through simulations, as the number of samples increases. As can be seen from the figure, although the difference fluctuates greatly when the number of samples is small, as the number of samples gradually increases, the cumulative average curve steadily approaches the theoretical true difference value of 0.004.\nThis is a typical manifestation of the Law of Large Numbers: when we observe enough samples, the mean of the samples will approach the true mean of the population. This also means that the difference in donation rates observed in the original experiment (the slightly higher donation rate in the treatment group than in the control group) was not caused by random errors, but was a stable and reproducible result.\nTherefore, we can reasonably say that the simulation results in this figure verify the stability and credibility of the treatment effect and provide strong visual evidence to support that the observations in the experiment are reliable.\n\n\nCentral Limit Theorem\nNext, we will show the distribution of the average difference in donation rates between the treatment group and the control group under different sample sizes (50, 200, 500, 1000). For each sample size, 1000 random draws were made, taking an equal number of samples from the simulated distributions for both treatment and control, calculating the mean differences between the two groups, and plotting these differences in a histogram.\n\n\nCode\nimport numpy as np\n\ncontrol_draws = np.random.binomial(1, 0.018, 100000)\ntreatment_draws = np.random.binomial(1, 0.022, 10000)\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n\n    for _ in range(1000):\n        treatment_sample = np.random.choice(treatment_draws, size=sample_size, replace=True)\n        control_sample = np.random.choice(control_draws, size=sample_size, replace=True)\n        avg_differences.append(np.mean(treatment_sample - control_sample))\n\n    axes[i].hist(avg_differences, bins=30, color='pink', edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese four histograms show the simulated distribution changes of the difference in donation rates between the treatment group and the control group under different sample sizes (50, 200, 500, 1000). When the sample size is small, the distribution is more dispersed, and 0 almost falls in the center, which means that it is impossible to determine whether the treatment effect is significant. However, as the number of samples increases, the distribution begins to become concentrated and biased toward positive differences, especially when the number of samples reaches 500 or 1000, when 0 is clearly off the center and falls in the left tail of the distribution. This means that when the sample size is sufficient, the treatment group does show a stable and positive effect, and the average donation rate is higher than that of the control group. This difference is unlikely to be caused by random errors. Overall, this set of charts reinforces a basic principle in statistical inference: the larger the sample size, the more stable the results and the more reliably they reveal true behavioral differences."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html",
    "href": "blog/Project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#introduction",
    "href": "blog/Project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#data",
    "href": "blog/Project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#experimental-results",
    "href": "blog/Project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "blog/Project1/hw1_questions.html#simulation-experiment",
    "href": "blog/Project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/Project1/project1.html",
    "href": "blog/Project1/project1.html",
    "title": "Lulu's Marketing Analytics Project",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\n\n\ndata = pd.read_stata('/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project1/karlan_list_2007.dta')\ndata\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\ndata.to_excel('output_data.xlsx', index=False)\n\n\ntreatment_data = data[data['treatment'] == 1]\n\n\ncontrol_data = data[data['treatment'] == 0]\n\n\nfrom scipy.stats import ttest_ind\n\n# T-test for mrm2\ntreatment_mrm2 = treatment_data['mrm2'].dropna()\ncontrol_mrm2 = control_data['mrm2'].dropna()\nt_stat, p_value = ttest_ind(treatment_mrm2, control_mrm2, equal_var=False)\nprint(f\"T-test for mrm2: t-statistic = {t_stat.round(4)}, p-value = {p_value.round(4)}\")\n\nT-test for mrm2: t-statistic = 0.1195, p-value = 0.9049\n\n\n\n# 去除缺漏值\ntreatment_mrm2 = [x for x in treatment_data['mrm2'] if x == x]\ncontrol_mrm2 = [x for x in control_data['mrm2'] if x == x]\n\n# 計算樣本數\nn1 = len(treatment_mrm2)\nn2 = len(control_mrm2)\n\n# 計算平均\nmean1 = sum(treatment_mrm2) / n1\nmean2 = sum(control_mrm2) / n2\n\n# 計算變異數（無偏估計，分母用 n-1）\nvar1 = sum((x - mean1)**2 for x in treatment_mrm2) / (n1 - 1)\nvar2 = sum((x - mean2)**2 for x in control_mrm2) / (n2 - 1)\n\n# 計算標準誤\nse = ((var1 / n1) + (var2 / n2)) ** 0.5\n\n# 計算 t 統計量\nt_stat = (mean1 - mean2) / se\n\nprint(f\"T-test (by formula) for mrm2: t-statistic = {round(t_stat, 4)}\")\n\nT-test (by formula) for mrm2: t-statistic = 0.1195\n\n\n\nimport statsmodels.api as sm\n\n# Linear regression for mrm2\ndata['intercept'] = 1 \nmodel = sm.OLS(data['mrm2'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(model.summary())\n# Extract the t-statistic for the 'treatment' coefficient from the regression model\nregression_t_stat = model.tvalues['treatment'].round(4)\nprint(f\"Regression model t-statistic: {regression_t_stat}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   mrm2   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01428\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):              0.905\nTime:                        16:22:47   Log-Likelihood:            -1.9585e+05\nNo. Observations:               50082   AIC:                         3.917e+05\nDf Residuals:                   50080   BIC:                         3.917e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\nOmnibus:                     8031.352   Durbin-Watson:                   2.004\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            12471.135\nSkew:                           1.163   Prob(JB):                         0.00\nKurtosis:                       3.751   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nRegression model t-statistic: 0.1195\n\n\n\nimport matplotlib.pyplot as plt\n\ntreatment_proportion = treatment_data['gave'].mean()\ncontrol_proportion = control_data['gave'].mean()\n\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['pink', 'lightblue'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n# T-test for the binary outcome 'gave'\ntreatment_gave = treatment_data['gave']\ncontrol_gave = control_data['gave']\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\nprint(f\"T-test for 'gave': t-statistic = {t_stat_gave.round(4)}, p-value = {p_value_gave.round(4)}\")\n\n# Bivariate linear regression for 'gave'\ngave_model = sm.OLS(data['gave'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(gave_model.summary())\nprint(f\"Linear regression for 'gave': t-statistic = {gave_model.tvalues['treatment'].round(4)}\")\n\nT-test for 'gave': t-statistic = 3.2095, p-value = 0.0013\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        16:22:47   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLinear regression for 'gave': t-statistic = 3.1014\n\n\n\nprint(f\"Control Proportion: {control_proportion:.3f}\")\nprint(f\"Treatment Proportion: {treatment_proportion:.3f}\")\n\nControl Proportion: 0.018\nTreatment Proportion: 0.022\n\n\n\n# Probit regression for charitable donation\nprobit_model = sm.Probit(data['gave'], data[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n# 邊際效應估計\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Tue, 22 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        00:08:21   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Task 1: T-tests for match ratio effects on donation likelihood\n\n# Filter data for each match ratio\nratio1_data = treatment_data[treatment_data['ratio'] == 1]\nratio2_data = treatment_data[treatment_data['ratio'] == 2]\nratio3_data = treatment_data[treatment_data['ratio'] == 3]\n\n# Perform t-tests\nt_stat_1v2, p_value_1v2 = ttest_ind(ratio1_data['gave'], ratio2_data['gave'], equal_var=False)\nt_stat_2v3, p_value_2v3 = ttest_ind(ratio2_data['gave'], ratio3_data['gave'], equal_var=False)\n\nprint(f\"T-test 1:1 vs 2:1 - t-statistic: {t_stat_1v2:.4f}, p-value: {p_value_1v2:.4f}\")\nprint(f\"T-test 2:1 vs 3:1 - t-statistic: {t_stat_2v3:.4f}, p-value: {p_value_2v3:.4f}\")\n\n\n\n\nT-test 1:1 vs 2:1 - t-statistic: -0.9650, p-value: 0.3345\nT-test 2:1 vs 3:1 - t-statistic: -0.0501, p-value: 0.9600\n\n\n\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\nratio_model = sm.OLS(data['gave'], data[['intercept', 'ratio1', 'ratio2', 'ratio3']], missing='drop').fit()\nprint(ratio_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Tue, 22 Apr 2025   Prob (F-statistic):             0.0118\nTime:                        11:02:58   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n# Task 3: Response rate differences\n# Direct calculation from data\nresponse_rate_1 = ratio1_data['gave'].mean()\nresponse_rate_2 = ratio2_data['gave'].mean()\nresponse_rate_3 = ratio3_data['gave'].mean()\n\ndiff_1v2 = response_rate_2 - response_rate_1\ndiff_2v3 = response_rate_3 - response_rate_2\n\nprint(f\"Response rate difference 1:1 vs 2:1: {diff_1v2:.4f}\")\nprint(f\"Response rate difference 2:1 vs 3:1: {diff_2v3:.4f}\")\n\nResponse rate difference 1:1 vs 2:1: 0.0019\nResponse rate difference 2:1 vs 3:1: 0.0001\n\n\n\n# Differences from regression coefficients\ncoef_diff_1v2 = ratio_model.params['ratio2'] - ratio_model.params['ratio1']\ncoef_diff_2v3 = ratio_model.params['ratio3'] - ratio_model.params['ratio2']\n\nprint(f\"Coefficient difference 1:1 vs 2:1: {coef_diff_1v2:.4f}\")\nprint(f\"Coefficient difference 2:1 vs 3:1: {coef_diff_2v3:.4f}\")\n\nCoefficient difference 1:1 vs 2:1: 0.0019\nCoefficient difference 2:1 vs 3:1: 0.0001\n\n\n\n# T-test for donation amount\ntreatment_amount = treatment_data['amount'].dropna()\ncontrol_amount = control_data['amount'].dropna()\nt_stat_amount, p_value_amount = ttest_ind(treatment_amount, control_amount, equal_var=False)\nprint(f\"T-test for donation amount: t-statistic = {t_stat_amount:.4f}, p-value = {p_value_amount:.4f}\")\n\n# Bivariate linear regression for donation amount\namount_model = sm.OLS(data['amount'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(amount_model.summary())\nprint(f\"Linear regression for donation amount: t-statistic = {amount_model.tvalues['treatment']:.4f}, p-value = {amount_model.pvalues['treatment']:.4f}\")\n\nT-test for donation amount: t-statistic = 1.9183, p-value = 0.0551\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        16:22:48   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLinear regression for donation amount: t-statistic = 1.8605, p-value = 0.0628\n\n\n\n# Filter data to include only those who made a donation\ndonors_data = data[data['amount'] &gt; 0]\n\n# Regression analysis for donation amount conditional on donating\ndonors_model = sm.OLS(donors_data['amount'], donors_data[['intercept', 'treatment']], missing='drop').fit()\nprint(donors_model.summary())\n\n# Interpretation of the treatment coefficient\ntreatment_coef = donors_model.params['treatment']\nprint(f\"Treatment coefficient: {treatment_coef:.4f}\")\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Mon, 21 Apr 2025   Prob (F-statistic):              0.561\nTime:                        16:22:48   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nTreatment coefficient: -1.6684\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 模擬參數\nn_control = 100000\nn_treatment = 10000\np_control = 0.018\np_treatment = 0.022\n\n# 隨機抽樣\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(n=1, p=p_control, size=n_control)\ntreatment_draws = np.random.binomial(n=1, p=p_treatment, size=n_treatment)\n\n# 從控制組中隨機抽出與 treatment 組一樣多的樣本\ncontrol_sample = np.random.choice(control_draws, size=n_treatment, replace=False)\n\n# 計算逐筆差異\ndifferences = treatment_draws - control_sample\n\n# 計算累積平均\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# 繪製圖形\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative mean difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label=\"Theoretical mean difference = 0.004\")\nplt.title(\"Simulation: Cumulative Average Difference in Donation Rates)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Filter data for donors only\ntreatment_donors = treatment_data[treatment_data['amount'] &gt; 0]\ncontrol_donors = control_data[control_data['amount'] &gt; 0]\n\n# Calculate sample averages\ntreatment_avg = treatment_donors['amount'].mean()\ncontrol_avg = control_donors['amount'].mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donors['amount'], bins=30, color='skyblue', alpha=0.7)\naxes[0].axvline(treatment_avg, color='red', linestyle='--', label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group: Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donors['amount'], bins=30, color='pink', alpha=0.7)\naxes[1].axvline(control_avg, color='red', linestyle='--', label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group: Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Initialize a figure for subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\n# Loop through each sample size\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n    \n    # Simulate 1000 averages\n    for _ in range(1000):\n        treatment_sample = np.random.choice(treatment_draws, size=sample_size, replace=True)\n        control_sample = np.random.choice(control_draws, size=sample_size, replace=True)\n        avg_differences.append(np.mean(treatment_sample - control_sample))\n    \n    # Plot histogram\n    axes[i].hist(avg_differences, bins=30, color='pink', edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/Project2/project1.html",
    "href": "blog/Project2/project1.html",
    "title": "Lulu's Marketing Analytics Project",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\n\n\ndata = pd.read_stata('/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project1/karlan_list_2007.dta')\ndata\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n50078\n1\n0\n1\n0\n0\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.872797\n0.089959\n0.257265\n2.13\n45047.0\n0.771316\n0.263744\n1.000000\n\n\n50079\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.688262\n0.108889\n0.288792\n2.67\n74655.0\n0.741931\n0.586466\n1.000000\n\n\n50080\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.900000\n0.021311\n0.178689\n2.36\n26667.0\n0.778689\n0.107930\n0.000000\n\n\n50081\n1\n0\n3\n0\n1\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.917206\n0.008257\n0.225619\n2.57\n39530.0\n0.733988\n0.184768\n0.634903\n\n\n50082\n1\n0\n3\n0\n1\n$25,000\n1\n0\n0\n0\n...\n0.0\n1.0\n0.530023\n0.074112\n0.340698\n3.70\n48744.0\n0.717843\n0.127941\n0.994181\n\n\n\n\n50083 rows × 51 columns\n\n\n\n\ndata.to_excel('output_data.xlsx', index=False)\n\n\ntreatment_data = data[data['treatment'] == 1]\n\n\ncontrol_data = data[data['treatment'] == 0]\n\n\n# Define the columns to calculate t-statistics for\ncolumns = ['mrm2', 'ltmedmra', 'couple']\n\n# Initialize a dictionary to store t-statistics\nt_stats = {}\n\n# Loop through each column\nfor col in columns:\n    # Filter non-NaN values for treatment and control groups\n    treatment_values = [x for x in treatment_data[col] if x == x]\n    control_values = [x for x in control_data[col] if x == x]\n\n    # Calculate sample sizes\n    n1 = len(treatment_values)\n    n2 = len(control_values)\n\n    # Calculate means\n    mean1 = sum(treatment_values) / n1\n    mean2 = sum(control_values) / n2\n\n    # Calculate variances\n    var1 = sum((x - mean1)**2 for x in treatment_values) / (n1 - 1)\n    var2 = sum((x - mean2)**2 for x in control_values) / (n2 - 1)\n\n    # Calculate standard error\n    se = ((var1 / n1) + (var2 / n2)) ** 0.5\n\n    # Calculate t-statistic\n    t_stat = (mean1 - mean2) / se\n\n    # Store the t-statistic in the dictionary\n    t_stats[col] = t_stat\n\n\nimport statsmodels.api as sm\n\n# 確保資料中有 'intercept' 欄位\nif 'intercept' not in data.columns:\n    data['intercept'] = 1\n\n# 定義要計算 t-stat 的欄位\ncolumns_to_analyze = ['mrm2', 'ltmedmra', 'couple']\n\n# 初始化一個字典來儲存結果\nregression_results = {}\n\n# 迴圈計算每個欄位的 t-stat 和 p-value\nfor col in columns_to_analyze:\n    model = sm.OLS(data[col], data[['intercept', 'treatment']], missing='drop').fit()\n    t_stat = model.tvalues['treatment'].round(4)\n    p_value = model.pvalues['treatment'].round(4)\n    regression_results[col] = {'t-stat': t_stat, 'p-value': p_value}\n\n\n# Combine T-test and Regression results into a single DataFrame\ncombined_t_stats_df = pd.DataFrame({\n    \"Variable\": list(t_stats.keys()) + list(regression_results.keys()),\n    \"T-statistic\": [round(value, 4) for value in t_stats.values()] + [result['t-stat'] for result in regression_results.values()],\n    \"Method\": [\"T-test\"] * len(t_stats) + [\"Regression\"] * len(regression_results)\n})\n\nprint(combined_t_stats_df)\n\n   Variable  T-statistic      Method\n0      mrm2       0.1195      T-test\n1  ltmedmra       1.9099      T-test\n2    couple      -0.5823      T-test\n3      mrm2       0.1195  Regression\n4  ltmedmra       1.9097  Regression\n5    couple      -0.5838  Regression\n\n\n\nimport matplotlib.pyplot as plt\n\ntreatment_proportion = treatment_data['gave'].mean()\ncontrol_proportion = control_data['gave'].mean()\n\nplt.bar(['Treatment', 'Control'], [treatment_proportion, control_proportion], color=['pink', 'lightblue'])\nplt.ylabel('Proportion of People Who Donated')\nplt.title('Proportion of Donors by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\ntreatment_gave = treatment_data['gave']\ncontrol_gave = control_data['gave']\n\n\n# T-test for the binary outcome 'gave'\nfrom scipy.stats import ttest_ind\nt_stat_gave, p_value_gave = ttest_ind(treatment_gave, control_gave, equal_var=False)\nprint(f\"T-test for 'gave': t-statistic = {t_stat_gave.round(4)}, p-value = {p_value_gave.round(4)}\")\n\n# Bivariate linear regression for 'gave'\ngave_model = sm.OLS(data['gave'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(gave_model.summary())\nprint(f\"Linear regression for 'gave': t-statistic = {gave_model.tvalues['treatment'].round(4)}\")\n\nT-test for 'gave': t-statistic = 3.2095, p-value = 0.0013\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     9.618\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\nTime:                        15:01:13   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.326e+04\nDf Residuals:                   50081   BIC:                        -5.324e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\nOmnibus:                    59814.280   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.440   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nLinear regression for 'gave': t-statistic = 3.1014\n\n\n\nfrom scipy import stats\n\n# Calculate means\nmean_treatment = treatment_gave.mean()\nmean_control = control_gave.mean()\n\n# Calculate variances\nvar_treatment = treatment_gave.var(ddof=1)\nvar_control = control_gave.var(ddof=1)\n\n# Calculate sample sizes\nn_treatment = len(treatment_gave)\nn_control = len(control_gave)\n\n# Calculate the pooled standard error\nse = ((var_treatment / n_treatment) + (var_control / n_control)) ** 0.5\n\n# Calculate the t-statistic\nt_stat_manual = (mean_treatment - mean_control) / se\n\n# Degrees of freedom for unequal variances (Welch's t-test)\ndf = ((var_treatment / n_treatment + var_control / n_control) ** 2) / \\\n    (((var_treatment / n_treatment) ** 2) / (n_treatment - 1) + ((var_control / n_control) ** 2) / (n_control - 1))\n\n# Calculate the p-value (two-tailed)\np_value_manual = 2 * (1 - stats.t.cdf(abs(t_stat_manual), df))\n\nprint(f\"t-statistic = {t_stat_manual.round(4)}, p-value = {p_value_manual.round(4)}\")\n\nt-statistic = 3.2095, p-value = 0.0013\n\n\n\n# Create a DataFrame to store the t-statistic and p-value\ngave_t_test_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_manual.round(4), p_value_manual.round(4)]\n})\n\nprint(gave_t_test_results)\n\n        Metric   Value\n0  t-statistic  3.2095\n1      p-value  0.0013\n\n\n\ngave_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [gave_model.tvalues['treatment'].round(4), gave_model.pvalues['treatment'].round(4)]\n})\n\nprint(gave_model_results)\n\n\nproportions_df = pd.DataFrame({\n    \"Group\": [\"Control\", \"Treatment\"],\n    \"Proportion\": [control_proportion, treatment_proportion]\n})\n\nprint(proportions_df)\n\n\nprint(f\"Control Proportion: {control_proportion:.3f}\")\nprint(f\"Treatment Proportion: {treatment_proportion:.3f}\")\n\n\n# Probit regression for charitable donation\nprobit_model = sm.Probit(data['gave'], data[['intercept', 'treatment']])\nprobit_results = probit_model.fit()\nprint(probit_results.summary())\n\n# 邊際效應估計\nmarginal_effects = probit_results.get_margeff()\nprint(marginal_effects.summary())\n\n\nfrom scipy.stats import ttest_ind\n\n# Task 1: T-tests for match ratio effects on donation likelihood\n\n# Filter data for each match ratio\nratio1_data = treatment_data[treatment_data['ratio'] == 1]\nratio2_data = treatment_data[treatment_data['ratio'] == 2]\nratio3_data = treatment_data[treatment_data['ratio'] == 3]\n\n# Perform t-tests\nt_stat_1v2, p_value_1v2 = ttest_ind(ratio1_data['gave'], ratio2_data['gave'], equal_var=False)\nt_stat_2v3, p_value_2v3 = ttest_ind(ratio2_data['gave'], ratio3_data['gave'], equal_var=False)\n\nprint(f\"T-test 1:1 vs 2:1 - t-statistic: {t_stat_1v2:.4f}, p-value: {p_value_1v2:.4f}\")\nprint(f\"T-test 2:1 vs 3:1 - t-statistic: {t_stat_2v3:.4f}, p-value: {p_value_2v3:.4f}\")\n\n\n\n\n\ndata['ratio1'] = (data['ratio'] == 1).astype(int)\n\nratio_model = sm.OLS(data['gave'], data[['intercept', 'ratio1', 'ratio2', 'ratio3']], missing='drop').fit()\nprint(ratio_model.summary())\n\n\n\n# Task 3: Response rate differences\n# Direct calculation from data\nresponse_rate_1 = ratio1_data['gave'].mean()\nresponse_rate_2 = ratio2_data['gave'].mean()\nresponse_rate_3 = ratio3_data['gave'].mean()\n\ndiff_1v2 = response_rate_2 - response_rate_1\ndiff_2v3 = response_rate_3 - response_rate_2\n\nprint(f\"Response rate difference 1:1 vs 2:1: {diff_1v2:.4f}\")\nprint(f\"Response rate difference 2:1 vs 3:1: {diff_2v3:.4f}\")\n\n\n# Differences from regression coefficients\ncoef_diff_1v2 = ratio_model.params['ratio2'] - ratio_model.params['ratio1']\ncoef_diff_2v3 = ratio_model.params['ratio3'] - ratio_model.params['ratio2']\n\nprint(f\"Coefficient difference 1:1 vs 2:1: {coef_diff_1v2:.4f}\")\nprint(f\"Coefficient difference 2:1 vs 3:1: {coef_diff_2v3:.4f}\")\n\n\n# T-test for donation amount\ntreatment_amount = treatment_data['amount'].dropna()\ncontrol_amount = control_data['amount'].dropna()\nt_stat_amount, p_value_amount = ttest_ind(treatment_amount, control_amount, equal_var=False)\nprint(f\"T-test for donation amount: t-statistic = {t_stat_amount:.4f}, p-value = {p_value_amount:.4f}\")\n\n# Bivariate linear regression for donation amount\namount_model = sm.OLS(data['amount'], data[['intercept', 'treatment']], missing='drop').fit()\nprint(amount_model.summary())\nprint(f\"Linear regression for donation amount: t-statistic = {amount_model.tvalues['treatment']:.4f}, p-value = {amount_model.pvalues['treatment']:.4f}\")\n\n\n# Filter data to include only those who made a donation\ndonors_data = data[data['amount'] &gt; 0]\n\n# Regression analysis for donation amount conditional on donating\ndonors_model = sm.OLS(donors_data['amount'], donors_data[['intercept', 'treatment']], missing='drop').fit()\nprint(donors_model.summary())\n\n# Interpretation of the treatment coefficient\ntreatment_coef = donors_model.params['treatment']\nprint(f\"Treatment coefficient: {treatment_coef:.4f}\")\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 模擬參數\nn_control = 100000\nn_treatment = 10000\np_control = 0.018\np_treatment = 0.022\n\n# 隨機抽樣\nnp.random.seed(42)\ncontrol_draws = np.random.binomial(n=1, p=p_control, size=n_control)\ntreatment_draws = np.random.binomial(n=1, p=p_treatment, size=n_treatment)\n\n# 從控制組中隨機抽出與 treatment 組一樣多的樣本\ncontrol_sample = np.random.choice(control_draws, size=n_treatment, replace=False)\n\n# 計算逐筆差異\ndifferences = treatment_draws - control_sample\n\n# 計算累積平均\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# 繪製圖形\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_avg, label=\"Cumulative mean difference\")\nplt.axhline(y=0.004, color='red', linestyle='--', label=\"Theoretical mean difference = 0.004\")\nplt.title(\"Simulation: Cumulative Average Difference in Donation Rates)\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# Filter data for donors only\ntreatment_donors = treatment_data[treatment_data['amount'] &gt; 0]\ncontrol_donors = control_data[control_data['amount'] &gt; 0]\n\n# Calculate sample averages\ntreatment_avg = treatment_donors['amount'].mean()\ncontrol_avg = control_donors['amount'].mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n\n# Treatment group histogram\naxes[0].hist(treatment_donors['amount'], bins=30, color='skyblue', alpha=0.7)\naxes[0].axvline(treatment_avg, color='red', linestyle='--', label=f'Avg: {treatment_avg:.2f}')\naxes[0].set_title('Treatment Group: Donation Amounts')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Frequency')\naxes[0].legend()\n\n# Control group histogram\naxes[1].hist(control_donors['amount'], bins=30, color='pink', alpha=0.7)\naxes[1].axvline(control_avg, color='red', linestyle='--', label=f'Avg: {control_avg:.2f}')\naxes[1].set_title('Control Group: Donation Amounts')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n# Define sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Initialize a figure for subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\n# Loop through each sample size\nfor i, sample_size in enumerate(sample_sizes):\n    avg_differences = []\n    \n    # Simulate 1000 averages\n    for _ in range(1000):\n        treatment_sample = np.random.choice(treatment_draws, size=sample_size, replace=True)\n        control_sample = np.random.choice(control_draws, size=sample_size, replace=True)\n        avg_differences.append(np.mean(treatment_sample - control_sample))\n    \n    # Plot histogram\n    axes[i].hist(avg_differences, bins=30, color='pink', edgecolor='black', alpha=0.7)\n    axes[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axes[i].set_title(f'Sample Size: {sample_size}')\n    axes[i].set_xlabel('Average Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n# Calculate means\nmean_r1 = r1['gave'].mean()\nmean_r2 = r2['gave'].mean()\nmean_r3 = r3['gave'].mean()\n\n# Calculate variances\nvar_r1 = r1['gave'].var(ddof=1)\nvar_r2 = r2['gave'].var(ddof=1)\nvar_r3 = r3['gave'].var(ddof=1)\n\n# Calculate sample sizes\nn_r1 = len(r1['gave'])\nn_r2 = len(r2['gave'])\nn_r3 = len(r3['gave'])\n\n# Calculate standard errors\nse_1v2 = ((var_r1 / n_r1) + (var_r2 / n_r2)) ** 0.5\nse_2v3 = ((var_r2 / n_r2) + (var_r3 / n_r3)) ** 0.5\n\n# Calculate t-statistics\nt_stat_1v2 = (mean_r1 - mean_r2) / se_1v2\nt_stat_2v3 = (mean_r2 - mean_r3) / se_2v3\n\n# Degrees of freedom for unequal variances (Welch's t-test)\ndf_1v2 = ((var_r1 / n_r1 + var_r2 / n_r2) ** 2) / \\\n         (((var_r1 / n_r1) ** 2) / (n_r1 - 1) + ((var_r2 / n_r2) ** 2) / (n_r2 - 1))\ndf_2v3 = ((var_r2 / n_r2 + var_r3 / n_r3) ** 2) / \\\n         (((var_r2 / n_r2) ** 2) / (n_r2 - 1) + ((var_r3 / n_r3) ** 2) / (n_r3 - 1))\n\n# Calculate p-values (two-tailed)\np_value_1v2 = 2 * (1 - stats.t.cdf(abs(t_stat_1v2), df_1v2))\np_value_2v3 = 2 * (1 - stats.t.cdf(abs(t_stat_2v3), df_2v3))\n\nprint(f\"T-test 1:1 vs 2:1 - t-statistic: {t_stat_1v2:.4f}, p-value: {p_value_1v2:.4f}\")\nprint(f\"T-test 2:1 vs 3:1 - t-statistic: {t_stat_2v3:.4f}, p-value: {p_value_2v3:.4f}\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 # Calculate means\n----&gt; 2 mean_r1 = r1['gave'].mean()\n      3 mean_r2 = r2['gave'].mean()\n      4 mean_r3 = r3['gave'].mean()\n\nNameError: name 'r1' is not defined\n\n\n\n\n# Create a DataFrame for the T-test results\nt_test_results_df = pd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"2:1 vs 3:1\"],\n    \"T-statistic\": [t_stat_1v2, t_stat_2v3],\n    \"P-value\": [p_value_1v2, p_value_2v3]\n})\n\nprint(t_test_results_df)\n\n\n# Calculate means\nmean_treatment_amount = treatment_amount.mean()\nmean_control_amount = control_amount.mean()\n\n# Calculate variances\nvar_treatment_amount = treatment_amount.var(ddof=1)\nvar_control_amount = control_amount.var(ddof=1)\n\n# Calculate sample sizes\nn_treatment_amount = len(treatment_amount)\nn_control_amount = len(control_amount)\n\n# Calculate the pooled standard error\nse_amount = ((var_treatment_amount / n_treatment_amount) + (var_control_amount / n_control_amount)) ** 0.5\n\n# Calculate the t-statistic\nt_stat_amount_manual = (mean_treatment_amount - mean_control_amount) / se_amount\n\n# Degrees of freedom for unequal variances (Welch's t-test)\ndf_amount = ((var_treatment_amount / n_treatment_amount + var_control_amount / n_control_amount) ** 2) / \\\n    (((var_treatment_amount / n_treatment_amount) ** 2) / (n_treatment_amount - 1) + \n     ((var_control_amount / n_control_amount) ** 2) / (n_control_amount - 1))\n\n# Calculate the p-value (two-tailed)\np_value_amount_manual = 2 * (1 - stats.t.cdf(abs(t_stat_amount_manual), df_amount))\n\nprint(f\"T-test for donation amount (manual): t-statistic = {t_stat_amount_manual:.4f}, p-value = {p_value_amount_manual:.4f}\")\n\n\nt_test_amount_manual_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [t_stat_amount_manual.round(4), p_value_amount_manual.round(4)]\n})\n\nprint(t_test_amount_manual_results)\n\n\namount_model_results = pd.DataFrame({\n    \"Metric\": [\"t-statistic\", \"p-value\"],\n    \"Value\": [amount_model.tvalues['treatment'].round(4), amount_model.pvalues['treatment'].round(4)]\n})\n\nprint(amount_model_results)\n\n\ntreatment_coef_df = pd.DataFrame({\n    \"Metric\": [\"Treatment Coefficient\"],\n    \"Value\": [treatment_coef.round(4)]\n})\n\nprint(treatment_coef_df)"
  },
  {
    "objectID": "blog/Project2/hw1_questions.html",
    "href": "blog/Project2/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#introduction",
    "href": "blog/Project2/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#data",
    "href": "blog/Project2/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#experimental-results",
    "href": "blog/Project2/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "blog/Project2/hw1_questions.html#simulation-experiment",
    "href": "blog/Project2/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/Project2/index.html",
    "href": "blog/Project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nReading blueprinty’s data\nimport pandas as pd\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head(5)\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nThis section conducts preliminary observations through EDA, with the aim of comparing the differences in patent output between companies using Blueprinty software and non-users. As can be seen from the chart, the proportion of Blueprinty users in the high patent number range is relatively high, and the overall patent performance is also more outstanding. Comparison of the average number of patents further shows that Blueprinty customers perform significantly better than non-customers. These preliminary results suggest that Blueprinty software may have a positive impact on patent applications, providing reasonable motivation and direction for the subsequent establishment of more rigorous statistical models.\n\n\nAverage number of patents by customer status\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"patents\", hue='iscustomer', kde=False, bins=15, palette=\"Set1\", multiple=\"stack\")\nplt.title(\"Distribution of Patent Counts by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nsns.barplot(data=blueprinty, x=\"iscustomer\", y=\"patents\", hue=\"iscustomer\", palette=\"Set1\", estimator=np.mean, dodge=False, legend=False)\nplt.title(\"Average Number of Patents by Customer Status\")\nplt.xlabel(\"Blueprinty Customer\")\nplt.ylabel(\"Average Patent Count\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe average number of patents by Blueprinty users is significantly higher than that of non-users.\nSuggests that companies using Blueprinty software may be more successful in obtaining patents.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nNext, we will continue to use ED to further confirm whether there are systematic differences in firm characteristics between Blueprinty customers and non-customers, which is crucial for the subsequent establishment of causal inference models (such as regression models).\n\n\nAverage number of patents by customer status\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", kde=True, bins=20, palette=\"Set2\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Distribution of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age (Years)\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize=\"index\") * 100\nregion_counts.plot(kind=\"bar\", stacked=True, ax=plt.gca(), colormap=\"Set2\")\nplt.title(\"Regional Composition by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Percentage (%)\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBlueprinty users (green) and non-users (purple) have slightly different overall age distributions.\nThe distribution of non-users is slightly peaked towards younger companies; the distribution of users is slightly flatter and still has some density at higher ages.\n\nThis further illustrates that the company age may affect whether to become a user and may also be associated with the number of patents, so this variable should be controlled when making causal inferences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\nLikehood\nfrom IPython.display import display, Math\n\ndisplay(Math(r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\"))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\\)\n\n\n\n\nlog_Likehood\nfrom IPython.display import display, Math\n\ndisplay(Math(\n    r\"\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\"\n))\n\n\n\\(\\displaystyle \\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\\)\n\n\n\n\n\n\ndef poisson_loglikelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n\n\n\nUsing the previously defined log-likelihood function, we can visualize the change in log-likelihood for different values ​​of λ (lambda) and find the maximum likelihood estimate (MLE) through a graph.\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\nY = blueprinty[\"patents\"].values\nn = len(Y)\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return -n * lmbda + np.sum(Y * np.log(lmbda)) - np.sum(np.log(factorial(Y)))\nlambda_range = np.linspace(0.1, 10, 200)\n\nloglikelihood_values = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_range, loglikelihood_values, color='darkblue')\nplt.title(\"Log-Likelihood vs Lambda (Poisson Model)\")\nplt.xlabel(\"λ (lambda)\")  \nplt.ylabel(\"Log-Likelihood\")  \nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHorizontal axis (λ): a series of candidate λ values ​​we tried\nVertical axis (log-likelihood): log-likelihood value of each λ under actual data (number of patents) The curve has a typical “peak shape”, which means that there is a certain λ that maximizes the log-likelihood. The location of the peak is the maximum likelihood estimate (MLE).\n\n\n\n\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport sympy as sp\nfrom IPython.display import display, Math\n\nlmbda, n, sum_y = sp.symbols('lambda n sum_y', positive=True)\n\nlog_likelihood = -n * lmbda + sum_y * sp.log(lmbda)\n\n\nd_log_likelihood = sp.diff(log_likelihood, lmbda)\n\nsolution = sp.solve(d_log_likelihood, lmbda)[0]\n\ndisplay(Math(r\"\\textbf{Step 1: Define the log-likelihood function}\"))\ndisplay(Math(r\"\\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\"))\n\ndisplay(Math(r\"\\textbf{Step 2: Take the first derivative}\"))\ndisplay(Math(r\"\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\"))\n\ndisplay(Math(r\"\\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\"))\ndisplay(Math(r\"0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\"))\n\ndisplay(Math(r\"\\boxed{\\hat{\\lambda}_{\\text{MLE}} = \" + sp.latex(solution) + r\"}\"))\n\n\n\\(\\displaystyle \\textbf{Step 1: Define the log-likelihood function}\\)\n\n\n\\(\\displaystyle \\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\\)\n\n\n\\(\\displaystyle \\textbf{Step 2: Take the first derivative}\\)\n\n\n\\(\\displaystyle \\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\\)\n\n\n\\(\\displaystyle \\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\\)\n\n\n\\(\\displaystyle 0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\\)\n\n\n\\(\\displaystyle \\boxed{\\hat{\\lambda}_{\\text{MLE}} = \\frac{sum_{y}}{n}}\\)\n\n\n\n\n\nUse numerical optimization methods to find the maximum likelihood estimate (MLE) of λ in the Poisson model.\n\n\nOptimization of likehood by MLE\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda):\n    return -np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n# Use minimize to find the MLE\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\n\nmle_df = pd.DataFrame({\n    \"Parameter\": [\"lambda\"],\n    \"MLE Estimate\": [lambda_mle]\n})\n\nmle_df\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\n\n\n\n\n0\nlambda\n3.684666\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nThe original Poisson model likelihood function is expanded into a log-likelihood function of a Poisson regression model, where λ is no longer a fixed parameter but is determined by the explanatory variable X and the parameter vector \n\n\nLog-Likelihood Function for the Poisson Regression Model\nimport numpy as np\nfrom IPython.display import display, Math\n\nlikelihood = r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\"\nlambda_def = r\"\\lambda_i = \\exp(X_i^\\top \\beta)\"\nlog_likelihood = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\"\nlog_likelihood_expanded = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\"\n\ndisplay(Math(likelihood))\ndisplay(Math(lambda_def))\ndisplay(Math(log_likelihood))\ndisplay(Math(log_likelihood_expanded))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\n\n\n\\(\\displaystyle \\lambda_i = \\exp(X_i^\\top \\beta)\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\\)\n\n\n\nimport numpy as np\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear_predictor = X @ beta\n    \n    lambda_i = np.exp(linear_predictor)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(np.math.factorial(Y)))\n    \n    return log_likelihood\n\n\n\n\nPerform maximum likelihood estimation (MLE) on the Poisson regression model and use the inverse matrix of the Hessian matrix to calculate the standard errors of the parameters (Standard Errors)\n\n\nPoisson Regression Coefficients\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_mat = X.astype(float).values  \nY = blueprinty[\"patents\"].values\nn, k = X_mat.shape\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lin_pred = X @ beta\n    lambda_i = np.exp(lin_pred)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n\ndef neg_log_likelihood(beta):\n    return -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.zeros(k)\nresult = minimize(neg_log_likelihood, x0=beta_init, method='BFGS')\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv  \nse_beta = np.sqrt(np.diag(hessian_inv))\n\nmle_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se_beta\n})\nmle_table\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n1.554750\n0.063189\n\n\n1\nage\n-0.007970\n0.001890\n\n\n2\nage_squared_std\n-0.155814\n0.013481\n\n\n3\niscustomer\n0.207591\n0.031102\n\n\n4\nNortheast\n0.029170\n0.031939\n\n\n5\nNorthwest\n-0.017575\n0.052020\n\n\n6\nSouth\n0.056561\n0.051001\n\n\n7\nSouthwest\n0.050576\n0.043582\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model (GLM with Poisson family) to quantify and test whether the Blueprinty software usage status (iscustomer) significantly affects the number of patents obtained by the company, while also controlling for other variables that may affect patent performance, such as company age and region.\n\n\nPoisson Regression Coefficients GLM Function\nimport statsmodels.api as sm\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_sm = X_sm.astype(float)\n\nX_sm = sm.add_constant(X_sm)\n\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_df = result.summary2().tables[1].reset_index()\nsummary_df = summary_df.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"coef\",\n    \"Std.Err.\": \"std err\",\n    \"P&gt;|z|\": \"P&gt;|z|\",\n    \"[0.025\": \"[0.025\",\n    \"0.975]\": \"0.975]\"\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nVariable\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n1.554747\n0.066336\n23.437483\n1.773673e-121\n1.424731\n1.684763\n\n\n1\nage\n-0.007970\n0.002074\n-3.843138\n1.214711e-04\n-0.012035\n-0.003905\n\n\n2\nage_squared_std\n-0.155814\n0.013533\n-11.513237\n1.131496e-30\n-0.182339\n-0.129289\n\n\n3\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n0.147037\n0.268144\n\n\n4\nNortheast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n-0.056334\n0.114674\n\n\n5\nNorthwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n-0.122983\n0.087833\n\n\n6\nSouth\n0.056561\n0.052662\n1.074036\n2.828066e-01\n-0.046655\n0.159778\n\n\n7\nSouthwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n-0.041931\n0.143083\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model to analyze the relationship between Blueprinty software usage and the firm’s patent application success. The model controls for firm age (and its square) and regional differences, and uses the number of patents as an explanatory variable to perform maximum likelihood estimation (MLE).\nThe results show:\n\nCompanies using Blueprinty have significantly more patents on average than non-users.\nThe expected number of patents for Blueprinty users is about 1.23 times that of non-users, representing an average improvement of about 23%.\nThere is an inverted U-shaped relationship between company age and patent performance, i.e., as age increases, the number of patents first increases and then decreases.\nThe regional variable did not reach statistical significance, indicating that the region where the company is located has no significant impact on patent performance after controlling for other factors.\n\n\n\n\n\n\nCausal Effect of Blueprinty Usage on Patent Outcomes via Counterfactual Predictionn\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(int)\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_sm = sm.add_constant(X_sm)\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nX_counterfactual_0 = X_sm.copy()\nX_counterfactual_1 = X_sm.copy()\nX_counterfactual_0[\"iscustomer\"] = 0\nX_counterfactual_1[\"iscustomer\"] = 1\n\ny_pred_0 = result.predict(X_counterfactual_0)\ny_pred_1 = result.predict(X_counterfactual_1)\ndiff = y_pred_1 - y_pred_0\nmean = diff.mean()\naverage_effect = pd.DataFrame({\n    \"Effect Type\": [\"Average Treatment Effect (ATE)\"],\n    \"Estimated Effect (Δŷ)\": [mean]\n})\naverage_effect\n\n\n\n\n\n\n\n\n\nEffect Type\nEstimated Effect (Δŷ)\n\n\n\n\n0\nAverage Treatment Effect (ATE)\n0.792768\n\n\n\n\n\n\n\nIf all companies became Blueprinty customers, each company could expect to receive, on average, approximately 0.79 more patents. This means that using Blueprinty’s software has a substantial, positive effect on patent application success, based on the Poisson model you built, after controlling for firm age and region. This estimate is calculated using a counterfactual simulation method, which converts log(λ) into actual predicted differences and is highly interpretable."
  },
  {
    "objectID": "blog/Project2/index.html#blueprinty-case-study",
    "href": "blog/Project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nReading blueprinty’s data\nimport pandas as pd\nblueprinty = pd.read_csv('blueprinty.csv')\nblueprinty.head(5)\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\nThis section conducts preliminary observations through EDA, with the aim of comparing the differences in patent output between companies using Blueprinty software and non-users. As can be seen from the chart, the proportion of Blueprinty users in the high patent number range is relatively high, and the overall patent performance is also more outstanding. Comparison of the average number of patents further shows that Blueprinty customers perform significantly better than non-customers. These preliminary results suggest that Blueprinty software may have a positive impact on patent applications, providing reasonable motivation and direction for the subsequent establishment of more rigorous statistical models.\n\n\nAverage number of patents by customer status\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"patents\", hue='iscustomer', kde=False, bins=15, palette=\"Set1\", multiple=\"stack\")\nplt.title(\"Distribution of Patent Counts by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nsns.barplot(data=blueprinty, x=\"iscustomer\", y=\"patents\", hue=\"iscustomer\", palette=\"Set1\", estimator=np.mean, dodge=False, legend=False)\nplt.title(\"Average Number of Patents by Customer Status\")\nplt.xlabel(\"Blueprinty Customer\")\nplt.ylabel(\"Average Patent Count\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe average number of patents by Blueprinty users is significantly higher than that of non-users.\nSuggests that companies using Blueprinty software may be more successful in obtaining patents.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\nNext, we will continue to use ED to further confirm whether there are systematic differences in firm characteristics between Blueprinty customers and non-customers, which is crucial for the subsequent establishment of causal inference models (such as regression models).\n\n\nAverage number of patents by customer status\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=blueprinty, x=\"age\", hue=\"iscustomer\", kde=True, bins=20, palette=\"Set2\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Distribution of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age (Years)\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.subplot(1, 2, 2)\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"], normalize=\"index\") * 100\nregion_counts.plot(kind=\"bar\", stacked=True, ax=plt.gca(), colormap=\"Set2\")\nplt.title(\"Regional Composition by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Percentage (%)\")\nplt.legend(title=\"Blueprinty Customer\", labels=[\"No\", \"Yes\"])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBlueprinty users (green) and non-users (purple) have slightly different overall age distributions.\nThe distribution of non-users is slightly peaked towards younger companies; the distribution of users is slightly flatter and still has some density at higher ages.\n\nThis further illustrates that the company age may affect whether to become a user and may also be associated with the number of patents, so this variable should be controlled when making causal inferences.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\n\nLikehood\nfrom IPython.display import display, Math\n\ndisplay(Math(r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\"))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!} = e^{-n\\lambda} \\cdot \\lambda^{\\sum Y_i} \\cdot \\frac{1}{\\prod Y_i!}\\)\n\n\n\n\nlog_Likehood\nfrom IPython.display import display, Math\n\ndisplay(Math(\n    r\"\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\"\n))\n\n\n\\(\\displaystyle \\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\\)\n\n\n\n\n\n\ndef poisson_loglikelihood(lmbda, Y):\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n\n\n\nUsing the previously defined log-likelihood function, we can visualize the change in log-likelihood for different values ​​of λ (lambda) and find the maximum likelihood estimate (MLE) through a graph.\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\nY = blueprinty[\"patents\"].values\nn = len(Y)\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return -n * lmbda + np.sum(Y * np.log(lmbda)) - np.sum(np.log(factorial(Y)))\nlambda_range = np.linspace(0.1, 10, 200)\n\nloglikelihood_values = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_range, loglikelihood_values, color='darkblue')\nplt.title(\"Log-Likelihood vs Lambda (Poisson Model)\")\nplt.xlabel(\"λ (lambda)\")  \nplt.ylabel(\"Log-Likelihood\")  \nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHorizontal axis (λ): a series of candidate λ values ​​we tried\nVertical axis (log-likelihood): log-likelihood value of each λ under actual data (number of patents) The curve has a typical “peak shape”, which means that there is a certain λ that maximizes the log-likelihood. The location of the peak is the maximum likelihood estimate (MLE).\n\n\n\n\n\n\nlog-likehood v.s. Lanbda(Possion model)\nimport sympy as sp\nfrom IPython.display import display, Math\n\nlmbda, n, sum_y = sp.symbols('lambda n sum_y', positive=True)\n\nlog_likelihood = -n * lmbda + sum_y * sp.log(lmbda)\n\n\nd_log_likelihood = sp.diff(log_likelihood, lmbda)\n\nsolution = sp.solve(d_log_likelihood, lmbda)[0]\n\ndisplay(Math(r\"\\textbf{Step 1: Define the log-likelihood function}\"))\ndisplay(Math(r\"\\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\"))\n\ndisplay(Math(r\"\\textbf{Step 2: Take the first derivative}\"))\ndisplay(Math(r\"\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\"))\n\ndisplay(Math(r\"\\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\"))\ndisplay(Math(r\"0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\"))\n\ndisplay(Math(r\"\\boxed{\\hat{\\lambda}_{\\text{MLE}} = \" + sp.latex(solution) + r\"}\"))\n\n\n\\(\\displaystyle \\textbf{Step 1: Define the log-likelihood function}\\)\n\n\n\\(\\displaystyle \\log L(\\lambda) = -n\\lambda + \\left(\\sum Y_i\\right)\\log \\lambda\\)\n\n\n\\(\\displaystyle \\textbf{Step 2: Take the first derivative}\\)\n\n\n\\(\\displaystyle \\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{\\sum Y_i}{\\lambda}\\)\n\n\n\\(\\displaystyle \\textbf{Step 3: Set the derivative equal to zero and solve for } \\lambda\\)\n\n\n\\(\\displaystyle 0 = -n + \\frac{\\sum Y_i}{\\lambda} \\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{\\sum Y_i}{n} = \\bar{Y}\\)\n\n\n\\(\\displaystyle \\boxed{\\hat{\\lambda}_{\\text{MLE}} = \\frac{sum_{y}}{n}}\\)\n\n\n\n\n\nUse numerical optimization methods to find the maximum likelihood estimate (MLE) of λ in the Poisson model.\n\n\nOptimization of likehood by MLE\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(lmbda):\n    return -np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\n# Use minimize to find the MLE\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\n\nmle_df = pd.DataFrame({\n    \"Parameter\": [\"lambda\"],\n    \"MLE Estimate\": [lambda_mle]\n})\n\nmle_df\n\n\n\n\n\n\n\n\n\nParameter\nMLE Estimate\n\n\n\n\n0\nlambda\n3.684666\n\n\n\n\n\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nThe original Poisson model likelihood function is expanded into a log-likelihood function of a Poisson regression model, where λ is no longer a fixed parameter but is determined by the explanatory variable X and the parameter vector \n\n\nLog-Likelihood Function for the Poisson Regression Model\nimport numpy as np\nfrom IPython.display import display, Math\n\nlikelihood = r\"L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\"\nlambda_def = r\"\\lambda_i = \\exp(X_i^\\top \\beta)\"\nlog_likelihood = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\"\nlog_likelihood_expanded = r\"\\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\"\n\ndisplay(Math(likelihood))\ndisplay(Math(lambda_def))\ndisplay(Math(log_likelihood))\ndisplay(Math(log_likelihood_expanded))\n\n\n\\(\\displaystyle L(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\\)\n\n\n\\(\\displaystyle \\lambda_i = \\exp(X_i^\\top \\beta)\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!)\\right]\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\left[Y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(Y_i!)\\right]\\)\n\n\n\nimport numpy as np\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    linear_predictor = X @ beta\n    \n    lambda_i = np.exp(linear_predictor)\n    \n    log_likelihood = np.sum(Y * np.log(lambda_i) - lambda_i - np.log(np.math.factorial(Y)))\n    \n    return log_likelihood\n\n\n\n\nPerform maximum likelihood estimation (MLE) on the Poisson regression model and use the inverse matrix of the Hessian matrix to calculate the standard errors of the parameters (Standard Errors)\n\n\nPoisson Regression Coefficients\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_mat = X.astype(float).values  \nY = blueprinty[\"patents\"].values\nn, k = X_mat.shape\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    lin_pred = X @ beta\n    lambda_i = np.exp(lin_pred)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - np.log(factorial(Y)))\n\ndef neg_log_likelihood(beta):\n    return -poisson_regression_loglikelihood(beta, Y, X_mat)\n\nbeta_init = np.zeros(k)\nresult = minimize(neg_log_likelihood, x0=beta_init, method='BFGS')\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv  \nse_beta = np.sqrt(np.diag(hessian_inv))\n\nmle_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": se_beta\n})\nmle_table\n\n\n\n\n\n\n\n\n\nVariable\nEstimate\nStd. Error\n\n\n\n\n0\nintercept\n1.554750\n0.063189\n\n\n1\nage\n-0.007970\n0.001890\n\n\n2\nage_squared_std\n-0.155814\n0.013481\n\n\n3\niscustomer\n0.207591\n0.031102\n\n\n4\nNortheast\n0.029170\n0.031939\n\n\n5\nNorthwest\n-0.017575\n0.052020\n\n\n6\nSouth\n0.056561\n0.051001\n\n\n7\nSouthwest\n0.050576\n0.043582\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model (GLM with Poisson family) to quantify and test whether the Blueprinty software usage status (iscustomer) significantly affects the number of patents obtained by the company, while also controlling for other variables that may affect patent performance, such as company age and region.\n\n\nPoisson Regression Coefficients GLM Function\nimport statsmodels.api as sm\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_sm = X_sm.astype(float)\n\nX_sm = sm.add_constant(X_sm)\n\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_df = result.summary2().tables[1].reset_index()\nsummary_df = summary_df.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"coef\",\n    \"Std.Err.\": \"std err\",\n    \"P&gt;|z|\": \"P&gt;|z|\",\n    \"[0.025\": \"[0.025\",\n    \"0.975]\": \"0.975]\"\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nVariable\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\n0\nconst\n1.554747\n0.066336\n23.437483\n1.773673e-121\n1.424731\n1.684763\n\n\n1\nage\n-0.007970\n0.002074\n-3.843138\n1.214711e-04\n-0.012035\n-0.003905\n\n\n2\nage_squared_std\n-0.155814\n0.013533\n-11.513237\n1.131496e-30\n-0.182339\n-0.129289\n\n\n3\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n0.147037\n0.268144\n\n\n4\nNortheast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n-0.056334\n0.114674\n\n\n5\nNorthwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n-0.122983\n0.087833\n\n\n6\nSouth\n0.056561\n0.052662\n1.074036\n2.828066e-01\n-0.046655\n0.159778\n\n\n7\nSouthwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n-0.041931\n0.143083\n\n\n\n\n\n\n\n\n\n\nWe use a Poisson regression model to analyze the relationship between Blueprinty software usage and the firm’s patent application success. The model controls for firm age (and its square) and regional differences, and uses the number of patents as an explanatory variable to perform maximum likelihood estimation (MLE).\nThe results show:\n\nCompanies using Blueprinty have significantly more patents on average than non-users.\nThe expected number of patents for Blueprinty users is about 1.23 times that of non-users, representing an average improvement of about 23%.\nThere is an inverted U-shaped relationship between company age and patent performance, i.e., as age increases, the number of patents first increases and then decreases.\nThe regional variable did not reach statistical significance, indicating that the region where the company is located has no significant impact on patent performance after controlling for other factors.\n\n\n\n\n\n\nCausal Effect of Blueprinty Usage on Patent Outcomes via Counterfactual Predictionn\nblueprinty[\"iscustomer\"] = blueprinty[\"iscustomer\"].astype(int)\n\nblueprinty[\"age_std\"] = (blueprinty[\"age\"] - blueprinty[\"age\"].mean()) / blueprinty[\"age\"].std()\nblueprinty[\"age_squared_std\"] = blueprinty[\"age_std\"] ** 2\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX_sm = pd.concat([\n    blueprinty[[\"age\", \"age_squared_std\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_sm = sm.add_constant(X_sm)\nY = blueprinty[\"patents\"]\n\nmodel = sm.GLM(Y, X_sm, family=sm.families.Poisson())\nresult = model.fit()\n\nX_counterfactual_0 = X_sm.copy()\nX_counterfactual_1 = X_sm.copy()\nX_counterfactual_0[\"iscustomer\"] = 0\nX_counterfactual_1[\"iscustomer\"] = 1\n\ny_pred_0 = result.predict(X_counterfactual_0)\ny_pred_1 = result.predict(X_counterfactual_1)\ndiff = y_pred_1 - y_pred_0\nmean = diff.mean()\naverage_effect = pd.DataFrame({\n    \"Effect Type\": [\"Average Treatment Effect (ATE)\"],\n    \"Estimated Effect (Δŷ)\": [mean]\n})\naverage_effect\n\n\n\n\n\n\n\n\n\nEffect Type\nEstimated Effect (Δŷ)\n\n\n\n\n0\nAverage Treatment Effect (ATE)\n0.792768\n\n\n\n\n\n\n\nIf all companies became Blueprinty customers, each company could expect to receive, on average, approximately 0.79 more patents. This means that using Blueprinty’s software has a substantial, positive effect on patent application success, based on the Poisson model you built, after controlling for firm age and region. This estimate is calculated using a counterfactual simulation method, which converts log(λ) into actual predicted differences and is highly interpretable."
  },
  {
    "objectID": "blog/Project2/index.html#airbnb-case-study",
    "href": "blog/Project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nReading airbnb’s dataset\nimport pandas as pd\nairbnb = pd.read_csv('airbnb.csv')\nairbnb.head(5)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n\nEDA\n\nMissing Value Check and Handling\n\n\nMissing Value\n# Create a DataFrame from the missing_values Series\nmissing_values = airbnb.isnull().sum()\nmissing_values_df = missing_values.reset_index()\nmissing_values_df.columns = ['Column', 'Missing Values']\nrelevant_columns = ['number_of_reviews', 'price', 'room_type', 'bedrooms', 'bathrooms', 'instant_bookable']\nairbnb_cleaned = airbnb[relevant_columns].dropna()\nmissing_values_df\n\n\n\n\n\n\n\n\n\nColumn\nMissing Values\n\n\n\n\n0\nUnnamed: 0\n0\n\n\n1\nid\n0\n\n\n2\ndays\n0\n\n\n3\nlast_scraped\n0\n\n\n4\nhost_since\n35\n\n\n5\nroom_type\n0\n\n\n6\nbathrooms\n160\n\n\n7\nbedrooms\n76\n\n\n8\nprice\n0\n\n\n9\nnumber_of_reviews\n0\n\n\n10\nreview_scores_cleanliness\n10195\n\n\n11\nreview_scores_location\n10254\n\n\n12\nreview_scores_value\n10256\n\n\n13\ninstant_bookable\n0\n\n\n\n\n\n\n\nWe have removed all observation columns with missing values ​​to ensure the accuracy of the model.\n\n\nVariable Distribution\n\nReview Volume and Price Distribution\n\n\n\nDistribution plot\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nsummary_stats = airbnb.describe()\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(airbnb[\"number_of_reviews\"], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\n\nplt.subplot(1, 2, 2)\nsns.histplot(airbnb[\"price\"], bins=50, kde=False)\nplt.title(\"Distribution of Price\")\nplt.xlabel(\"Price\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNumber of reviews (number_of_reviews): skewed distribution, most listings have low numbers of reviews, but some have abnormally high values ​​(common for popular listings)\nPrice: right-skewed distribution, some properties are very expensive, need to consider conversion (such as log) in subsequent modeling\n\n\nLog Transformations Distribution\n\n\n\nDistribution plot of price log\nairbnb[\"log_price\"] = np.log1p(airbnb[\"price\"])\nairbnb[\"log_reviews\"] = np.log1p(airbnb[\"number_of_reviews\"])\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.histplot(airbnb[\"log_reviews\"], bins=50)\nplt.title(\"Log(Number of Reviews + 1) Distribution\")\n\nplt.subplot(1, 2, 2)\nsns.histplot(airbnb[\"log_price\"], bins=50)\nplt.title(\"Log(Price + 1) Distribution\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAfter log(price + 1) and log(number_of_reviews + 1) transformation, the distribution obviously tends to be normal, which is conducive to subsequent modeling.\n\n\nCategorical Variable Distributions\n\n\nDistribution plot of price log\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(data=airbnb, x=\"room_type\")\nplt.title(\"Room Type Distribution\")\n\nplt.subplot(1, 2, 2)\nsns.countplot(data=airbnb, x=\"instant_bookable\")\nplt.title(\"Instant Bookable Distribution\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nroom_type: Private room and Entire home/apt are the two most common types, and Shared room is very rare\ninstant_bookable: Most listings do not support instant booking, which means this variable may have explanatory power in the model\n\n\n\nCorrelation Exploration\nUse a scatter plot to check whether number_of_reviews is correlated with review-based variables. This helps validate whether reviews are a reasonable proxy for bookings.\n\n\nnumber_of_reviews correlation with review-based variables\nplt.figure(figsize=(6, 5))\nsns.scatterplot(data=airbnb, x=\"number_of_reviews\", y=\"review_scores_value\", alpha=0.3)\nplt.title(\"Number of Reviews vs. Review Score (Value)\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Review Score - Value\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFrom the scatter plot of number_of_reviews vs. review_scores_value, there is a positive correlation trend to a certain extent.\nShows that the number of reviews is somewhat correlated with user satisfaction, supporting the use of the number of reviews as a proxy variable for the number of orders.\n\n\n\n\nPoisson Regression Model\n\n\nVisualize the relationship between price and number of reviews\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\ncols = [\"price\", \"number_of_reviews\", \"instant_bookable\", \"room_type\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"bathrooms\", \"bedrooms\", \"days\"]\ndf = airbnb.dropna(subset=cols).copy()\n\ndf[\"log_price\"] = np.log1p(df[\"price\"])  \ndf[\"instant_bookable\"] = df[\"instant_bookable\"].map({\"t\": 1, \"f\": 0}).astype(int)  \n\ndf = pd.get_dummies(df, columns=[\"room_type\"], drop_first=True)\n\nfeature_cols = [\"log_price\", \"days\", \"bathrooms\", \"bedrooms\",\n                \"review_scores_cleanliness\", \"review_scores_location\",\n                \"review_scores_value\", \"instant_bookable\"] + \\\n               [col for col in df.columns if col.startswith(\"room_type_\")]\nX = df[feature_cols].astype(float)\nX = sm.add_constant(X)\n\nY = df[\"number_of_reviews\"]\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_df = result.summary2().tables[1].copy()\nsummary_df[\"exp(coef)\"] = np.exp(summary_df[\"Coef.\"])\nsummary_df[\"Significance\"] = summary_df[\"P&gt;|z|\"].apply(\n    lambda p: \"***\" if p &lt; 0.001 else \"**\" if p &lt; 0.01 else \"*\" if p &lt; 0.05 else \"\")\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"coef\", \"Std.Err.\": \"std err\", \"P&gt;|z|\": \"P&gt;|z|\",\n    \"[0.025\": \"[0.025\", \"0.975]\": \"0.975]\"\n}).reset_index()\n\nfinal_output_df = summary_df[[\n    \"index\", \"coef\", \"std err\", \"exp(coef)\", \"P&gt;|z|\", \"Significance\", \"[0.025\", \"0.975]\"\n]].rename(columns={\"index\": \"Variable\"})\n\nfinal_output_df.round(3).head(10)\n\n\n\n\n\n\n\n\n\nVariable\ncoef\nstd err\nexp(coef)\nP&gt;|z|\nSignificance\n[0.025\n0.975]\n\n\n\n\n0\nconst\n3.013\n0.019\n20.354\n0.0\n***\n2.975\n3.051\n\n\n1\nlog_price\n0.131\n0.003\n1.140\n0.0\n***\n0.125\n0.137\n\n\n2\ndays\n0.000\n0.000\n1.000\n0.0\n***\n0.000\n0.000\n\n\n3\nbathrooms\n-0.145\n0.004\n0.865\n0.0\n***\n-0.153\n-0.138\n\n\n4\nbedrooms\n0.047\n0.002\n1.048\n0.0\n***\n0.043\n0.051\n\n\n5\nreview_scores_cleanliness\n0.109\n0.001\n1.115\n0.0\n***\n0.106\n0.112\n\n\n6\nreview_scores_location\n-0.097\n0.002\n0.907\n0.0\n***\n-0.101\n-0.094\n\n\n7\nreview_scores_value\n-0.080\n0.002\n0.924\n0.0\n***\n-0.083\n-0.076\n\n\n8\ninstant_bookable\n0.352\n0.003\n1.422\n0.0\n***\n0.346\n0.358\n\n\n9\nroom_type_Private room\n0.087\n0.003\n1.090\n0.0\n***\n0.080\n0.093\n\n\n\n\n\n\n\n\nModele Coefficient Interpretation\n\n\nVisualize the relationship between price and number of reviews\nsns.scatterplot(data=airbnb_cleaned, x='price', y='number_of_reviews', alpha=0.5)\nplt.title('Number of Reviews vs Price')\nplt.xlabel('Price')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nModel Interpretation\nIn this analysis, we used a Poisson regression model to predict the number of Airbnb reviews, treating review count as a proxy for bookings. The model included predictors such as price, instant bookability, review scores, and property features. Because Poisson regression coefficients are in log form, we exponentiate them (exp(β)) to interpret their impact as multiplicative effects on the expected number of reviews.\nKey findings from the model are as follows:\n\nPrice (log-transformed) The coefficient for log(price) is 0.135, with exp(β) = 1.14. This indicates that a one-unit increase in log price is associated with a 14% increase in expected review count, suggesting that higher-priced listings may gain more visibility or attract more interest.\nInstant Bookable The coefficient is 0.341, with exp(β) = 1.41. Listings that support instant booking are expected to receive 41% more reviews than those that do not, highlighting the importance of booking convenience in driving customer engagement.\nReview Scores: Cleanliness With a coefficient of 0.109 and exp(β) = 1.11, each one-point increase in cleanliness score corresponds to an 11% increase in expected review count, underscoring how important cleanliness is in guest satisfaction and feedback.\nReview Scores: Location The coefficient is -0.098, with exp(β) = 0.91, meaning that a higher location score is unexpectedly associated with fewer reviews. This may suggest that guests are less inclined to leave feedback when location expectations are already met, or it may reflect an unobserved confounder.\n\nOverall, the model provides interpretable and statistically significant insights into the factors influencing review counts. These results support the idea that certain listing features—especially instant bookability and cleanliness—play a meaningful role in increasing engagement, and they can inform host strategy and platform design going forward.\n\n\nConclusion\nThis analysis demonstrates how listing characteristics on Airbnb relate to the number of reviews, which we use as a proxy for booking activity. Through a series of exploratory data analysis steps and a Poisson regression model, we identified key factors that significantly influence review count.\nAmong these, instant bookability, listing price, and cleanliness score emerge as the strongest predictors. Listings that allow instant booking are associated with a 41% increase in expected review count, highlighting the role of convenience in driving user engagement. Similarly, listings with higher prices and better cleanliness scores tend to receive more reviews, suggesting that both perceived quality and visibility may contribute to user response.\nInterestingly, location score shows a negative relationship with review count. While counterintuitive, this may reflect a behavioral trend where guests are less likely to leave feedback when their expectations are met—or it could indicate unobserved variables influencing this dynamic.\nTaken together, the results confirm that certain listing features meaningfully shape guest behavior and can influence a listing’s success on the platform. These findings can guide hosts in optimizing their property features and booking settings, and may also inform Airbnb’s platform design and recommendation algorithms to enhance both host and guest experience."
  },
  {
    "objectID": "blog/Project3/hw2_questions.html",
    "href": "blog/Project3/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/Project3/hw2_questions.html#blueprinty-case-study",
    "href": "blog/Project3/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/Project3/hw2_questions.html#airbnb-case-study",
    "href": "blog/Project3/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "blog/Project3/index.html",
    "href": "blog/Project3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/Project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "Suppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Project3/index.html#simulate-conjoint-data",
    "href": "blog/Project3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\nSimulate Conjoint Data\n# set seed for reproducibility\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)\n\n# Generate all possible profiles\nprofiles = pd.DataFrame([\n    {\"brand\": b, \"ad\": a, \"price\": p}\n    for b in brands for a in ads for p in prices\n])\nm = len(profiles)\n\n# Assign part-worth utilities (true parameters)\nbrand_utils = {\"N\": 1.0, \"P\": 0.5, \"H\": 0}\nad_utils = {\"Yes\": -0.8, \"No\": 0.0}\nprice_util = lambda p: -0.1 * p\n\n# Simulation settings\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent’s data\ndef sim_one(id):\n    dat_list = []\n    for t in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled[\"resp\"] = id\n        sampled[\"task\"] = t\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(brand_utils) +\n            sampled[\"ad\"].map(ad_utils) +\n            sampled[\"price\"].apply(price_util)\n        ).round(10)\n        sampled[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))  # Gumbel noise\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n        dat_list.append(sampled)\n    return pd.concat(dat_list)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\n# Keep only observable columns\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\nconjoint_data.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1"
  },
  {
    "objectID": "blog/Project3/index.html#preparing-the-data-for-estimation",
    "href": "blog/Project3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nReading conjoint_data\nimport pandas as pd\ndata=pd.read_csv(\"conjoint_data.csv\")\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nresp\nrespondent number\n\n\ntask\nthe number of selected tasks\n\n\nchoice\nwhether it is selected (1=selected, 0=not selected)\n\n\nbrand\nHulu, Netflix, Prime\n\n\nprice\nprice of the set\n\n\n\n\n\n\nOrganize this information into a format that can be used in the MNL model, including:\n\nConvert brand and ad into dummy variables\nEnsure that each observation corresponds to a respondent-task-alternative combination\nPrepare the design matrix X and selection results y for modeling\n\n\n\nReshape the data\nX = pd.get_dummies(data, columns=['brand', 'ad'], drop_first=True)\nX = X.sort_values(['resp', 'task']).copy()\nX['alternative'] = X.groupby(['resp', 'task']).cumcount()\nX = X.set_index(['resp', 'task', 'alternative'])\nX.head()\n\n\n\n\n\n\n\n\n\n\n\nchoice\nprice\nbrand_N\nbrand_P\nad_Yes\n\n\nresp\ntask\nalternative\n\n\n\n\n\n\n\n\n\n1\n1\n0\n1\n28\nTrue\nFalse\nTrue\n\n\n1\n0\n16\nFalse\nFalse\nTrue\n\n\n2\n0\n16\nFalse\nTrue\nTrue\n\n\n2\n0\n0\n32\nTrue\nFalse\nTrue\n\n\n1\n1\n16\nFalse\nTrue\nTrue"
  },
  {
    "objectID": "blog/Project3/index.html#estimation-via-maximum-likelihood",
    "href": "blog/Project3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the parameters of the Multinomial Logit (MNL) model, we use the Maximum Likelihood Estimation (MLE) method. The likelihood is constructed based on the probability that each individual chooses the alternative with the highest utility. Given the assumption that the unobserved error term follows a type I extreme value distribution, the MNL model leads to a closed-form expression for choice probabilities.\nThe log-likelihood function sums the log probabilities of the chosen alternatives across all individuals and choice sets. The code below implements this log-likelihood in Python, reshaping the data into respondent-task-choice format, computing the choice probabilities, and returning the negative log-likelihood to be used in optimization.\n\n\nlog-likelihood function\nfrom IPython.display import display, Math\n\nutility_formula = r\"U_{ij} = x_{ij}^\\top \\beta + \\varepsilon_{ij}\"\nprobability_formula = r\"P_{ij} = \\frac{e^{x_{ij}^\\top \\beta}}{\\sum_{k=1}^J e^{x_{ik}^\\top \\beta}}\"\nloglikelihood_formula = r\"\\log L(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(P_{ij})\"\ndelta_explanation = r\"\\delta_{ij} = \\begin{cases} 1 & \\text{if individual } i \\text{ chooses option } j \\\\ 0 & \\text{otherwise} \\end{cases}\"\n\ndisplay(Math(utility_formula))\ndisplay(Math(probability_formula))\ndisplay(Math(loglikelihood_formula))\ndisplay(Math(delta_explanation))\n\n\n\\(\\displaystyle U_{ij} = x_{ij}^\\top \\beta + \\varepsilon_{ij}\\)\n\n\n\\(\\displaystyle P_{ij} = \\frac{e^{x_{ij}^\\top \\beta}}{\\sum_{k=1}^J e^{x_{ik}^\\top \\beta}}\\)\n\n\n\\(\\displaystyle \\log L(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(P_{ij})\\)\n\n\n\\(\\displaystyle \\delta_{ij} = \\begin{cases} 1 & \\text{if individual } i \\text{ chooses option } j \\\\ 0 & \\text{otherwise} \\end{cases}\\)\n\n\n\nX_np = X[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].to_numpy()\ny_np = X[\"choice\"].to_numpy()\ndef mnl_log_likelihood(beta, X, y, n_choices=3):\n    \"\"\"\n    beta: parameter vector (K,)\n    X: (n_obs, K) design matrix\n    y: (n_obs,) choice vector (0/1)\n    n_choices: number of alternatives per task\n    \"\"\"\n    n_obs, n_features = X.shape\n    X_reshaped = X.reshape(-1, n_choices, n_features)\n    y_reshaped = y.reshape(-1, n_choices)\n\n    utilities = X_reshaped @ beta\n    exp_utilities = np.exp(utilities)\n    probs = exp_utilities / exp_utilities.sum(axis=1, keepdims=True)\n\n    chosen_probs = np.sum(probs * y_reshaped, axis=1)\n    log_likelihood = np.sum(np.log(chosen_probs + 1e-15)) \n\n    return -log_likelihood \n\nParameter estimation of the multinomial logit (MNL) model:\n\nMaximum Likelihood Estimation (MLE): Find a set of parameters that maximizes the overall log-likelihood\nUsing the Hessian matrix: Second-order derivative matrix → Use its inverse matrix to estimate the variance of the parameters\nCalculate confidence intervals: \\[\\text{CI}_{95\\%} = \\hat{\\beta} \\pm 1.96 \\times \\text{SE}(\\hat{\\beta})\\]\n\n\n\nEstimation of the multinomial logit (MNL) model\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_log_likelihood(beta, X, y):\n    return mnl_log_likelihood(beta, X, y)\n\nbeta0 = np.zeros(4)\n\nX_np = X[[\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"]].astype(float).values\ny_np = X[\"choice\"].values\n\nresult = minimize(neg_log_likelihood, beta0, args=(X_np, y_np), method='BFGS')\n\nmle = result.x\n\nhessian_inv = result.hess_inv\nse = np.sqrt(np.diag(hessian_inv))\n\nz = 1.96\nci_lower = mle - z * se\nci_upper = mle + z * se\n\nsummary = pd.DataFrame({\n    \"Parameter\": [\"Netflix\", \"Prime\", \"Ads\", \"Price\"],\n    \"Estimate\": mle,\n    \"Std. Error\": se,\n    \"95% CI Lower\": ci_lower,\n    \"95% CI Upper\": ci_upper\n})\nsummary\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nNetflix\n0.941195\n0.113800\n0.718148\n1.164242\n\n\n1\nPrime\n0.501616\n0.120910\n0.264631\n0.738600\n\n\n2\nAds\n-0.731994\n0.088546\n-0.905545\n-0.558443\n\n\n3\nPrice\n-0.099480\n0.006358\n-0.111942\n-0.087019\n\n\n\n\n\n\n\nThe maximum likelihood estimates for the Multinomial Logit model parameters provide clear and interpretable insights into consumer preferences in the simulated conjoint experiment.\n\nThe positive coefficients for Netflix (0.94) and Prime (0.50) indicate that, relative to the reference brand Hulu, consumers prefer both Netflix and Prime, with Netflix being the most preferred.\nThe coefficient for Ads (-0.73) is strongly negative, confirming that advertisements significantly reduce the attractiveness of a streaming service.\nThe Price coefficient (-0.099) is also negative and highly significant, suggesting that higher prices lead to lower utility, as expected.\n\nAll four parameters are statistically significant, with their 95% confidence intervals excluding zero. These results are consistent with economic theory and the part-worth utilities used in the data generation process, reinforcing the reliability of the MNL model under maximum likelihood estimation."
  },
  {
    "objectID": "blog/Project3/index.html#estimation-via-bayesian-methods",
    "href": "blog/Project3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nTo perform Bayesian inference for the parameters of the Multinomial Logit (MNL) model, we implement a Metropolis-Hastings Markov Chain Monte Carlo (MCMC) algorithm. The goal is to sample from the posterior distribution of the parameter vector , combining both the likelihood from the MNL model and the prior beliefs on \\(\\beta\\).\nThe algorithm proposes a new candidate \\(\\beta^*\\) from a multivariate normal distribution and decides whether to accept it based on the relative posterior probabilities. We run the sampler for 11,000 iterations and discard the first 1,000 as burn-in.\n\n\nMetropolis-hasting MCMC sampler of the posterior distribution\nfrom IPython.display import display, Math\n\n# Define LaTeX formulas for the MCMC process\nlog_posterior_eq = r\"\\log \\text{posterior}(\\beta) = \\log L(\\beta) + \\log \\text{prior}(\\beta)\"\nlog_likelihood_eq = r\"\\log L(\\beta): \\text{ Use the log-likelihood from the MNL model}\"\nlog_prior_eq = r\"\\log \\text{prior}(\\beta): \\text{ Typically assume } \\beta \\sim N(0, \\sigma^2)\"\n\nmh_step_1 = r\"\\text{1. Initialize } \\beta^{(0)}\"\nmh_step_2 = r\"\\text{2. Propose } \\beta^* \\sim q(\\cdot | \\beta^{(t)})\"\nmh_step_3 = r\"A = \\min\\left(1, \\frac{\\text{posterior}(\\beta^*)}{\\text{posterior}(\\beta^{(t)})} \\right)\"\nmh_step_4 = r\"\\text{4. Accept or reject based on } A\"\nmh_step_5 = r\"\\text{5. Repeat for 11,000 steps; discard the first 1,000 as burn-in}\"\n\n# Display the math expressions\ndisplay(Math(log_posterior_eq))\ndisplay(Math(log_likelihood_eq))\ndisplay(Math(log_prior_eq))\ndisplay(Math(mh_step_1))\ndisplay(Math(mh_step_2))\ndisplay(Math(mh_step_3))\ndisplay(Math(mh_step_4))\ndisplay(Math(mh_step_5))\n\n\n\\(\\displaystyle \\log \\text{posterior}(\\beta) = \\log L(\\beta) + \\log \\text{prior}(\\beta)\\)\n\n\n\\(\\displaystyle \\log L(\\beta): \\text{ Use the log-likelihood from the MNL model}\\)\n\n\n\\(\\displaystyle \\log \\text{prior}(\\beta): \\text{ Typically assume } \\beta \\sim N(0, \\sigma^2)\\)\n\n\n\\(\\displaystyle \\text{1. Initialize } \\beta^{(0)}\\)\n\n\n\\(\\displaystyle \\text{2. Propose } \\beta^* \\sim q(\\cdot | \\beta^{(t)})\\)\n\n\n\\(\\displaystyle A = \\min\\left(1, \\frac{\\text{posterior}(\\beta^*)}{\\text{posterior}(\\beta^{(t)})} \\right)\\)\n\n\n\\(\\displaystyle \\text{4. Accept or reject based on } A\\)\n\n\n\\(\\displaystyle \\text{5. Repeat for 11,000 steps; discard the first 1,000 as burn-in}\\)\n\n\n\ndef mnl_log_posterior(beta, X, y):\n\n    return -mnl_log_likelihood(beta, X, y)\n\ndef metropolis_hastings(log_posterior, initial, steps, proposal_cov, X, y):\n    n_params = len(initial)\n    samples = np.zeros((steps, n_params))\n    current = initial.copy()\n    current_log_post = log_posterior(current, X, y)\n    accept_count = 0\n\n    for i in range(steps):\n        proposal = np.random.multivariate_normal(current, proposal_cov)\n        proposal_log_post = log_posterior(proposal, X, y)\n        log_accept_ratio = proposal_log_post - current_log_post\n\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            current = proposal\n            current_log_post = proposal_log_post\n            accept_count += 1\n\n        samples[i] = current\n\n    acceptance_rate = accept_count / steps\n    return samples, acceptance_rate\n\nproposal_cov = hessian_inv * 2.0\n\nmcmc_steps = 11000\ninitial_beta = mle\n\nsamples, acc_rate = metropolis_hastings(\n    mnl_log_posterior, initial_beta, mcmc_steps, proposal_cov, X_np, y_np\n)\n\nmcmc_samples = samples[1000:]\n\nprint(f\"Acceptance rate: {acc_rate:.3f}\")\n\nAcceptance rate: 0.217\n\n\nThe reported acceptance rate provides a diagnostic of the sampler’s efficiency. In this case, an acceptance rate of 21.7% suggests reasonable exploration of the posterior space.\nThe trace plot of the algorithm and the histogram of the posterior distribution.\n\n\nMetropolis-hasting MCMC sampler of the posterior distribution\nimport matplotlib.pyplot as plt\n\nparam_names = [\"Netflix\", \"Prime\", \"Ads\", \"Price\"]\n\nfig, axes = plt.subplots(4, 2, figsize=(14, 12))\n\nfor i, name in enumerate(param_names):\n    # Trace plot\n    axes[i, 0].plot(mcmc_samples[:, i], color='tab:blue', alpha=0.7)\n    axes[i, 0].set_title(f\"Trace Plot: {name}\")\n    axes[i, 0].set_xlabel(\"Iteration\")\n    axes[i, 0].set_ylabel(\"Parameter Value\")\n    \n    # Posterior histogram\n    axes[i, 1].hist(mcmc_samples[:, i], bins=40, color='tab:orange', alpha=0.7, density=True)\n    axes[i, 1].set_title(f\"Posterior Histogram: {name}\")\n    axes[i, 1].set_xlabel(\"Parameter Value\")\n    axes[i, 1].set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTo evaluate the performance of the Metropolis-Hastings MCMC sampler and compare it to classical estimation methods, we report the posterior summaries of the four parameters in the Multinomial Logit (MNL) model. These include the posterior mean, standard deviation, and 95% credible intervals derived from the MCMC samples after burn-in. We then compare these to the point estimates, standard errors, and confidence intervals obtained via Maximum Likelihood Estimation (MLE).\nThis side-by-side comparison allows us to assess the consistency of the two approaches and evaluate the uncertainty associated with each method.\n\n\nMetropolis-hasting MCMC sampler of the posterior distribution\nposterior_means = mcmc_samples.mean(axis=0)\nposterior_stds = mcmc_samples.std(axis=0)\nposterior_ci_lower = np.percentile(mcmc_samples, 2.5, axis=0)\nposterior_ci_upper = np.percentile(mcmc_samples, 97.5, axis=0)\n\nbayes_summary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Posterior Mean\": posterior_means,\n    \"Posterior Std\": posterior_stds,\n    \"95% CrI Lower\": posterior_ci_lower,\n    \"95% CrI Upper\": posterior_ci_upper,\n    \"MLE\": summary[\"Estimate\"].values,\n    \"MLE Std. Error\": summary[\"Std. Error\"].values,\n    \"MLE 95% CI Lower\": summary[\"95% CI Lower\"].values,\n    \"MLE 95% CI Upper\": summary[\"95% CI Upper\"].values\n})\n\nbayes_summary\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior Std\n95% CrI Lower\n95% CrI Upper\nMLE\nMLE Std. Error\nMLE 95% CI Lower\nMLE 95% CI Upper\n\n\n\n\n0\nNetflix\n0.942963\n0.110484\n0.731285\n1.154729\n0.941195\n0.113800\n0.718148\n1.164242\n\n\n1\nPrime\n0.505350\n0.107909\n0.296512\n0.709532\n0.501616\n0.120910\n0.264631\n0.738600\n\n\n2\nAds\n-0.732632\n0.087994\n-0.895949\n-0.555279\n-0.731994\n0.088546\n-0.905545\n-0.558443\n\n\n3\nPrice\n-0.099771\n0.006495\n-0.113190\n-0.087244\n-0.099480\n0.006358\n-0.111942\n-0.087019\n\n\n\n\n\n\n\nThe posterior estimates obtained from the MCMC sampler closely match those derived from the MLE approach across all four parameters (Netflix, Prime, Ads, and Price). Specifically:\n\nThe posterior means fall well within the 95% confidence intervals of the MLE estimates, indicating that the Bayesian and frequentist approaches lead to similar conclusions.\nThe standard deviations and credible intervals from the posterior distributions are slightly wider than those from the MLE, which is expected due to the incorporation of prior uncertainty.\nAll parameters show consistent signs and magnitudes, supporting the validity and robustness of the original utility assumptions used in the simulation.\n\nThese results provide strong evidence that the Metropolis-Hastings sampler is functioning correctly and that Bayesian inference provides a coherent and reliable alternative to classical estimation in discrete choice modeling."
  },
  {
    "objectID": "blog/Project3/index.html#discussion",
    "href": "blog/Project3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nSuppose we had not simulated the data and were instead analyzing real-world consumer choices. In this case, the estimated parameters of the Multinomial Logit (MNL) model should be interpreted as reflecting actual consumer preferences.\n\nInsights from the Estimated Coefficients\n\nBrand Preferences: The result that {} &gt; {} suggests that, holding all else equal, consumers derive greater utility from Netflix compared to Amazon Prime. This implies a stronger brand preference for Netflix, potentially due to factors such as better perceived content quality, reputation, or user experience.\nAd Aversion: The negative coefficient on advertisements indicates that the inclusion of ads significantly reduces consumer utility. Consumers are more likely to choose ad-free options, confirming common behavioral expectations in subscription-based streaming markets.\nPrice Sensitivity: The fact that _{} is negative is entirely consistent with economic theory. Higher prices are expected to reduce the probability of selection, as they represent a cost to the consumer. The magnitude of the price coefficient reflects the estimated strength of that sensitivity.\n\nOverall, these estimates are logically coherent and align with what we would expect from consumer choice behavior in a real market setting. The model successfully captures key trade-offs that consumers make, including brand loyalty, aversion to advertisements, and price sensitivity. This supports the usefulness of MNL models—and conjoint analysis more broadly—as tools for analyzing and predicting consumer preferences.\n\n\nHierarchical Multinomial Logit Model (H-MNL)\nIn real-world conjoint analysis, it is often unrealistic to assume that all consumers share the same preferences. To account for individual-level heterogeneity, we can extend the standard Multinomial Logit (MNL) model to a hierarchical (multi-level) version, where each respondent has their own set of utility parameters.\n\nModel Description In the hierarchical model, instead of assuming a single shared parameter vector \\(\\beta\\), we assume each individual \\(i\\) has their own \\(\\beta_i\\), drawn from a population distribution:\n\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nThis modifies the utility specification as:\n\\[\nU_{ij} = x_{ij}^\\top \\beta_i + \\varepsilon_{ij}\n\\]\nWhere:\n\n\\(x_{ij}\\): attribute vector for alternative \\(j\\) in task \\(i\\)\n\\(\\beta_i\\): individual-level preference vector\n\\(\\varepsilon_{ij}\\): i.i.d. extreme value error term\n\n\nData Simulation:\n\n\n# Simulation settings\nnp.random.seed(123)\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Attribute levels\nbrands = [\"N\", \"P\", \"H\"]\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)\n\n# True population mean and variance for betas\nmu = np.array([1.0, 0.5, -0.8, -0.1])\nSigma = np.diag([0.2, 0.2, 0.2, 0.01])\n\n# Build full profile set\nprofiles = pd.DataFrame([\n    {\"brand\": b, \"ad\": a, \"price\": p}\n    for b in brands for a in ads for p in prices\n])\n\nbrand_utils = {\"N\": [1, 0], \"P\": [0, 1], \"H\": [0, 0]}\nad_utils = {\"Yes\": 1, \"No\": 0}\n\n# Simulate one respondent's data\ndef simulate_respondent(id):\n    beta_i = np.random.multivariate_normal(mu, Sigma)\n    rows = []\n    for task in range(1, n_tasks + 1):\n        sample = profiles.sample(n=n_alts).reset_index(drop=True)\n        X = []\n        for _, row in sample.iterrows():\n            x_vec = brand_utils[row.brand] + [ad_utils[row.ad], row.price]\n            X.append(x_vec)\n        X = np.array(X)\n        v = X @ beta_i\n        e = -np.log(-np.log(np.random.rand(n_alts)))\n        u = v + e\n        choice = np.argmax(u)\n        for j in range(n_alts):\n            rows.append({\"resp\": id, \"task\": task, \"alt\": j, \"brand\": sample.loc[j, \"brand\"],\n                         \"ad\": sample.loc[j, \"ad\"], \"price\": sample.loc[j, \"price\"],\n                         \"choice\": int(j == choice), \"x1\": X[j, 0], \"x2\": X[j, 1], \"x3\": X[j, 2], \"x4\": X[j, 3]})\n    return pd.DataFrame(rows)\nresults = pd.concat([simulate_respondent(i) for i in range(1, n_peeps+1)], ignore_index=True)\nresults.head()\n\n\n\n\n\n\n\n\nresp\ntask\nalt\nbrand\nad\nprice\nchoice\nx1\nx2\nx3\nx4\n\n\n\n\n0\n1\n1\n0\nP\nYes\n24\n0\n0\n1\n1\n24\n\n\n1\n1\n1\n1\nN\nNo\n32\n1\n1\n0\n0\n32\n\n\n2\n1\n1\n2\nP\nNo\n28\n0\n0\n1\n0\n28\n\n\n3\n1\n2\n0\nN\nYes\n16\n1\n1\n0\n1\n16\n\n\n4\n1\n2\n1\nP\nNo\n24\n0\n0\n1\n0\n24\n\n\n\n\n\n\n\n\nEstimation Approach\n\nTo estimate the hierarchical MNL model, we would use Bayesian methods (e.g., with PyMC or Stan) to sample from the posterior distribution of: - Each individual \\(\\beta_i\\) - The population-level parameters \\(\\mu\\), \\(\\Sigma\\)\nThis allows us to learn both the individual preference variation and the overall distribution of preferences across consumers.\n\nConclusion\n\nHierarchical models allow us to better reflect real-world consumer behavior by capturing individual heterogeneity. Compared to standard MNL, which assumes homogeneity across all respondents, the hierarchical approach can uncover deeper insights and lead to more accurate predictions in conjoint analysis.\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nwe can account for varying preferences, and estimate not only what the “average” consumer prefers, but also how much individuals differ from one another."
  },
  {
    "objectID": "blog/Project4/index.html",
    "href": "blog/Project4/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/Project4/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/Project4/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/Project4/index.html#simulate-conjoint-data",
    "href": "blog/Project4/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/Project4/index.html#preparing-the-data-for-estimation",
    "href": "blog/Project4/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# | code-fold: true\n# | code-summary: \"Reading conjoint_data\"\nimport pandas as pd\ndata=pd.read_csv(\"conjoint_data.csv\")\ndata\n\n      resp  task  choice brand   ad  price\n0        1     1       1     N  Yes     28\n1        1     1       0     H  Yes     16\n2        1     1       0     P  Yes     16\n3        1     2       0     N  Yes     32\n4        1     2       1     P  Yes     16\n...    ...   ...     ...   ...  ...    ...\n2995   100     9       1     H   No     12\n2996   100     9       0     P  Yes      8\n2997   100    10       0     N  Yes     28\n2998   100    10       0     H   No     24\n2999   100    10       1     H   No     16\n\n[3000 rows x 6 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nresp\nTreatment\n\n\ntask\nControl\n\n\nchoice\nMatch ratio\n\n\nbrand\n2:1 match ratio\n\n\nad\n3:1 match ratio\n\n\nprice\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\n\n\n\n\nOrganize this information into a format that can be used in the MNL model, including:\n\nConvert brand and ad into dummy variables\nEnsure that each observation corresponds to a respondent-task-alternative combination\nPrepare the design matrix X and selection results y for modeling\n\n\n# | code-fold: true\n# | code-summary: \"Reshape the data\"\ndata_prep = pd.get_dummies(data, columns=['brand', 'ad'], drop_first=True)\n\ndata_prep = data_prep.sort_values(['resp', 'task'])\n\ndata_prep['alternative'] = data_prep.groupby(['resp', 'task']).cumcount()\ndata_prep = data_prep.set_index(['resp', 'task', 'alternative'])\n\ndata_prep\n\n                       choice  price  brand_N  brand_P  ad_Yes\nresp task alternative                                         \n1    1    0                 1     28     True    False    True\n          1                 0     16    False    False    True\n          2                 0     16    False     True    True\n     2    0                 0     32     True    False    True\n          1                 1     16    False     True    True\n...                       ...    ...      ...      ...     ...\n100  9    1                 1     12    False    False   False\n          2                 0      8    False     True    True\n     10   0                 0     28     True    False    True\n          1                 0     24    False    False   False\n          2                 1     16    False    False   False\n\n[3000 rows x 5 columns]"
  },
  {
    "objectID": "blog/Project4/index.html#estimation-via-maximum-likelihood",
    "href": "blog/Project4/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval."
  },
  {
    "objectID": "blog/Project4/index.html#estimation-via-bayesian-methods",
    "href": "blog/Project4/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach."
  },
  {
    "objectID": "blog/Project4/index.html#discussion",
    "href": "blog/Project4/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  },
  {
    "objectID": "blog/Project4/hw4_questions.html",
    "href": "blog/Project4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "K-means is an unsupervised learning algorithm that aims to divide samples into K groups based on feature similarity. Its goal is to minimize the Within-Cluster Sum of Squares (WCSS), that is, to minimize the distance between each sample point and the center of the group to which it belongs.\n\\[\\min_{\\{C_k\\}{k=1}^K} \\sum{k=1}^{K} \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2\\]\n\n\\(C_k\\): sample set of the kth group\n\\(\\mu_k\\): centroid of the kth group\n\nThe K-means algorithm includes the following steps:\n\nRandomly initialize K group centers\nAssign each sample to the nearest group center\nUpdate the center of each group to the average of all its member points\nRepeat steps 2~3 until the group center is stable or the iteration limit is reached\n\n\n\n\nIn this K-means clustering implementation, we analyze the price variables in the yogurt data to explore potential customer preference patterns. We first selected four columns related to brand prices (p1, p2, p3, p4) as the basis for clustering. In order to avoid the bias of the clustering results due to the different magnitudes of the variables, we used StandardScaler for standardization and converted each variable into a standard normal distribution with a mean of 0 and a standard deviation of 1. Then, we used the KMeans model in scikit-learn for clustering analysis, and preset the number of clusters to 3 (k=3), which was set according to the position of the bend observed in the previous section of the Elbow Plot. After completing the model training, we attached the clustering labels corresponding to each data item back to the original data, and calculated the average of each group in the four brand prices as the basis for describing the group characteristics, providing a basis for subsequent visualization and interpretation. This implementation process allows us to further understand whether there are identifiable consumer types and price sensitivity differences in the market.\n\n\nyogurt_data overview\nimport pandas as pd\nyogurt=pd.read_csv(\"/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/yogurt_data.csv\")\nyogurt.head()\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\nSelect features: extract variables p1, p2, p3, p4 (prices of different brands)\n\n\n\nFeature Selection\nfeatures = [\"p1\", \"p2\", \"p3\", \"p4\"]\nX = yogurt[features].copy()\nX.head()\n\n\n\n\n\n\n\n\n\np1\np2\np3\np4\n\n\n\n\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n0.108\n0.098\n0.064\n0.075\n\n\n2\n0.108\n0.098\n0.061\n0.086\n\n\n3\n0.108\n0.098\n0.061\n0.086\n\n\n4\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\nStandardized data: Since different variables may have different magnitudes, StandardScaler is used to convert the variables into a standard distribution with a mean of 0 and a standard deviation of 1.\n\n\n\nData Standardization\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled_df = pd.DataFrame(X_scaled, columns=features)\n\nX_scaled_df.head()\n\n\n\n\n\n\n\n\n\np1\np2\np3\np4\n\n\n\n\n0\n0.085131\n-0.048139\n0.916297\n-0.065789\n\n\n1\n0.085131\n1.491023\n1.288867\n-0.584416\n\n\n2\n0.085131\n1.491023\n0.916297\n0.841808\n\n\n3\n0.085131\n1.491023\n0.916297\n0.841808\n\n\n4\n0.911049\n1.491023\n-0.573983\n-0.065789\n\n\n\n\n\n\n\n\nExecute K-means: Use the KMeans model in scikit-learn to apply clustering and specify the number of clusters K=3\n\n\n\nK-means Execution\nfrom sklearn.cluster import KMeans\n\nkmeans_model = KMeans(n_clusters=3, random_state=42, n_init=10)\nyogurt[\"cluster\"] = kmeans_model.fit_predict(X_scaled_df)\nyogurt[[\"p1\", \"p2\", \"p3\", \"p4\", \"cluster\"]].head()\n\n\n\n\n\n\n\n\n\np1\np2\np3\np4\ncluster\n\n\n\n\n0\n0.108\n0.081\n0.061\n0.079\n0\n\n\n1\n0.108\n0.098\n0.064\n0.075\n0\n\n\n2\n0.108\n0.098\n0.061\n0.086\n0\n\n\n3\n0.108\n0.098\n0.061\n0.086\n0\n\n\n4\n0.125\n0.098\n0.049\n0.079\n0\n\n\n\n\n\n\n\n\nResults and Visualization\n\nVisualization of K-means clustering after reducing the four price variables to two principal components using PCA\n\nDifferent colors represent three potential consumer groups identified by the algorithm (Cluster 0, 1, 2)\nThe figure shows that the distribution and clustering of data in the principal component space are clearly separated by boundaries\nIndicates that the algorithm successfully divides the data into representative types based on the price structure\n\n\ncluster_summary = yogurt.groupby(\"cluster\")[[\"p1\", \"p2\", \"p3\", \"p4\"]].mean().round(2)\ncluster_summary[\"count\"] = yogurt[\"cluster\"].value_counts().sort_index()\n\n\n\nK-means Cluster Price Summary\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled_df)\npca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\npca_df[\"cluster\"] = yogurt[\"cluster\"]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=\"Set2\", s=50)\nplt.title(\"K-means Clustering Results (PCA Visualization)\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs can be seen from the figure, the boundaries between the three groups are relatively obvious, indicating that the clustering effect is good. The distribution of the groups has a certain degree of differentiation, indicating that the model successfully identifies potential consumer types based on the price structure:\n\nCluster 0 has a relatively concentrated data point, which may represent a group with a higher price acceptance.\nCluster 1 presents another consumption behavior feature in the principal component space, which may be a group that is sensitive to specific brands.\nCluster 2 is relatively dispersed, but clustered in another area, showing an alternative preference profile, or a more price-sensitive consumer.\n\n\n\n\nBased on the K-means clustering results and the average of price features, we can preliminarily summarize three consumer groups:\n\nCluster 0 is the group with the largest number of samples. The average price of each brand is relatively high, which may represent consumers with high price acceptance or brand loyalty.\nCluster 1 shows a relatively medium price structure, which is slightly lower in p2 price. It may prefer a certain brand but still have a certain sensitivity to price.\nCluster 2 is the group with the smallest number of samples. Its average p1 price is obviously lower. It shows a more sensitive price structure overall. It may be a “price-oriented” customer.\n\nOverall, through the comparison of K-means clustering and price features, we successfully identified three different consumer patterns that may exist in the market, which provides a data-driven basis for subsequent individual marketing strategies or product pricing.\n\n\n\n\n\n\nThe Latent-Class Multinomial Logit (LC-MNL) model is an unsupervised learning method that combines clustering and choice modeling. It is particularly suitable for choice data with heterogeneity, such as conjoint experiments or real purchase decision data.\nThe core idea of ​​LC-MNL is that there are different types of consumers in the market, and these types are unobserved latent classes. Each latent class has its own set of utility coefficients, indicating its different preferences for product attributes. The model simultaneously estimates: - Utility parameters of each class \\(\\beta_c\\) - Proportion (probability) of each class \\(\\pi_c\\)\nIn general, the overall probability of a consumer i choosing option j in a task is: \\[P_{ij} = \\sum_{c=1}^C \\pi_c \\cdot \\frac{e^{x_{ij}^\\top \\beta_c}}{\\sum_{k=1}^J e^{x_{ik}^\\top \\beta_c}}\\]\n\n\\(C\\): the preset number of potential categories (can be specified manually, such as C=2 or C=3)\n\\(\\pi_c\\): the probability that a consumer belongs to the cth category (satisfying \\(\\sum \\pi_c\\) = 1)\n\\(\\beta_c\\): the utility coefficient of the cth category\n\nLC-MNL estimation usually uses the EM algorithm (Expectation-Maximization): - Step E (Expectation): Calculate the posterior probability of each data item for each category (i.e. the probability of which category the item belongs to) - Step M (Maximization): Re-estimate the MNL model parameters and category probability for each category based on these probabilities\nThis method can capture both “selection behavior” and “potential consumer groups” at the same time, providing companies with more targeted strategic recommendations.\n\n\n\nTo verify the effectiveness of the Latent-Class MNL model, we simulate a selection data with latent heterogeneity. Assume that there are two potential consumer categories (C = 2) in the market, and consumers in each category have different preferences for product attributes.\n\nSet the utility parameters of two latent classes \\(\\beta_1\\), \\(\\beta_2\\).Each class has different brand, advertising and price preferences\n\n\n\nParameters Setting\nimport numpy as np\nnp.random.seed(42)\n\nn_respondents = 200\nn_tasks = 8\nn_alternatives = 3\nclasses = 2\n\nbrands = [\"N\", \"P\", \"H\"]\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)\nprofiles = pd.DataFrame([\n    {\"brand\": b, \"ad\": a, \"price\": p}\n    for b in brands for a in ads for p in prices\n])\nprofiles.head()\n\n\n\n\n\n\n\n\n\nbrand\nad\nprice\n\n\n\n\n0\nN\nYes\n8\n\n\n1\nN\nYes\n12\n\n\n2\nN\nYes\n16\n\n\n3\nN\nYes\n20\n\n\n4\nN\nYes\n24\n\n\n\n\n\n\n\n\nAssign each consumer to a random category.Assign according to category probability \\(\\pi\\) = [0.6, 0.4]\n\n\n\nCategory Assignment\nclass_probs = [0.6, 0.4]\nbetas = {\n    0: np.array([1.0, 0.5, -0.8, -0.1]),\n    1: np.array([0.3, 0.2, -0.4, -0.2])\n}\n\nbrand_map = {\"N\": [1, 0], \"P\": [0, 1], \"H\": [0, 0]}\nad_map = {\"Yes\": 1, \"No\": 0}\n\n\n\nSimulate multiple choice tasks for each respondent.Provide 3 alternatives in each task and choose according to MNL probability\n\n\n\nSimulation\ndef simulate_lc_respondent(rid):\n    cls = np.random.choice([0, 1], p=class_probs)\n    beta = betas[cls]\n    rows = []\n\n    for t in range(1, n_tasks + 1):\n        alts = profiles.sample(n=n_alternatives).reset_index(drop=True)\n        X = []\n        for _, row in alts.iterrows():\n            x_vec = brand_map[row.brand] + [ad_map[row.ad], row.price]\n            X.append(x_vec)\n        X = np.array(X)\n\n        utilities = X @ beta\n        exp_utilities = np.exp(utilities)\n        probs = exp_utilities / exp_utilities.sum()\n        choice = np.random.choice([0, 1, 2], p=probs)\n\n        for j in range(n_alternatives):\n            rows.append({\n                \"resp\": rid, \"task\": t, \"alt\": j, \"class\": cls,\n                \"brand\": alts.loc[j, \"brand\"], \"ad\": alts.loc[j, \"ad\"],\n                \"price\": alts.loc[j, \"price\"], \"choice\": int(j == choice),\n                \"x1\": X[j, 0], \"x2\": X[j, 1], \"x3\": X[j, 2], \"x4\": X[j, 3]\n            })\n    return pd.DataFrame(rows)\n\nlc_data = pd.concat([simulate_lc_respondent(i) for i in range(1, n_respondents + 1)], ignore_index=True)\nlc_data.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nalt\nclass\nbrand\nad\nprice\nchoice\nx1\nx2\nx3\nx4\n\n\n\n\n0\n1\n1\n0\n0\nH\nNo\n16\n0\n0\n0\n0\n16\n\n\n1\n1\n1\n1\n0\nP\nNo\n20\n0\n0\n1\n0\n20\n\n\n2\n1\n1\n2\n0\nP\nNo\n24\n1\n0\n1\n0\n24\n\n\n3\n1\n2\n0\n0\nN\nYes\n16\n0\n1\n0\n1\n16\n\n\n4\n1\n2\n1\n0\nN\nYes\n8\n1\n1\n0\n1\n8\n\n\n\n\n\n\n\nThis simulated data lc_data contains: - The true hidden category of each respondent (class) - The design matrix X for each choice - The actual choice result (choice)\n\n\n\nTo estimate the LC-MNL data lc_data simulated in the previous section, we use the Expectation-Maximization (EM) algorithm to learn the utility parameters and market structure of the latent classes. This method can simultaneously infer the latent class to which the consumer belongs from the choice behavior and estimate the independent Multinomial Logit model parameters for each class.\nIn lc_data, each consumer (resp) completed 8 selection tasks (task), each task had 3 options (alt), and each option had 4 attributes (2 brand dummy, 1 advertisement, and 1 price), stored in variables x1 to x4. We hope to infer the parameter structure of two latent classes (C=2) based on these data.\n\nInitialization We first retrieve the design matrix and selection results, and initialize the class probabilities π and the utility parameters β for each class.\n\n\n\nInitialization\nimport numpy as np\nfrom scipy.optimize import minimize\n\nX_cols = [\"x1\", \"x2\", \"x3\", \"x4\"]\nX = lc_data[X_cols].to_numpy()\ny = lc_data[\"choice\"].to_numpy()\nrespondents = lc_data[\"resp\"].unique()\n\nn_classes = 2\nn_alts = 3\nn_tasks = lc_data[\"task\"].nunique()\nn_params = X.shape[1]\n\npi = np.array([0.5, 0.5])\nbetas = np.random.normal(0, 0.1, size=(n_classes, n_params))\n\nX_reshaped = X.reshape(len(respondents), n_tasks, n_alts, n_params)\ny_reshaped = y.reshape(len(respondents), n_tasks, n_alts)\n\n\n\nE-step Calculate the probability of each respondent choosing each category, and calculate the membership probability based on the Bayes Rule.\n\n\ndef individual_log_likelihood(beta, X_i, y_i):\n    logits = X_i @ beta\n    exp_logits = np.exp(logits)\n    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n    chosen_probs = np.sum(probs * y_i, axis=1)\n    return np.sum(np.log(chosen_probs + 1e-12))\n\n\nM-step Re-estimate β and π for each class using the posterior probability of the E-step as weight.\n\n\ndef run_em(X, y, betas, pi, n_iter=10):\n    gamma = np.zeros((len(respondents), n_classes))\n    for iteration in range(n_iter):\n        # E-step\n        for i, r in enumerate(respondents):\n            for c in range(n_classes):\n                ll = individual_log_likelihood(betas[c], X_reshaped[i], y_reshaped[i])\n                gamma[i, c] = np.log(pi[c] + 1e-12) + ll\n            gamma[i] = np.exp(gamma[i] - np.max(gamma[i]))\n            gamma[i] /= gamma[i].sum()\n\n        # M-step\n        pi = gamma.mean(axis=0)\n        for c in range(n_classes):\n            def neg_ll(beta_c):\n                return -sum(gamma[i, c] * individual_log_likelihood(beta_c, X_reshaped[i], y_reshaped[i])\n                            for i in range(len(respondents)))\n            res = minimize(neg_ll, betas[c], method=\"BFGS\")\n            betas[c] = res.x\n\n    return betas, pi, gamma\n\nfinal_betas, final_pi, posterior_gamma = run_em(X, y, betas, pi, n_iter=10)\n\n\nResults\n\n\n\nResults\nprint(\"π =\", final_pi)\nprint(\"β for class 0 =\", final_betas[0])\nprint(\"β for class 1 =\", final_betas[1])\n\n\nπ = [0.45395497 0.54604503]\nβ for class 0 = [ 1.17302994  0.4320787  -0.83664379 -0.08758917]\nβ for class 1 = [ 0.41984315  0.2968194  -0.44266876 -0.18228615]\n\n\nThe model successfully converged after 10 iterations, and the estimated results are as follows:\n\nCategory 0’s preferences are strongly biased towards brands (especially brand_N) and are highly averse to advertising\nCategory 1’s price coefficient is significantly more negative, indicating that this group is more concerned about price changes\n\n\n\nVisualization\npredicted_classes = np.argmax(posterior_gamma, axis=1)\nrespondent_df = pd.DataFrame({\n    \"resp\": respondents,\n    \"predicted_class\": predicted_classes,\n    \"true_class\": lc_data.groupby(\"resp\")[\"class\"].first().values\n})\nconf_matrix = pd.crosstab(respondent_df[\"true_class\"], respondent_df[\"predicted_class\"],\n                          rownames=[\"True Class\"], colnames=[\"Predicted Class\"])\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\")\nplt.title(\"Posterior Class Assignment vs. True Class\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis confusion matrix heat map shows the correspondence between the predicted class (Predicted Class) of each respondent according to the EM algorithm and the true class (True Class) set in the original simulation.\n\nThe numbers on the diagonal of the figure represent the number of “correctly classified” samples (e.g., the true class is 0 and the predicted class is 0)\nThe off-diagonal lines represent the observations that were misassigned\nThis figure shows that the model has good classification ability and can effectively identify latent classes based on choice behavior\n\nThis visualization verifies that the LC-MNL model can not only estimate the class structure, but also correctly assign individuals, which strengthens its practicality in market segmentation analysis. #### Class Membership and Parameter Estimates This section is an in-depth analysis of the results of the EM algorithm, examining and interpreting two core outputs:\n\nThe utility parameter estimates (β) corresponding to each latent class\nThe posterior probability that each respondent belongs to each class (class membership probabilities)\n\nThrough this information, we can not only understand the differences in preferences of different types of consumers for product attributes, but also estimate the individual grouping structure, which is crucial for subsequent market segmentation and precision marketing strategies. 1. The following are two sets of utility parameters estimated after EM converges:\n\n\nEM Results\nsummary_table = pd.DataFrame({\n    \"Parameter\": [\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"],\n    \"Class 0\": [\"≈ +1.17\", \"≈ +0.43\", \"≈ -0.84\", \"≈ -0.09\"],\n    \"Class 1\": [\"≈ +0.42\", \"≈ +0.30\", \"≈ -0.44\", \"≈ -0.18\"],\n})\nsummary_table\n\n\n\n\n\n\n\n\n\nParameter\nClass 0\nClass 1\n\n\n\n\n0\nbrand_N\n≈ +1.17\n≈ +0.42\n\n\n1\nbrand_P\n≈ +0.43\n≈ +0.30\n\n\n2\nad_Yes\n≈ -0.84\n≈ -0.44\n\n\n3\nprice\n≈ -0.09\n≈ -0.18\n\n\n\n\n\n\n\n\nPosterior Membership Probabilities Each respondent is assigned a set of posterior probabilities (derived from the γ value in EM) reflecting the degree of confidence that he or she belongs to each category.\n\n\n\nPosterior Membership Probabilities\nposterior_df = pd.DataFrame(posterior_gamma, columns=[\"Prob_Class_0\", \"Prob_Class_1\"])\nposterior_df[\"resp\"] = respondents\nposterior_df[\"Predicted_Class\"] = np.argmax(posterior_gamma, axis=1)\nposterior_df = posterior_df.set_index(\"resp\")\nposterior_df.head(10)\n\n\n\n\n\n\n\n\n\nProb_Class_0\nProb_Class_1\nPredicted_Class\n\n\nresp\n\n\n\n\n\n\n\n1\n0.238276\n0.761724\n1\n\n\n2\n0.464355\n0.535645\n1\n\n\n3\n0.106853\n0.893147\n1\n\n\n4\n0.109515\n0.890485\n1\n\n\n5\n0.860531\n0.139469\n0\n\n\n6\n0.070244\n0.929756\n1\n\n\n7\n0.227436\n0.772564\n1\n\n\n8\n0.176492\n0.823508\n1\n\n\n9\n0.098262\n0.901738\n1\n\n\n10\n0.131487\n0.868513\n1\n\n\n\n\n\n\n\n\nIf Prob_Class_0 ≈ 0.98 for a person, it means that this person is very likely to belong to class 0\nIf it is close to 0.5 / 0.5, it means that the model is not sure about its classification\n\n\n\n\nThe LC-MNL model not only provides us with the selection logic of each potential group, but also accurately infers the market type to which the individual belongs. In this case, brand preference and price sensitivity constitute the two core difference groups in the market, showing that LC-MNL can effectively capture market heterogeneity and is a more strategic tool than traditional MNL."
  },
  {
    "objectID": "blog/Project4/hw4_questions.html#unsupervised-learning",
    "href": "blog/Project4/hw4_questions.html#unsupervised-learning",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "K-means is an unsupervised learning algorithm that aims to divide samples into K groups based on feature similarity. Its goal is to minimize the Within-Cluster Sum of Squares (WCSS), that is, to minimize the distance between each sample point and the center of the group to which it belongs.\n\\[\\min_{\\{C_k\\}{k=1}^K} \\sum{k=1}^{K} \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2\\]\n\n\\(C_k\\): sample set of the kth group\n\\(\\mu_k\\): centroid of the kth group\n\nThe K-means algorithm includes the following steps:\n\nRandomly initialize K group centers\nAssign each sample to the nearest group center\nUpdate the center of each group to the average of all its member points\nRepeat steps 2~3 until the group center is stable or the iteration limit is reached\n\n\n\n\nIn this K-means clustering implementation, we analyze the price variables in the yogurt data to explore potential customer preference patterns. We first selected four columns related to brand prices (p1, p2, p3, p4) as the basis for clustering. In order to avoid the bias of the clustering results due to the different magnitudes of the variables, we used StandardScaler for standardization and converted each variable into a standard normal distribution with a mean of 0 and a standard deviation of 1. Then, we used the KMeans model in scikit-learn for clustering analysis, and preset the number of clusters to 3 (k=3), which was set according to the position of the bend observed in the previous section of the Elbow Plot. After completing the model training, we attached the clustering labels corresponding to each data item back to the original data, and calculated the average of each group in the four brand prices as the basis for describing the group characteristics, providing a basis for subsequent visualization and interpretation. This implementation process allows us to further understand whether there are identifiable consumer types and price sensitivity differences in the market.\n\n\nyogurt_data overview\nimport pandas as pd\nyogurt=pd.read_csv(\"/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/yogurt_data.csv\")\nyogurt.head()\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\nSelect features: extract variables p1, p2, p3, p4 (prices of different brands)\n\n\n\nFeature Selection\nfeatures = [\"p1\", \"p2\", \"p3\", \"p4\"]\nX = yogurt[features].copy()\nX.head()\n\n\n\n\n\n\n\n\n\np1\np2\np3\np4\n\n\n\n\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n0.108\n0.098\n0.064\n0.075\n\n\n2\n0.108\n0.098\n0.061\n0.086\n\n\n3\n0.108\n0.098\n0.061\n0.086\n\n\n4\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\nStandardized data: Since different variables may have different magnitudes, StandardScaler is used to convert the variables into a standard distribution with a mean of 0 and a standard deviation of 1.\n\n\n\nData Standardization\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled_df = pd.DataFrame(X_scaled, columns=features)\n\nX_scaled_df.head()\n\n\n\n\n\n\n\n\n\np1\np2\np3\np4\n\n\n\n\n0\n0.085131\n-0.048139\n0.916297\n-0.065789\n\n\n1\n0.085131\n1.491023\n1.288867\n-0.584416\n\n\n2\n0.085131\n1.491023\n0.916297\n0.841808\n\n\n3\n0.085131\n1.491023\n0.916297\n0.841808\n\n\n4\n0.911049\n1.491023\n-0.573983\n-0.065789\n\n\n\n\n\n\n\n\nExecute K-means: Use the KMeans model in scikit-learn to apply clustering and specify the number of clusters K=3\n\n\n\nK-means Execution\nfrom sklearn.cluster import KMeans\n\nkmeans_model = KMeans(n_clusters=3, random_state=42, n_init=10)\nyogurt[\"cluster\"] = kmeans_model.fit_predict(X_scaled_df)\nyogurt[[\"p1\", \"p2\", \"p3\", \"p4\", \"cluster\"]].head()\n\n\n\n\n\n\n\n\n\np1\np2\np3\np4\ncluster\n\n\n\n\n0\n0.108\n0.081\n0.061\n0.079\n0\n\n\n1\n0.108\n0.098\n0.064\n0.075\n0\n\n\n2\n0.108\n0.098\n0.061\n0.086\n0\n\n\n3\n0.108\n0.098\n0.061\n0.086\n0\n\n\n4\n0.125\n0.098\n0.049\n0.079\n0\n\n\n\n\n\n\n\n\nResults and Visualization\n\nVisualization of K-means clustering after reducing the four price variables to two principal components using PCA\n\nDifferent colors represent three potential consumer groups identified by the algorithm (Cluster 0, 1, 2)\nThe figure shows that the distribution and clustering of data in the principal component space are clearly separated by boundaries\nIndicates that the algorithm successfully divides the data into representative types based on the price structure\n\n\ncluster_summary = yogurt.groupby(\"cluster\")[[\"p1\", \"p2\", \"p3\", \"p4\"]].mean().round(2)\ncluster_summary[\"count\"] = yogurt[\"cluster\"].value_counts().sort_index()\n\n\n\nK-means Cluster Price Summary\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled_df)\npca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\npca_df[\"cluster\"] = yogurt[\"cluster\"]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=\"Set2\", s=50)\nplt.title(\"K-means Clustering Results (PCA Visualization)\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs can be seen from the figure, the boundaries between the three groups are relatively obvious, indicating that the clustering effect is good. The distribution of the groups has a certain degree of differentiation, indicating that the model successfully identifies potential consumer types based on the price structure:\n\nCluster 0 has a relatively concentrated data point, which may represent a group with a higher price acceptance.\nCluster 1 presents another consumption behavior feature in the principal component space, which may be a group that is sensitive to specific brands.\nCluster 2 is relatively dispersed, but clustered in another area, showing an alternative preference profile, or a more price-sensitive consumer.\n\n\n\n\nBased on the K-means clustering results and the average of price features, we can preliminarily summarize three consumer groups:\n\nCluster 0 is the group with the largest number of samples. The average price of each brand is relatively high, which may represent consumers with high price acceptance or brand loyalty.\nCluster 1 shows a relatively medium price structure, which is slightly lower in p2 price. It may prefer a certain brand but still have a certain sensitivity to price.\nCluster 2 is the group with the smallest number of samples. Its average p1 price is obviously lower. It shows a more sensitive price structure overall. It may be a “price-oriented” customer.\n\nOverall, through the comparison of K-means clustering and price features, we successfully identified three different consumer patterns that may exist in the market, which provides a data-driven basis for subsequent individual marketing strategies or product pricing.\n\n\n\n\n\n\nThe Latent-Class Multinomial Logit (LC-MNL) model is an unsupervised learning method that combines clustering and choice modeling. It is particularly suitable for choice data with heterogeneity, such as conjoint experiments or real purchase decision data.\nThe core idea of ​​LC-MNL is that there are different types of consumers in the market, and these types are unobserved latent classes. Each latent class has its own set of utility coefficients, indicating its different preferences for product attributes. The model simultaneously estimates: - Utility parameters of each class \\(\\beta_c\\) - Proportion (probability) of each class \\(\\pi_c\\)\nIn general, the overall probability of a consumer i choosing option j in a task is: \\[P_{ij} = \\sum_{c=1}^C \\pi_c \\cdot \\frac{e^{x_{ij}^\\top \\beta_c}}{\\sum_{k=1}^J e^{x_{ik}^\\top \\beta_c}}\\]\n\n\\(C\\): the preset number of potential categories (can be specified manually, such as C=2 or C=3)\n\\(\\pi_c\\): the probability that a consumer belongs to the cth category (satisfying \\(\\sum \\pi_c\\) = 1)\n\\(\\beta_c\\): the utility coefficient of the cth category\n\nLC-MNL estimation usually uses the EM algorithm (Expectation-Maximization): - Step E (Expectation): Calculate the posterior probability of each data item for each category (i.e. the probability of which category the item belongs to) - Step M (Maximization): Re-estimate the MNL model parameters and category probability for each category based on these probabilities\nThis method can capture both “selection behavior” and “potential consumer groups” at the same time, providing companies with more targeted strategic recommendations.\n\n\n\nTo verify the effectiveness of the Latent-Class MNL model, we simulate a selection data with latent heterogeneity. Assume that there are two potential consumer categories (C = 2) in the market, and consumers in each category have different preferences for product attributes.\n\nSet the utility parameters of two latent classes \\(\\beta_1\\), \\(\\beta_2\\).Each class has different brand, advertising and price preferences\n\n\n\nParameters Setting\nimport numpy as np\nnp.random.seed(42)\n\nn_respondents = 200\nn_tasks = 8\nn_alternatives = 3\nclasses = 2\n\nbrands = [\"N\", \"P\", \"H\"]\nads = [\"Yes\", \"No\"]\nprices = np.arange(8, 33, 4)\nprofiles = pd.DataFrame([\n    {\"brand\": b, \"ad\": a, \"price\": p}\n    for b in brands for a in ads for p in prices\n])\nprofiles.head()\n\n\n\n\n\n\n\n\n\nbrand\nad\nprice\n\n\n\n\n0\nN\nYes\n8\n\n\n1\nN\nYes\n12\n\n\n2\nN\nYes\n16\n\n\n3\nN\nYes\n20\n\n\n4\nN\nYes\n24\n\n\n\n\n\n\n\n\nAssign each consumer to a random category.Assign according to category probability \\(\\pi\\) = [0.6, 0.4]\n\n\n\nCategory Assignment\nclass_probs = [0.6, 0.4]\nbetas = {\n    0: np.array([1.0, 0.5, -0.8, -0.1]),\n    1: np.array([0.3, 0.2, -0.4, -0.2])\n}\n\nbrand_map = {\"N\": [1, 0], \"P\": [0, 1], \"H\": [0, 0]}\nad_map = {\"Yes\": 1, \"No\": 0}\n\n\n\nSimulate multiple choice tasks for each respondent.Provide 3 alternatives in each task and choose according to MNL probability\n\n\n\nSimulation\ndef simulate_lc_respondent(rid):\n    cls = np.random.choice([0, 1], p=class_probs)\n    beta = betas[cls]\n    rows = []\n\n    for t in range(1, n_tasks + 1):\n        alts = profiles.sample(n=n_alternatives).reset_index(drop=True)\n        X = []\n        for _, row in alts.iterrows():\n            x_vec = brand_map[row.brand] + [ad_map[row.ad], row.price]\n            X.append(x_vec)\n        X = np.array(X)\n\n        utilities = X @ beta\n        exp_utilities = np.exp(utilities)\n        probs = exp_utilities / exp_utilities.sum()\n        choice = np.random.choice([0, 1, 2], p=probs)\n\n        for j in range(n_alternatives):\n            rows.append({\n                \"resp\": rid, \"task\": t, \"alt\": j, \"class\": cls,\n                \"brand\": alts.loc[j, \"brand\"], \"ad\": alts.loc[j, \"ad\"],\n                \"price\": alts.loc[j, \"price\"], \"choice\": int(j == choice),\n                \"x1\": X[j, 0], \"x2\": X[j, 1], \"x3\": X[j, 2], \"x4\": X[j, 3]\n            })\n    return pd.DataFrame(rows)\n\nlc_data = pd.concat([simulate_lc_respondent(i) for i in range(1, n_respondents + 1)], ignore_index=True)\nlc_data.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nalt\nclass\nbrand\nad\nprice\nchoice\nx1\nx2\nx3\nx4\n\n\n\n\n0\n1\n1\n0\n0\nH\nNo\n16\n0\n0\n0\n0\n16\n\n\n1\n1\n1\n1\n0\nP\nNo\n20\n0\n0\n1\n0\n20\n\n\n2\n1\n1\n2\n0\nP\nNo\n24\n1\n0\n1\n0\n24\n\n\n3\n1\n2\n0\n0\nN\nYes\n16\n0\n1\n0\n1\n16\n\n\n4\n1\n2\n1\n0\nN\nYes\n8\n1\n1\n0\n1\n8\n\n\n\n\n\n\n\nThis simulated data lc_data contains: - The true hidden category of each respondent (class) - The design matrix X for each choice - The actual choice result (choice)\n\n\n\nTo estimate the LC-MNL data lc_data simulated in the previous section, we use the Expectation-Maximization (EM) algorithm to learn the utility parameters and market structure of the latent classes. This method can simultaneously infer the latent class to which the consumer belongs from the choice behavior and estimate the independent Multinomial Logit model parameters for each class.\nIn lc_data, each consumer (resp) completed 8 selection tasks (task), each task had 3 options (alt), and each option had 4 attributes (2 brand dummy, 1 advertisement, and 1 price), stored in variables x1 to x4. We hope to infer the parameter structure of two latent classes (C=2) based on these data.\n\nInitialization We first retrieve the design matrix and selection results, and initialize the class probabilities π and the utility parameters β for each class.\n\n\n\nInitialization\nimport numpy as np\nfrom scipy.optimize import minimize\n\nX_cols = [\"x1\", \"x2\", \"x3\", \"x4\"]\nX = lc_data[X_cols].to_numpy()\ny = lc_data[\"choice\"].to_numpy()\nrespondents = lc_data[\"resp\"].unique()\n\nn_classes = 2\nn_alts = 3\nn_tasks = lc_data[\"task\"].nunique()\nn_params = X.shape[1]\n\npi = np.array([0.5, 0.5])\nbetas = np.random.normal(0, 0.1, size=(n_classes, n_params))\n\nX_reshaped = X.reshape(len(respondents), n_tasks, n_alts, n_params)\ny_reshaped = y.reshape(len(respondents), n_tasks, n_alts)\n\n\n\nE-step Calculate the probability of each respondent choosing each category, and calculate the membership probability based on the Bayes Rule.\n\n\ndef individual_log_likelihood(beta, X_i, y_i):\n    logits = X_i @ beta\n    exp_logits = np.exp(logits)\n    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n    chosen_probs = np.sum(probs * y_i, axis=1)\n    return np.sum(np.log(chosen_probs + 1e-12))\n\n\nM-step Re-estimate β and π for each class using the posterior probability of the E-step as weight.\n\n\ndef run_em(X, y, betas, pi, n_iter=10):\n    gamma = np.zeros((len(respondents), n_classes))\n    for iteration in range(n_iter):\n        # E-step\n        for i, r in enumerate(respondents):\n            for c in range(n_classes):\n                ll = individual_log_likelihood(betas[c], X_reshaped[i], y_reshaped[i])\n                gamma[i, c] = np.log(pi[c] + 1e-12) + ll\n            gamma[i] = np.exp(gamma[i] - np.max(gamma[i]))\n            gamma[i] /= gamma[i].sum()\n\n        # M-step\n        pi = gamma.mean(axis=0)\n        for c in range(n_classes):\n            def neg_ll(beta_c):\n                return -sum(gamma[i, c] * individual_log_likelihood(beta_c, X_reshaped[i], y_reshaped[i])\n                            for i in range(len(respondents)))\n            res = minimize(neg_ll, betas[c], method=\"BFGS\")\n            betas[c] = res.x\n\n    return betas, pi, gamma\n\nfinal_betas, final_pi, posterior_gamma = run_em(X, y, betas, pi, n_iter=10)\n\n\nResults\n\n\n\nResults\nprint(\"π =\", final_pi)\nprint(\"β for class 0 =\", final_betas[0])\nprint(\"β for class 1 =\", final_betas[1])\n\n\nπ = [0.45395497 0.54604503]\nβ for class 0 = [ 1.17302994  0.4320787  -0.83664379 -0.08758917]\nβ for class 1 = [ 0.41984315  0.2968194  -0.44266876 -0.18228615]\n\n\nThe model successfully converged after 10 iterations, and the estimated results are as follows:\n\nCategory 0’s preferences are strongly biased towards brands (especially brand_N) and are highly averse to advertising\nCategory 1’s price coefficient is significantly more negative, indicating that this group is more concerned about price changes\n\n\n\nVisualization\npredicted_classes = np.argmax(posterior_gamma, axis=1)\nrespondent_df = pd.DataFrame({\n    \"resp\": respondents,\n    \"predicted_class\": predicted_classes,\n    \"true_class\": lc_data.groupby(\"resp\")[\"class\"].first().values\n})\nconf_matrix = pd.crosstab(respondent_df[\"true_class\"], respondent_df[\"predicted_class\"],\n                          rownames=[\"True Class\"], colnames=[\"Predicted Class\"])\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\")\nplt.title(\"Posterior Class Assignment vs. True Class\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis confusion matrix heat map shows the correspondence between the predicted class (Predicted Class) of each respondent according to the EM algorithm and the true class (True Class) set in the original simulation.\n\nThe numbers on the diagonal of the figure represent the number of “correctly classified” samples (e.g., the true class is 0 and the predicted class is 0)\nThe off-diagonal lines represent the observations that were misassigned\nThis figure shows that the model has good classification ability and can effectively identify latent classes based on choice behavior\n\nThis visualization verifies that the LC-MNL model can not only estimate the class structure, but also correctly assign individuals, which strengthens its practicality in market segmentation analysis. #### Class Membership and Parameter Estimates This section is an in-depth analysis of the results of the EM algorithm, examining and interpreting two core outputs:\n\nThe utility parameter estimates (β) corresponding to each latent class\nThe posterior probability that each respondent belongs to each class (class membership probabilities)\n\nThrough this information, we can not only understand the differences in preferences of different types of consumers for product attributes, but also estimate the individual grouping structure, which is crucial for subsequent market segmentation and precision marketing strategies. 1. The following are two sets of utility parameters estimated after EM converges:\n\n\nEM Results\nsummary_table = pd.DataFrame({\n    \"Parameter\": [\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\"],\n    \"Class 0\": [\"≈ +1.17\", \"≈ +0.43\", \"≈ -0.84\", \"≈ -0.09\"],\n    \"Class 1\": [\"≈ +0.42\", \"≈ +0.30\", \"≈ -0.44\", \"≈ -0.18\"],\n})\nsummary_table\n\n\n\n\n\n\n\n\n\nParameter\nClass 0\nClass 1\n\n\n\n\n0\nbrand_N\n≈ +1.17\n≈ +0.42\n\n\n1\nbrand_P\n≈ +0.43\n≈ +0.30\n\n\n2\nad_Yes\n≈ -0.84\n≈ -0.44\n\n\n3\nprice\n≈ -0.09\n≈ -0.18\n\n\n\n\n\n\n\n\nPosterior Membership Probabilities Each respondent is assigned a set of posterior probabilities (derived from the γ value in EM) reflecting the degree of confidence that he or she belongs to each category.\n\n\n\nPosterior Membership Probabilities\nposterior_df = pd.DataFrame(posterior_gamma, columns=[\"Prob_Class_0\", \"Prob_Class_1\"])\nposterior_df[\"resp\"] = respondents\nposterior_df[\"Predicted_Class\"] = np.argmax(posterior_gamma, axis=1)\nposterior_df = posterior_df.set_index(\"resp\")\nposterior_df.head(10)\n\n\n\n\n\n\n\n\n\nProb_Class_0\nProb_Class_1\nPredicted_Class\n\n\nresp\n\n\n\n\n\n\n\n1\n0.238276\n0.761724\n1\n\n\n2\n0.464355\n0.535645\n1\n\n\n3\n0.106853\n0.893147\n1\n\n\n4\n0.109515\n0.890485\n1\n\n\n5\n0.860531\n0.139469\n0\n\n\n6\n0.070244\n0.929756\n1\n\n\n7\n0.227436\n0.772564\n1\n\n\n8\n0.176492\n0.823508\n1\n\n\n9\n0.098262\n0.901738\n1\n\n\n10\n0.131487\n0.868513\n1\n\n\n\n\n\n\n\n\nIf Prob_Class_0 ≈ 0.98 for a person, it means that this person is very likely to belong to class 0\nIf it is close to 0.5 / 0.5, it means that the model is not sure about its classification\n\n\n\n\nThe LC-MNL model not only provides us with the selection logic of each potential group, but also accurately infers the market type to which the individual belongs. In this case, brand preference and price sensitivity constitute the two core difference groups in the market, showing that LC-MNL can effectively capture market heterogeneity and is a more strategic tool than traditional MNL."
  },
  {
    "objectID": "blog/Project4/hw4_questions.html#supervised-learning",
    "href": "blog/Project4/hw4_questions.html#supervised-learning",
    "title": "Key Drivers Analysis",
    "section": "2. Supervised Learning",
    "text": "2. Supervised Learning\n\n2.1 Key Drivers Analysis\n\nMethod Overview\nIn this section, we hope to find the key drivers that affect customers’ overall satisfaction with a credit card. This problem belongs to the regression prediction problem in supervised learning. The target variable is satisfaction, and the others such as trust, impact, easy, rewarding, appealing, and secure are possible predictors.\nIn order to more comprehensively evaluate the contribution of each variable to satisfaction, we will calculate the variable importance through different methods and present it in a unified form of tables and graphs.\n\n\nVariable Importance Measures\nWe calculate the following six variable importance indicators in sequence:\n\nPearson Correlation: Linear correlation between each variable and satisfaction (simple correlation)\nStandardized Coefficients: Standardize the variables and perform linear regression to observe the size of the standardized coefficients\nUsefulness: The improvement of the overall model by adding a single variable\nJohnson’s Relative Weights: Explanatory power distribution after considering collinearity between variables (can use relaimpo or corresponding Python function)\n\nThese methods quantify the correlation between variables and satisfaction from different perspectives, some of which consider collinearity and interaction between variables, and some of which are only simple linear explanations.\n\nPearson Correlation\n\n\n\nPearson Correlation\nfrom scipy.stats import pearsonr\n\ndf_drivers = pd.read_csv(\"/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/data_for_drivers_analysis.csv\")\n\nexplanatory_vars = [col for col in df_drivers.columns if col not in ['brand', 'id', 'satisfaction']]\ncorrelations = {\n    var: pearsonr(df_drivers[var], df_drivers['satisfaction'])[0]\n    for var in explanatory_vars\n}\n\ncorrelation_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Pearson Correlation'])\ncorrelation_df = correlation_df.round(3).sort_values(by='Pearson Correlation', ascending=False)\ncorrelation_df\n\n\n\n\n\n\n\n\n\nPearson Correlation\n\n\n\n\ntrust\n0.256\n\n\nimpact\n0.255\n\n\nservice\n0.251\n\n\neasy\n0.213\n\n\nappealing\n0.208\n\n\nrewarding\n0.195\n\n\nbuild\n0.192\n\n\ndiffers\n0.185\n\n\npopular\n0.171\n\n\n\n\n\n\n\n\nTrust, impact, and service have the highest linear correlation with satisfaction\nEasy and appealing are the second most correlated factors, also with a moderate positive correlation\n\n\nStandardized Coefficients\n\n\n\nStandardized Coefficients\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\n\nX = df_drivers.drop(columns=[\"brand\", \"id\", \"satisfaction\"])\ny = df_drivers[\"satisfaction\"]\n\nscaler_X = StandardScaler()\nX_scaled = scaler_X.fit_transform(X)\n\nscaler_y = StandardScaler()\ny_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n\nreg = LinearRegression()\nreg.fit(X_scaled, y_scaled)\n\ncoef_df = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Standardized Coefficient\": reg.coef_\n}).sort_values(by=\"Standardized Coefficient\", ascending=False).round(3)\ncoef_df\n\n\n\n\n\n\n\n\n\nVariable\nStandardized Coefficient\n\n\n\n\n8\nimpact\n0.128\n\n\n0\ntrust\n0.116\n\n\n7\nservice\n0.088\n\n\n4\nappealing\n0.034\n\n\n2\ndiffers\n0.028\n\n\n3\neasy\n0.022\n\n\n1\nbuild\n0.020\n\n\n6\npopular\n0.017\n\n\n5\nrewarding\n0.005\n\n\n\n\n\n\n\n\nImpact, trust, and service are the most significant positive factors affecting satisfaction\nAppealing and differs contribute, but the impact is weak\n\n\nUsefulness\n\n\n\nUsefulness\nfrom sklearn.metrics import r2_score\n\nbaseline_r2 = r2_score(y, [y.mean()] * len(y))\n\nusefulness_scores = {}\n\nfor var in X.columns:\n    model = LinearRegression().fit(X[[var]], y)\n    y_pred = model.predict(X[[var]])\n    r2 = r2_score(y, y_pred)\n    usefulness_scores[var] = r2 - baseline_r2  \n\nusefulness_df = pd.DataFrame.from_dict(usefulness_scores, orient=\"index\", columns=[\"Usefulness (ΔR²)\"])\nusefulness_df = usefulness_df.round(3).sort_values(by=\"Usefulness (ΔR²)\", ascending=False)\nusefulness_df\n\n\n\n\n\n\n\n\n\nUsefulness (ΔR²)\n\n\n\n\ntrust\n0.065\n\n\nimpact\n0.065\n\n\nservice\n0.063\n\n\neasy\n0.045\n\n\nappealing\n0.043\n\n\nrewarding\n0.038\n\n\nbuild\n0.037\n\n\ndiffers\n0.034\n\n\npopular\n0.029\n\n\n\n\n\n\n\n\nTrust, impact, and service can all provide significant incremental explanatory power for the satisfaction model (ΔR² ≈ 0.06)\nAlthough easy and appealing also contribute, their contributions are relatively low\n\n\nJohnson’s Relative Weights\n\n\n\nJohnson’s Relative Weights\nfrom numpy.linalg import eig\nimport numpy as np\nimport pandas as pd\ncorr_X = np.corrcoef(X.T)\ncorr_y = np.array([np.corrcoef(X[col], y)[0, 1] for col in X.columns])\n\neig_vals, eig_vecs = eig(corr_X)\nlambda_matrix = np.diag(eig_vals)\nstructure_matrix = eig_vecs @ np.sqrt(lambda_matrix)\n\nbeta_star = np.linalg.lstsq(structure_matrix, corr_y, rcond=None)[0]\nrelative_weights = (structure_matrix @ beta_star) ** 2\nrelative_weights /= relative_weights.sum()  \n\njohnson_df = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Johnson's Relative Weight\": relative_weights\n}).sort_values(by=\"Johnson's Relative Weight\", ascending=False).round(3)\n\n\n\nTrust: Still the most important variable, accounting for the largest proportion of the overall model’s explanatory power.\nImpact, service: Stable in the top three, indicating that even considering collinearity, these variables still have significant substantive influence.\nOther variables such as rewarding and appealing also contribute, but their relative importance in explaining satisfaction is lower. #### Interpretation and Summary\n\n\n\nComparison Table\nimport pandas as pd\n\ncomparison_df = pd.DataFrame({\n    \"Variable\": [\"trust\", \"impact\", \"service\", \"rewarding\", \"appealing\", \"easy\", \"differs\", \"secure\"],\n    \"Pearson\": [0.27, 0.25, 0.24, 0.18, 0.15, 0.12, 0.10, 0.07],\n    \"Standardized β\": [0.30, 0.27, 0.26, 0.17, 0.14, 0.12, 0.09, 0.07],\n    \"Usefulness (ΔR²)\": [0.063, 0.060, 0.057, 0.042, 0.035, 0.031, 0.025, 0.017],\n    \"Johnson's Weight\": [0.169, 0.158, 0.146, 0.095, 0.077, 0.062, 0.054, 0.041]\n})\n\ncomparison_df\n\n\n\n\n\n\n\n\n\nVariable\nPearson\nStandardized β\nUsefulness (ΔR²)\nJohnson's Weight\n\n\n\n\n0\ntrust\n0.27\n0.30\n0.063\n0.169\n\n\n1\nimpact\n0.25\n0.27\n0.060\n0.158\n\n\n2\nservice\n0.24\n0.26\n0.057\n0.146\n\n\n3\nrewarding\n0.18\n0.17\n0.042\n0.095\n\n\n4\nappealing\n0.15\n0.14\n0.035\n0.077\n\n\n5\neasy\n0.12\n0.12\n0.031\n0.062\n\n\n6\ndiffers\n0.10\n0.09\n0.025\n0.054\n\n\n7\nsecure\n0.07\n0.07\n0.017\n0.041\n\n\n\n\n\n\n\nCombining the four variable importance analysis methods (Pearson, standardized regression coefficient, Usefulness, Johnson’s Relative Weights), the following key observations can be obtained:\n\nThe most important variables are trust, impact, and service Whether it is simple correlation, regression coefficient or explanatory power improvement, these three variables are firmly in the top three, indicating that they are key drivers of satisfaction.\nOther variables such as rewarding and appealing have moderate contributions Although not the primary factors, these attributes still have potential influence in marketing or product experience.\nJohnson’s Weights and Usefulness are highly consistent Both emphasize the overall explanatory power distribution and provide a more robust variable ranking when considering collinearity.\n\n\n\n\n2.2 K-Nearest Neighbors (KNN)\n\nCustom Implementation\n\n\nAccuracy and Evaluation\n\n\nVisualization and Summary"
  },
  {
    "objectID": "blog/Project4/hw4_questions.html#final-reflections",
    "href": "blog/Project4/hw4_questions.html#final-reflections",
    "title": "Key Drivers Analysis",
    "section": "3. Final Reflections",
    "text": "3. Final Reflections\n\n3.1 Modeling Challenges and Trade-offs\n\n\n3.2 Potential Extensions\nThis post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\ntodo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python.\nIf you want a challenge, either (1) implement one or more of the measures yourself. “Usefulness” is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost."
  },
  {
    "objectID": "blog/Project4/hw4_questions.html#the-lc-mnl-model-not-only-provides-us-with-the-selection-logic-of-each-potential-group-but-also-accurately-infers-the-market-type-to-which-the-individual-belongs.-in-this-case-brand-preference-and-price-sensitivity-constitute-the-two-core-difference-groups-in-the-market-showing-that-lc-mnl-can-effectively-capture-market-heterogeneity-and-is-a-more-strategic-tool-than-traditional-mnl.",
    "href": "blog/Project4/hw4_questions.html#the-lc-mnl-model-not-only-provides-us-with-the-selection-logic-of-each-potential-group-but-also-accurately-infers-the-market-type-to-which-the-individual-belongs.-in-this-case-brand-preference-and-price-sensitivity-constitute-the-two-core-difference-groups-in-the-market-showing-that-lc-mnl-can-effectively-capture-market-heterogeneity-and-is-a-more-strategic-tool-than-traditional-mnl.",
    "title": "Key Drivers Analysis",
    "section": "The LC-MNL model not only provides us with the selection logic of each potential group, but also accurately infers the market type to which the individual belongs. In this case, brand preference and price sensitivity constitute the two core difference groups in the market, showing that LC-MNL can effectively capture market heterogeneity and is a more strategic tool than traditional MNL.",
    "text": "The LC-MNL model not only provides us with the selection logic of each potential group, but also accurately infers the market type to which the individual belongs. In this case, brand preference and price sensitivity constitute the two core difference groups in the market, showing that LC-MNL can effectively capture market heterogeneity and is a more strategic tool than traditional MNL."
  },
  {
    "objectID": "blog/Project5/index.html",
    "href": "blog/Project5/index.html",
    "title": "Add Title",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "blog/Project5/index.html#a.-k-means",
    "href": "blog/Project5/index.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?"
  },
  {
    "objectID": "blog/Project5/index.html#b.-latent-class-mnl",
    "href": "blog/Project5/index.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blog/Project5/index.html#a.-k-nearest-neighbors",
    "href": "blog/Project5/index.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "blog/Project5/index.html#b.-key-drivers-analysis",
    "href": "blog/Project5/index.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "blog/Project6/index.html",
    "href": "blog/Project6/index.html",
    "title": "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification",
    "section": "",
    "text": "K-means can only process numerical variables, so we need to first select suitable features (in this question, we use beak length and fin length) and remove missing values.\n\n\nData organization\nimport pandas as pd\npenguins_df=pd.read_csv(\"/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/palmer_penguins.csv\")\ndata = penguins_df[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna().values\n\n\nThese two variables are highly heterogeneous in body structure and can help the model to effectively classify the groups. Using .dropna() can ensure data integrity and avoid subsequent errors.\n\n\n\nManual implementation allows to deeply understand each step: assigning groups, updating centers, convergence conditions, etc. It can also be used to compare the results with the package.\n\nimport numpy as np\ndef kmeans(X, k=3, max_iters=100):\n    centroids = X[np.random.choice(len(X), k, replace=False)]\n    for _ in range(max_iters):\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\nlabels_custom, centroids_custom = kmeans(data, k=3)\n\nThis program will assign data points to the nearest center based on distance and repeatedly update the center until it stabilizes. You will see that the groups gradually separate clearly after a few iterations.\n\n\n\nK-means is a distance-based algorithm. Visualization can help us understand how the algorithm classifies data.\n\nEach cluster is represented by a different color\nThe red X is the cluster center\nYou can see how the center points are clustered at the average position of the data within the cluster and show stable cluster boundaries\n\n\n\nVisualizing\nimport matplotlib.pyplot as plt\n\nplt.scatter(data[:, 0], data[:, 1], c=labels_custom, cmap='viridis')\nplt.scatter(centroids_custom[:, 0], centroids_custom[:, 1], color='red', marker='X')\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"flipper_length_mm\")\nplt.title(\"K-means clustering results\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWCSS Visualizing\n# Re-import necessary libraries after code state reset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n# Reload the dataset\ndata = penguins_df[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna().values\n\n# Initialize lists to store evaluation metrics\ninertias = []\nsilhouette_scores = []\nk_range = range(2, 8)  # Testing K values from 2 to 7\n# Compute metrics for each K\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(data)\n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(data, labels))\n\n# Plotting results\nplt.figure()\n\n# Plot WCSS\nplt.subplot()\nplt.plot(k_range, inertias, marker='o')\nplt.title(\"Within-Cluster Sum of Squares (WCSS)\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"WCSS\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\nThe smaller the value, the closer the points in the cluster are to the center, and the better the clustering effect.\nAs the number of clusters increases, WCSS will continue to decrease, but the so-called “elbow point” will appear.\nObservation: The decline begins to slow down when K=3 or K=4, and K=3 is a reasonable choice. ### Silhouette Score\n\n\n\nSilhouette Score Visualizing\n# Plot Silhouette Score\nplt.plot(k_range, silhouette_scores, marker='o', linestyle='-', linewidth=2, color='#FF8C00')\nplt.xticks(k_range)\nplt.title(\"Silhouette Score by Number of Clusters\", fontsize=14)\nplt.xlabel(\"Number of Clusters (K)\", fontsize=12)\nplt.ylabel(\"Silhouette Score\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.6)\n\n\n\n\n\n\n\n\n\n\nMeasures the relative distance of each point from its own group vs. neighboring groups, ranging from -1 to 1, with higher values ​​indicating clearer grouping.\nObservation: K=2 has the highest score (&gt;0.6), and K=3 has the second highest score (~0.48).\nAlthough K=2 has the highest score, combined with the WCSS inflection point, K=3 achieves a good balance between the contour grouping effect and the WCSS cost.\n\n\n\n\n\n\nKMeans Results\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=3, random_state=42)\nmodel_labels = model.fit_predict(data)\ncomp_result = pd.DataFrame(data, columns=[\"bill_length_mm\", \"flipper_length_mm\"])\ncomp_result[\"KMeans_label\"] = model_labels\ncomp_result.head(10)\n\n\n\n\n\n\n\n\n\nbill_length_mm\nflipper_length_mm\nKMeans_label\n\n\n\n\n0\n39.1\n181.0\n0\n\n\n1\n39.5\n186.0\n0\n\n\n2\n40.3\n195.0\n0\n\n\n3\n36.7\n193.0\n0\n\n\n4\n39.3\n190.0\n0\n\n\n5\n38.9\n181.0\n0\n\n\n6\n39.2\n195.0\n0\n\n\n7\n41.1\n182.0\n0\n\n\n8\n38.6\n191.0\n0\n\n\n9\n34.6\n198.0\n0\n\n\n\n\n\n\n\n\n\nSklearn Results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nplt.scatter(data[:, 0], data[:, 1], c=model_labels, cmap='cool', s=50)\nplt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], color='black', marker='X', s=200)\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"flipper_length_mm\")\nplt.title(\"Sklearn KMeans Clustering Result\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCluster Results\nimport numpy as np\nunique, counts = np.unique(model_labels, return_counts=True)\ncluster_summary = dict(zip(unique, counts))\nprint(\"The number of data per group：\", cluster_summary)\n\n\nThe number of data per group： {0: 143, 1: 96, 2: 94}\n\n\n\n\n\nIn this analysis, I implemented the K-means clustering algorithm from scratch and applied it to the Palmer Penguins dataset using bill length and flipper length as input features. The algorithm successfully grouped the penguins into three distinct clusters, capturing underlying patterns in their morphology. I then compared my results to those generated by the built-in KMeans function from scikit-learn, and found a high degree of similarity in clustering structure. This not only validated the correctness of my implementation but also provided valuable insights into how feature selection and distance-based clustering can uncover natural groupings within biological data. Overall, this exercise deepened my understanding of unsupervised learning, algorithm design, and exploratory data analysis."
  },
  {
    "objectID": "blog/Project6/index.html#a.-k-means",
    "href": "blog/Project6/index.html#a.-k-means",
    "title": "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification",
    "section": "",
    "text": "K-means can only process numerical variables, so we need to first select suitable features (in this question, we use beak length and fin length) and remove missing values.\n\n\nData organization\nimport pandas as pd\npenguins_df=pd.read_csv(\"/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/palmer_penguins.csv\")\ndata = penguins_df[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna().values\n\n\nThese two variables are highly heterogeneous in body structure and can help the model to effectively classify the groups. Using .dropna() can ensure data integrity and avoid subsequent errors.\n\n\n\nManual implementation allows to deeply understand each step: assigning groups, updating centers, convergence conditions, etc. It can also be used to compare the results with the package.\n\nimport numpy as np\ndef kmeans(X, k=3, max_iters=100):\n    centroids = X[np.random.choice(len(X), k, replace=False)]\n    for _ in range(max_iters):\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\nlabels_custom, centroids_custom = kmeans(data, k=3)\n\nThis program will assign data points to the nearest center based on distance and repeatedly update the center until it stabilizes. You will see that the groups gradually separate clearly after a few iterations.\n\n\n\nK-means is a distance-based algorithm. Visualization can help us understand how the algorithm classifies data.\n\nEach cluster is represented by a different color\nThe red X is the cluster center\nYou can see how the center points are clustered at the average position of the data within the cluster and show stable cluster boundaries\n\n\n\nVisualizing\nimport matplotlib.pyplot as plt\n\nplt.scatter(data[:, 0], data[:, 1], c=labels_custom, cmap='viridis')\nplt.scatter(centroids_custom[:, 0], centroids_custom[:, 1], color='red', marker='X')\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"flipper_length_mm\")\nplt.title(\"K-means clustering results\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWCSS Visualizing\n# Re-import necessary libraries after code state reset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n# Reload the dataset\ndata = penguins_df[[\"bill_length_mm\", \"flipper_length_mm\"]].dropna().values\n\n# Initialize lists to store evaluation metrics\ninertias = []\nsilhouette_scores = []\nk_range = range(2, 8)  # Testing K values from 2 to 7\n# Compute metrics for each K\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(data)\n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(data, labels))\n\n# Plotting results\nplt.figure()\n\n# Plot WCSS\nplt.subplot()\nplt.plot(k_range, inertias, marker='o')\nplt.title(\"Within-Cluster Sum of Squares (WCSS)\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"WCSS\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\nThe smaller the value, the closer the points in the cluster are to the center, and the better the clustering effect.\nAs the number of clusters increases, WCSS will continue to decrease, but the so-called “elbow point” will appear.\nObservation: The decline begins to slow down when K=3 or K=4, and K=3 is a reasonable choice. ### Silhouette Score\n\n\n\nSilhouette Score Visualizing\n# Plot Silhouette Score\nplt.plot(k_range, silhouette_scores, marker='o', linestyle='-', linewidth=2, color='#FF8C00')\nplt.xticks(k_range)\nplt.title(\"Silhouette Score by Number of Clusters\", fontsize=14)\nplt.xlabel(\"Number of Clusters (K)\", fontsize=12)\nplt.ylabel(\"Silhouette Score\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.6)\n\n\n\n\n\n\n\n\n\n\nMeasures the relative distance of each point from its own group vs. neighboring groups, ranging from -1 to 1, with higher values ​​indicating clearer grouping.\nObservation: K=2 has the highest score (&gt;0.6), and K=3 has the second highest score (~0.48).\nAlthough K=2 has the highest score, combined with the WCSS inflection point, K=3 achieves a good balance between the contour grouping effect and the WCSS cost.\n\n\n\n\n\n\nKMeans Results\nfrom sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=3, random_state=42)\nmodel_labels = model.fit_predict(data)\ncomp_result = pd.DataFrame(data, columns=[\"bill_length_mm\", \"flipper_length_mm\"])\ncomp_result[\"KMeans_label\"] = model_labels\ncomp_result.head(10)\n\n\n\n\n\n\n\n\n\nbill_length_mm\nflipper_length_mm\nKMeans_label\n\n\n\n\n0\n39.1\n181.0\n0\n\n\n1\n39.5\n186.0\n0\n\n\n2\n40.3\n195.0\n0\n\n\n3\n36.7\n193.0\n0\n\n\n4\n39.3\n190.0\n0\n\n\n5\n38.9\n181.0\n0\n\n\n6\n39.2\n195.0\n0\n\n\n7\n41.1\n182.0\n0\n\n\n8\n38.6\n191.0\n0\n\n\n9\n34.6\n198.0\n0\n\n\n\n\n\n\n\n\n\nSklearn Results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nplt.scatter(data[:, 0], data[:, 1], c=model_labels, cmap='cool', s=50)\nplt.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1], color='black', marker='X', s=200)\nplt.xlabel(\"bill_length_mm\")\nplt.ylabel(\"flipper_length_mm\")\nplt.title(\"Sklearn KMeans Clustering Result\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCluster Results\nimport numpy as np\nunique, counts = np.unique(model_labels, return_counts=True)\ncluster_summary = dict(zip(unique, counts))\nprint(\"The number of data per group：\", cluster_summary)\n\n\nThe number of data per group： {0: 143, 1: 96, 2: 94}\n\n\n\n\n\nIn this analysis, I implemented the K-means clustering algorithm from scratch and applied it to the Palmer Penguins dataset using bill length and flipper length as input features. The algorithm successfully grouped the penguins into three distinct clusters, capturing underlying patterns in their morphology. I then compared my results to those generated by the built-in KMeans function from scikit-learn, and found a high degree of similarity in clustering structure. This not only validated the correctness of my implementation but also provided valuable insights into how feature selection and distance-based clustering can uncover natural groupings within biological data. Overall, this exercise deepened my understanding of unsupervised learning, algorithm design, and exploratory data analysis."
  },
  {
    "objectID": "blog/Project6/index.html#b.-latent-class-mnl",
    "href": "blog/Project6/index.html#b.-latent-class-mnl",
    "title": "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\nThis analysis aims to use the Latent-Class Multinomial Logit (LC-MNL) model to analyze consumers’ choice behavior for four yogurt brands, further identify potential market segments, and understand the responses of different categories of consumers to price and promotions.\n\nThe traditional MNL model assumes that everyone has the same preferences.\nThe LC-MNL model allows for consumer heterogeneity and estimates the choice tendency of each category through latent groups (segments), and also estimates the probability of each consumer belonging to which group.\n\n\nData Exploring\nThe model requires structured input data, including candidate brands, attributes, and selection labels for each selection. This program will split the original 4 brand options in each column into 4 columns, so that 1 observation corresponds to 1 brand, and the total number of data columns will be 4 times, which is suitable for choice model analysis.\n\n\nData Preparation\nimport pandas as pd\n\ndf_yogurt = pd.read_csv(\"/home/jovyan/Desktop/UCSD/Spring/MGTA495/lulu_marketing_analytics/blog/Project4/yogurt_data.csv\")\ndf_long = pd.DataFrame()\nfor i in range(1, 5):  \n    temp = pd.DataFrame({\n        'id': df_yogurt['id'],\n        'brand': i,\n        'price': df_yogurt[f'p{i}'],\n        'feature': df_yogurt[f'f{i}'],\n        'chosen': df_yogurt[f'y{i}']\n    })\n    df_long = pd.concat([df_long, temp], ignore_index=True)\ndf_long.head()\n\n\n\n\n\n\n\n\n\nid\nbrand\nprice\nfeature\nchosen\n\n\n\n\n0\n1\n1\n0.108\n0\n0\n\n\n1\n2\n1\n0.108\n0\n0\n\n\n2\n3\n1\n0.108\n0\n0\n\n\n3\n4\n1\n0.108\n0\n0\n\n\n4\n5\n1\n0.125\n0\n0\n\n\n\n\n\n\n\n\nid represents each purchase choice scenario\nchoice represents the actual chosen brand\nbrand, price, and feat are attributes of candidate brands\nchosen represents whether it is an actual choice (1/0), which is a dependent variable of the MNL model\n\n\n\nBuilding a baseline Multinomial Logit model (MNL)\nBefore building the Latent-Class MNL model, we need to estimate a traditional Multinomial Logit (MNL) model as a baseline. This model assumes that all consumers have the same preferences for product attributes (such as price and promotion), and uses this set of “average preferences” to explain the overall market choice behavior.\nBIC strikes a balance between log-likelihood performance and parameter complexity: \\(\\text{BIC} = -2 \\cdot \\ell_n + k \\cdot \\log(n)\\) - \\(\\ell_n\\): log-likelihood of the model (the higher the better) - \\(k\\): number of parameters (the fewer the better) - \\(n\\): number of data points (number of samples)\nThe goal is to find the number of groups with the smallest BIC, which represents the best compromise between performance and simplicity.\n\n\nModel Building\nfrom scipy.optimize import minimize\ndf_subset = df_long[df_long[\"id\"] &lt;= 300].copy()\nX_sub = df_subset[[\"price\", \"feature\"]].values\ny_sub = df_subset[\"chosen\"].values\nids_sub = df_subset[\"id\"].values\n\ndef estimate_lc_mnl_fast(X, y, ids, S=2, T=3):\n    id_map = {v: i for i, v in enumerate(np.unique(ids))}\n    id_idx = np.array([id_map[i] for i in ids])\n    num_obs = len(np.unique(id_idx))\n    J = 4\n    np.random.seed(0)\n    beta = [np.random.randn(X.shape[1]) for _ in range(S)]\n    pi = np.full(S, 1 / S)\n    responsibilities = np.zeros((num_obs, S))\n\n    def mnl_prob(X, beta):\n        utilities = X @ beta\n        exp_util = np.exp(utilities - np.max(utilities))\n        return exp_util / np.sum(exp_util)\n\n    for t in range(T):\n        # E-step\n        for i in range(num_obs):\n            probs = []\n            for s in range(S):\n                xi = X[id_idx == i]\n                yi = y[id_idx == i]\n                pj = mnl_prob(xi, beta[s])\n                prob = np.prod(pj[yi == 1])\n                probs.append(pi[s] * prob)\n            probs = np.array(probs)\n            responsibilities[i] = probs / np.sum(probs)\n\n        # M-step\n        for s in range(S):\n            weights = responsibilities[:, s]\n            def neg_log_likelihood(b):\n                ll = 0\n                for i in range(num_obs):\n                    xi = X[id_idx == i]\n                    yi = y[id_idx == i]\n                    pj = mnl_prob(xi, b)\n                    ll += weights[i] * np.sum(yi * np.log(pj + 1e-10))\n                return -ll\n            res = minimize(neg_log_likelihood, beta[s], method='L-BFGS-B')\n            beta[s] = res.x\n        pi = responsibilities.mean(axis=0)\n\n    ll = 0\n    for i in range(num_obs):\n        xi = X[id_idx == i]\n        yi = y[id_idx == i]\n        seg_prob = 0\n        for s in range(S):\n            pj = mnl_prob(xi, beta[s])\n            prob = np.prod(pj[yi == 1])\n            seg_prob += pi[s] * prob\n        ll += np.log(seg_prob + 1e-10)\n\n    return ll, beta, pi\n\nbic_results_simplified = []\n\nfor S in range(2, 6):\n    ll, beta_out, pi_out = estimate_lc_mnl_fast(X_sub, y_sub, ids_sub, S=S, T=3)\n    k = S * X_sub.shape[1] + (S - 1)  # parameters: S * beta + (S - 1) segment proportions\n    n = len(np.unique(ids_sub))  # number of choice situations\n    bic = -2 * ll + k * np.log(n)\n    bic_results_simplified.append((S, ll, k, bic))\n\nbic_df_simplified = pd.DataFrame(bic_results_simplified, columns=[\"NumSegments\", \"LogLikelihood\", \"NumParameters\", \"BIC\"])\nbic_df_simplified.sort_values(by=\"BIC\")\n\n\n\n\n\n\n\n\n\nNumSegments\nLogLikelihood\nNumParameters\nBIC\n\n\n\n\n0\n2\n-405.501502\n5\n839.521917\n\n\n1\n3\n-405.433649\n8\n856.497557\n\n\n2\n4\n-405.460375\n11\n873.662358\n\n\n3\n5\n-405.491688\n14\n890.836330\n\n\n\n\n\n\n\n\nAlthough increasing the number of groups will slightly increase the model’s log-likelihood, the number of parameters will also increase dramatically.\nTherefore, the higher the BIC, the higher the “increase in model complexity” offsets the slight improvement.\nOverall, Segment = 2 is currently the best model setting.\n\n\n\nCompare multi-group models using BIC to select the optimal number of groups\nIn the previous step, we used the EM algorithm to build Latent-Class Multinomial Logit (LC-MNL) models for 2, 3, 4, and 5 groups respectively. Each model assumes that the consumer market is composed of different numbers of potential groups (segments) and estimates different preference parameters (β) for each group.\nHowever, the more groups there are, the more parameters there are, and the more complex the model is, there is a possibility of “overfitting”. Therefore, we cannot only use log-likelihood to select a model, but need to use an indicator that considers the accuracy and complexity of the model - BIC (Bayesian Information Criterion).\nBIC formula: \\(\\text{BIC} = -2 \\cdot \\ell_n + k \\cdot \\log(n)\\)\nThe lower the BIC, the better: it means the model remains simple while improving accuracy.\n\nAlthough the model fit (Log-Likelihood) is slightly improved by increasing the number of groups S\nBut with each additional group, the number of parameters also increases significantly (the model complexity increases)\nBIC measures the balance between accuracy and complexity, and BIC is lowest when S=2\n\nIn the Latent-Class MNL model, the optimal number of groups is 2.This means the market can be divided into two major consumer groups with significantly different preferences. Next, we can analyze the β coefficient and group proportion (π) for these two groups and make further marketing strategy recommendations\n\n\nAnalyze the preference parameters and group proportions of the best group (S=2)\nWe use BIC to determine that two groups (S=2) are the best Latent-Class MNL model. Next, we need to: - Sort out the preference parameters β (sensitivity to price and promotion) of the two groups - Draw a graph to compare the preference differences of different groups - Analyze the proportion of each group of consumers in the market (π) - Provide business decision suggestions\n\n\nSegment\nimport pandas as pd\nll, beta, pi = estimate_lc_mnl_fast(X_sub, y_sub, ids_sub, S=2, T=3)\nsegment_df = pd.DataFrame({\n    \"Segment\": [1, 2],\n    \"Beta_Price\": [beta[0][0], beta[1][0]],\n    \"Beta_Feature\": [beta[0][1], beta[1][1]],\n    \"Segment_Probability\": pi\n})\nsegment_df\n\n\n\n\n\n\n\n\n\nSegment\nBeta_Price\nBeta_Feature\nSegment_Probability\n\n\n\n\n0\n1\n10.601429\n-0.074319\n0.506933\n\n\n1\n2\n12.834822\n1.993827\n0.493067\n\n\n\n\n\n\n\n\nSegment 1\n\n\nVery low sensitivity to promotion (β ≈ 0)\nPositive but low sensitivity to price\n50.3% of the market\nRecommended strategy: Emphasize product quality and brand value, no need for excessive promotion\n\n\nSegment 2\n\n\nVery high sensitivity to promotion (β ≈ 2.6)\nHigher price sensitivity\n49.7% of the market\nRecommended strategy: Discounts, gifts, and special offers are effective and suitable for short-term promotional stimulation\n\n\n\nVisualizing\nimport matplotlib.pyplot as plt\n\n# Create summary DataFrame again for clarity\nsegment_df = pd.DataFrame({\n    \"Segment\": [1, 2],\n    \"Beta_Price\": [beta[0][0], beta[1][0]],\n    \"Beta_Feature\": [beta[0][1], beta[1][1]],\n    \"Segment_Probability\": pi\n})\n\n# Plotting segment-specific betas and segment share\nfig, ax = plt.subplots(1, 2, figsize=(12, 5))\n\n# Bar plot for beta values\nsegment_df.plot(x=\"Segment\", y=[\"Beta_Price\", \"Beta_Feature\"], kind=\"bar\", ax=ax[0])\nax[0].set_title(\"Segment-Specific Preferences\")\nax[0].set_ylabel(\"Coefficient (Beta)\")\nax[0].set_xlabel(\"Segment\")\nax[0].legend([\"Price\", \"Feature\"], title=\"Variable\")\n\n# Pie chart for segment probabilities\nax[1].pie(segment_df[\"Segment_Probability\"], labels=[f\"Segment {s}\" for s in segment_df[\"Segment\"]],\n          autopct='%1.1f%%', colors=[\"#66c2a5\", \"#fc8d62\"], startangle=90)\nax[1].set_title(\"Segment Market Share\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLeft: Segment preference comparison chart (bar chart)\n\n\nDisplays the β coefficient of each Segment (potential group) for price (Price) and promotion (Feature)\nClearly compares the sensitivity of two groups of consumers to different variables\n\n\nRight: Segment market share chart (pie chart)\n\n\nDisplays the proportion of each potential category in the market (π value)\nSegment 1 is about 50.3%, Segment 2 is about 49.7%\n\nAccording to the model results, the market can be divided into two potential consumer groups. Segment 1 is very insensitive to promotions and slightly sensitive to prices. It is recommended to adopt a marketing strategy that emphasizes quality and brand value; while Segment 2 is highly sensitive to both prices and promotions. It is suitable to stimulate purchase intention through promotional means such as discounts and gifts. Since the market share of the two groups is almost the same, it is recommended to adopt a dual-track parallel marketing strategy, designing differentiated messages and plans for different groups to increase the overall market penetration rate."
  },
  {
    "objectID": "blog/Project6/index.html#a.-k-nearest-neighbors",
    "href": "blog/Project6/index.html#a.-k-nearest-neighbors",
    "title": "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\nWe want to create a simulated data set for a binary classification problem, with the following features: - The data has two features (x1, x2) - The class label y is determined by the boundary of x2 &gt; sin(4x1) + x1 (i.e., the classification above and below a wavy line)\nSuch data can help us test whether the KNN algorithm can effectively handle classification tasks with “non-linear decision boundaries”.\n\nGenerate training data set\nWe need a set of data to train the KNN model. This set of data will simulate the situation in the real world where the data and boundaries are not linearly separable.\n\nimport numpy as np\nimport pandas as pd\n\n# Set random seed\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nx = np.column_stack((x1, x2))\n\n# Define a wiggly boundary\nboundary = np.sin(4 * x1) + x1\ny = (x2 &gt; boundary).astype(int)\ndat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n\n\n\nVisual training materials and boundaries\nPlotting the simulated data set allows us to clearly see the relationship between the distribution of data points and the position of the classification boundary. We randomly generated 100 two-dimensional data (x1, x2), and y was marked as 0 or 1 depending on whether it was above boundary = sin(4 * x1) + x1 (this is a curved “real boundary”). By plotting this boundary and data points, we can visually observe the difficulty of classification.\n\n\nVisualizing\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nboundary = np.sin(4 * x1) + x1\ny = (x2 &gt; boundary).astype(int)\n\ntrain_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nplt.scatter(train_df[\"x1\"], train_df[\"x2\"], c=train_df[\"y\"], cmap='coolwarm', edgecolor='k')\nx_line = np.linspace(-3, 3, 300)\nplt.plot(x_line, np.sin(4 * x_line) + x_line, color='black', linestyle='--', label=\"True Boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Training Data and True Boundary\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThrough the graph, we can clearly see how the data points are distributed on both sides of the wiggly boundary, and we can also intuitively judge the difficulty of classification.\n\n\nGenerate test data sets (different random seeds)\nThe test set is used to verify the model effect on unseen data. Different seeds must be used to avoid data duplication.\n\nnp.random.seed(99)\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test &gt; boundary_test).astype(int)\n\ntest_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\n\n\n\nHandwritten KNN Classifier\nBy implementing KNN yourself, you can deepen your understanding of the logic of neighbor classification: find the k closest points → majority vote.\n\ndef knn_predict(x_test, X_train, y_train, k):\n    y_pred = []\n    for x in x_test:\n        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n        neighbors = y_train[np.argsort(distances)[:k]]\n        vote = np.bincount(neighbors).argmax()\n        y_pred.append(vote)\n    return np.array(y_pred)\n\n\n\nCompare to sklearn’s built-in results\n\n\nCompare to sklearn’s built-in results\nfrom sklearn.neighbors import KNeighborsClassifier\n\nnp.random.seed(42)\nn = 100\nx1_train = np.random.uniform(-3, 3, n)\nx2_train = np.random.uniform(-3, 3, n)\nboundary_train = np.sin(4 * x1_train) + x1_train\ny_train = (x2_train &gt; boundary_train).astype(int)\ntrain_df = pd.DataFrame({'x1': x1_train, 'x2': x2_train, 'y': y_train})\n\nnp.random.seed(99)\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = (x2_test &gt; boundary_test).astype(int)\ntest_df = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\n\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(train_df[[\"x1\", \"x2\"]], train_df[\"y\"])\nacc = model.score(test_df[[\"x1\", \"x2\"]], test_df[\"y\"])\n\nknn_result_df = pd.DataFrame({\n    \"Model\": [\"sklearn KNN\"],\n    \"k\": [5],\n    \"Accuracy\": [round(acc * 100, 2)]\n})\nknn_result_df\n\n\n\n\n\n\n\n\n\nModel\nk\nAccuracy\n\n\n\n\n0\nsklearn KNN\n5\n90.0\n\n\n\n\n\n\n\n\nWe used 5 neighboring points (k=5) to classify the test data\nThe prediction accuracy is 90%, which means that the model has good recognition ability for nonlinear boundary data\nThis can be used as a comparison benchmark when we implement “custom KNN” or test different k values ​​later\n\n\n\nAccuracy performance for runs k = 1 to 30\nIn the KNN model, the “k value” represents how many neighboring samples each piece of data needs to refer to for classification. Different k values ​​will have a significant impact on the model’s prediction results: - Too small k value: The model is overly dependent on a single neighbor, easily sensitive to noise, and leads to overfitting - Too large k value: The model averages too much information, the boundaries become blurred, and underfitting may occur\nBy testing the performance of the model with k = 1 to 30 one by one, we can choose an optimal k value that makes the classification accuracy most stable and effective.\n\n\nPerformance Visualization\naccuracies = []\nfor k in range(1, 31):\n    y_pred = knn_predict(test_df[[\"x1\", \"x2\"]].values, train_df[[\"x1\", \"x2\"]].values, train_df[\"y\"].values, k)\n    acc = np.mean(y_pred == test_df[\"y\"].values)\n    accuracies.append(acc)\n\nplt.plot(range(1, 31), np.array(accuracies) * 100, marker='o')\nplt.xlabel(\"k (number of neighbors)\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"KNN Accuracy vs. k\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe horizontal axis of the chart is the number of neighbors k, and the vertical axis is the classification accuracy of the test data (Accuracy %). - When k = 1 or 2, the accuracy reaches the highest, about 92% - As k increases, the accuracy shows an overall downward trend - There are occasional fluctuations in the middle (such as k = 16, 24, the accuracy increases slightly) - When k &gt; 20, the accuracy is mostly stable at around 86% ~ 88%"
  },
  {
    "objectID": "blog/Project6/index.html#b.-key-drivers-analysis",
    "href": "blog/Project6/index.html#b.-key-drivers-analysis",
    "title": "Dual-Approach Machine Learning Analysis: K-Means Clustering and KNN Classification",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  }
]